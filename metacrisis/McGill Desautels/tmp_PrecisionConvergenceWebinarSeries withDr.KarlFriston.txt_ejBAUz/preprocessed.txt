For those who are joining, I am Laura Dub√©, the co-chair of this webinar.
We have two more minutes before we formally start.
We will be officially starting at 11.03.
There's Lauren and Alain.
Is Alain here yet?
I forgot about Alain.
Yes, he's there. I see him. He's online.
Okay, good.
Yeah, we're up to 80 participants.
Dr. Dagger, it's good to see you.
See you in a while.
Yes, same here.
We all meet in suburb space those days.
Okay, we will be starting just now, I think it's close to 11.03.
Good morning, everyone.
Welcome to this webinar that is part of the Precision Convergence Capacity Building Webinar Series.
This series is co-hosted by McGill Centre for the Convergence of Health and Economics
and the Pittsburgh Supercomputing Centre.
The series was started about a year ago.
Following up the HPCI-performing computer, COVID-19 consortium that had been engaging
PSE, another research computing institution around the country, with IBM and many of the other data business,
for accelerating our ability to do research on health and health and health and health
and health and health and health and health and health and health and health and health
and health and health and health data business for accelerating our ability to bring solutions to COVID-19
This consortium was created for seeing, to what extent we can bridge the omics and all the digital capacity to accelerate solution to COVID-19
profiling, characterizing of the virus, vaccine and so on and so forth, and we said, what if we were to take as a silver lining of the COVID-19, continue building of this capacity of AI and digital technology and others to see to what extent we can bridge the various
science that are needed if we are to bring in a timely manner a solution to real world problem, whether it is pandemics or whether it is climate change and so on.
And a lot of it is tied to the fact that in the last 250 years, we have changed a lot the context within which the human machine has evolved.
The first industrial revolution was 250 years ago. It's essentially a few seconds in the journey of mankind. So this series is really to bring the scientists, the action leader, data operating on multiple scale in saying can we go beyond than what we are doing so far.
So before I introduce more, what is it we'll do together during this next two hours and the keynote.
I would like to ask my co-chair to introduce himself and then I will come back for introducing the keynote. Go ahead, Sean.
Hi, my name is Sean Brown. I'm the director of the Pittsburgh Supercomputing Center, a joint computational research center between Carnegie Mellon University and the University of Pittsburgh.
And I'm also the vice chancellor for research computing at the University of Pittsburgh. And I'm very excited about our talk today and our panel, follow up panel discussion. So I will take less, I will not take any more time. So thank you all for joining us.
Thank you, Sean. Very briefly in a nutshell, the way we spend the next two hours. I have been for those of you who were on the series before, we always start by saying it's not a webinar.
It's a capacity building webinar where we convene a keynote of significance in advancing science in one of the three domains, the human machine, the man-made system or the computer.
And we build a panel where everyone is carefully selected to not only be aware and complimentary to the keynote, but each of the panelists, as you can see, are also bringing those, their own perspective in advancing this precision convergence program.
So we have the first 30, 40 minutes of the keynote presentation. And then we have a first round of panel where each of the panelists is bringing a three to five minutes commentary from their perspective in response to the keynote presentation, but also from their own, their own knowledge and so on.
The keynote has about two or three minutes for each of them to answer in the first round. And then in a second part, then if there are questions from the audience, Sean will be managing them from the chat room to address them after the first round of the panelists.
And then we have a second round where everyone, including the keynote, is invited to kind of do the one minute commentary of where can we go as a world community if we are to really reach scale into the transformation that we are all wanting to achieve.
Okay. So, this is, this is kind of the logistic aspects. I would like before introducing Dr. Carl Friston to position a bit is presentation within the continuum of the webinar we have been having over the last year.
On the psychology self side, we have been having as keynote, Roy Bormester, Daniel Schachter, the last one on which Carl was on the panel was by Konstantin Stedikides.
All of them really, one can arguably say the most influential living scientists in the study of psychology and the mind and so on.
And we have been, and in the last webinar where Carl was presented was on the panel, the discussion was to see to what extent the cell, to what extent the cell, the self, and the end the psychology could be placed on the same platform of the neurobiology when we look at human being being
evolving, not only for survival, but also for achievement. And this is within that context that we have been discussing and that's why the work of the work that will be presented today on the cell,
is not to be dancing for individual and collective agency. To what extent can this substantive knowledge of human biology, psychology, and real world can be brought together in some degree of formalism that helped bridging all of the above so that is why, without further introduction,
I am extremely pleased to welcome Carl Freestone and as we, as Sean has said, we are doing introductions very short because there is a limited time with the richness of the intellectual and human capital we have together in the next two hours.
So, Carl Freestone is a theoretical neuroscientist and authority on brain imaging. I won't enumerate the various statistical methods that he has been developing.
His most, his main contribution to theoretical neurobiology is a free energy principle for action and perception, which is the focus of his talk today.
So Carl, if you excuse me, I will not give read the whole, the whole description of your achievement and I will go with you, go to you directly for your presentation.
Thank you very much. It's a great pleasure to be back here. And it's now my turn to do a keynote of significance.
And I've got to do that with two constraints. First of all, get everything that's important on the table within half an hour, and also use not my favorite words, but the favorite words of my panelists.
I think we all met last week just to sort of cover the key themes that we want to bring out in discussion. So my job is to try and situate those within this keynote and this keynote is going to address a first principle account of sentient behavior.
There's a story about how I behave and how we behave that can be elaborated from the first principles of self organization and the approach I'm going to take is from the point of view of self evidencing so I'm going to introduce the notion of self evidencing as simply as possible.
I'm going to give a why this might be an interesting approach in relation to current perspectives on artificial intelligence and machine learning, and then try to ground it in the neurobiology, as was, you know, my brief or our brief in the introduction.
I'm going to take a special focus on the mechanics of bleep propagation. And in particular, how we organize our behavior to solicit the right kind of data that supplies evidence for our own existence and sell that notion that imperative as an important drive for active engagement with the world.
And then finally, talking to the collective aspect and the importance of collective agency and shared narratives. So self evidencing what do I mean by that. And I simply mean that everything that we think or do can be understood or described in terms of trying to maximize model evidence.
So what's model evidence. It's simply the probability of some observable outcomes, given my model of how those outcomes were generated.
And on this view we can associate belief updating with perception where I'm denoting beliefs here as with queue of some hidden states or causes latent states of the world that are generating these outcomes.
In a similar vein, I also am going to assume that we have beliefs about what we're going to do about our actions, and prior to selecting a particular action, we have to form beliefs about what is the most likely action you that I'm going to commit to.
Another is that both of these these kinds of belief updating are driven by an imperative to maximize model evidence. So that has a degree of constructivity in relation to establish normative accounts of behavior.
And if you hear, if I read this log evidence as the value of an outcome in the sense that if I prefer these kinds of outcomes or these outcomes are characteristic of me, then I should behave to secure those kinds of outcomes as valuable outcomes and from that we can spin off.
If I was in psychology reinforcement learning that was an engineer, we could cast this about as optimal control theory. And indeed, if I was an economist, it would just simply be one simple specification of expected utility theory.
And if on the other hand, I was an information theorist, I would read this quantity or the negative this one to this negative log probability of an event or an outcome conditioned upon a model as a self information, sometimes called surprise or more simply surprise.
And it is this quantity that is I'm going to associate with a tractable measure of the value or the negative value or the surprise, namely a free energy and this quantity suggests that everything we do is in the service of minimizing surprise.
We can read that as the principle of maximum mutual information on the max principle principles of minimum redundancy or maximum efficiency that can be attributed to Horace Barlow and indeed the free energy principle, which is a formalism I'm going to use.
This is nice because the expected self information, the expected surprise before an event has happened is entropy. So this also says that we perceive an act in a way to minimize on average entropy.
And of course that's the holy grail things like self organization of physics energetics and if I was a physiologist, it would simply be a statement of homeostasis that I am in the game of keeping my outcomes within some viable bounds minimizing their dispersion or the dissipation.
And if I were a statistician, I would treat this quantity as the model evidence and from that we can spin off things like the Bayesian brain hypothesis evidence accumulation and various implementations of that such as predictive coding.
So,
I'm going to unpack this quantity, this surprise in terms of value in terms of a free energy functional and try and give you a number of different perspectives that on this quantity.
So the first perspective is that we can regard this statement as a first principle statement about the imperatives for self organization that inherits directly from James's maximum entropy principle.
So in maximizing this free energy functional here, we are effectively increasing the dispersion of our beliefs for keeping our options open to the greatest extent possible under some constraints.
I'll talk about constraints later the particular constraints here are afforded by something called a generative model is usually decomposed into a likelihood and a prior the likelihood of some outcomes given their causes latent states in the world, and some prior beliefs or preferences about the states that I am likely to be exposed to in
terms of generating my observable outcomes. There's another way of writing down this quantity which provides a complementary perspective and that's the kind of perspective you get if you're a statistician.
So I'm just rearranging the terms in that free energy functional and to express it as a mixture of accuracy and complexity.
So the accuracy here is it's simply the expected log likely to some outcomes given my beliefs about how those outcomes were formed.
And whilst the complexity is a very interesting term in this setting it's just the difference between my beliefs after and before seeing some observations or evidence for beliefs about particular states of the world.
It scores the degree I change my mind when I see an outcome or observe something. It scores the cost of belief updating. And the story that I'm going to tell for the remainder of this talk is that if I have to choose what to do that will get some more
information, get some more observations, then I can cast behaviour of a sentient sword in terms of choosing those actions that maximise the accuracy and minimise the complexity I expect following that action.
It turns out that the expected accuracy complies exactly with the principles of optimal Bayesian design while the optimal the expected complexity turns out to be risk and is exactly the quantity that underwrites optimal decision theory optimal Bayesian decision theory.
What I'm saying is that putting these two things together gives us self evidencing in the spirit of selecting those behaviours or actions that maximise this expected free energy and implicitly increase the expected evidence.
So just to make this slightly more heuristic and to compare and contrast with alternative approaches, imagine you're an owl and you're hungry. And if I was there with you, I would be asking somebody on the front row, what are you going to do?
And invariably they give me the correct answer. Well, I'm going to look for my food. I'm going to search for my prey. And that simple answer tells us something quite fundamental about the way that you would write down a calculus for sentient behaviour.
And the two kinds of calculus that I'm going to compare and contrast and then bring together later on rests upon the notion of a value function of states of the world.
So, I could say, okay, if I had some function that scored the goodness the value of a state that ensued following at the next time point, following a chosen action you, I could then just choose for every given state of the world, the best or the value function maximising action to give
myself a state action policy. Is this kind of calculus apt for describing behaviour? Well, the answer I'm searching for my food tells you no, it can't be, because searching is an act that reduces your uncertainty about the location of your prey.
But uncertainty is not a function of states of the world, it's a function of the beliefs about states of the world. So, we have an alternative way of writing down good behaviour.
That is something that maximises a function, a function of a function. In this instance, that function is beliefs about the states of the world that would ensue if I committed to a particular action.
So, a very different kind of functional form. The other aspect of searching for food tells you something also about the nature of the best way to describe or the best policies.
And that's it matters whether I search for my food and eat it or try to eat it and then search for my food. And that tells one immediately that what we're chasing here is a way of describing a sequence of actions or actions over time.
And this functional then becomes technically a path integral, a sum of goodnesses over time. And that gives you a very different functional form for the optimal policies.
Now a sequential policy that maximises this path integral of an energy functional. We'll see in a moment, it's a free energy functional.
And the reason I'm using those words is just to refer to the two kinds of principles that we could appeal to in underwriting these different approaches to good behaviour.
So this state action policy is predicated on Bellman's optimality principle. We'll find that in optimal control theory dynamic programming deep reinforcement learning basic decision theory and so on.
And this on the other hand, has a slightly deeper and simpler heritage it's it's basically a variational principle of station reaction.
The free energy principle being one instance of that.
And we'll see later that this is more apt to describe things like artificial curiosity or in robotics intrinsic motivation, and is more concerned with data gathering optimal Bayesian design than Bayesian decision theory.
And I repeat, is concerned with optimizing sequences of policies under uncertainty, using things like partially observed Markov decision processes.
Principles of least action.
This is the apologize for the equations, but there's a beautiful symmetry between the functional forms of this way of writing down evidence in terms of complexity or accuracy.
And the things the equivalent things that you are optimizing, if you are selecting those behaviors that you think will maximize the expected free energy.
So, just writing down again the expression for the free energy here, in terms of accuracy and complexity, and rearranging it in a particular way here that people in machine learning will recognize.
So, changing these terms around.
We can also express this as the log of the evidence, the probability of some outcomes given a model, plus something, a k r divergence, something that is never less than zero, because this thing is never less than zero.
This quantity this free energy now becomes an evidence bound known with by the acronym and elder in evidence low bound machine learning.
So there's two ways of carving up this objective function. And what would they look like in expectation. Well, what happens is that the expected complexity becomes risk, the expected inaccuracy becomes ambiguity.
The expected log log evidence or negative log evidence becomes expected cost, and the expected divergence becomes expected information gain.
And this is beautiful, but it's also quite unremarkable because most people in one field or another already know about this. So for example, if we ignore the expected value of the negative expected costs then we're just left with this expected divergence.
What is this. Well, it's simply the difference between my beliefs about states of the world in the future, following a particular policy, if I were able to observe the consequences of that behavior relative to if I were not.
So it scores the information gain, or the reduction of uncertainty about hidden states or latent states of the world, afforded by this particular policies as the epistemic affordance of this policy.
And the visual search literature this will be known as expected Bayesian surprise more simply it's just the, the mutual information or the expected mutual information conditioned on a, on a policy.
I'm going to successfully remove sources of uncertainty from the table and just see what emerges and what's going to happen is we'll get back to the Bellman optimality principle and expected utility theory that on route.
Let's assume that we live in a universe where there's no ambiguity states or outcomes and outcomes and states I can see everything directly.
And this means that we can ignore the ambiguity term and supplant the SS for OS and vice versa and we're just left with this which is another KL divergence, and it just scores the difference between what I anticipate will happen.
I pursue this policy, and what a priori, I prefer to happen, given I am me, this is the expected the value of the negative, the negative cost expresses a probability.
So this minimizing this difference this divergence between what I anticipate and what I prefer is known in engineering as KL control and economics it's really sensitive control.
And it's simply a description of behavior in which you choose those things to do, which minimize the divergence of the outcomes that you expect under a generative model and mixture of likelihood and prize.
Namely, your prior preferences. And then finally, if we remove the kind of uncertainty that is reducible by my epistemic foraging or responding to epistemic affordances or the intrinsic motivation for finding out what would happen if I did that.
And we can ignore the expected information game we just left now with the expected custom that clearly can be read now as expected utility theory if you're in economics.
In summary, what we're saying is that there's a simple way of writing down self evidencing that speaks to engagement with the future and the way that we choose what to do next, and we can decompose maximization of expected free energy,
as we refer to as active inference, in terms of to dual aspect Bayesian or dual aspect Bayesian optimality one in terms of maximizing expected value of minimizing expected cost in accord with Bayesian decision theory base of making base optimal decisions.
At the same time, we're maximizing expected information gain, and this inherits from the work of people like Dennis Lindley on optimal Bayesian design basically designing your experiments and the way that you solicit those data that resolve the greatest amount of uncertainty about your
hypotheses or models of how data are generated.
So to close, I'm just going to race through things you can do with this first principle like account or method.
The first thing that one can do obviously is to simulate and in silica reproduces what agents and artifacts and see how they behave, but to do that you have to commit to a generative model these are the constraints that when supplied to James is maximum
entry principle gives you the free energy principle. So one key thing is that if you can write down a generative model, you can always express it in terms of a probabilistic graphical model.
What this slide is meant to convey is that then there always exists something called a factor graph. So why is this important. Well, the factor graph is just if you like a dual or an adjunct way of expressing your generative model in a way that reveals
the nature of the messages that need to be passed amongst the different belief structures that are over the random variables the hidden causes that generally are organized according to this Markov decision process so that we have outcomes that are generated by states of some
universe. Those states generate outcomes for a likelihood mapping. That's likely part of my generative model, but the states evolve over time through transition matrices that constitute my prior beliefs about the dynamics and the contingencies and the laws that denote the causal structure
of the universe. And it is these transitions that I'm in charge of in terms of the policies that I select and I select my policies to maximize the expected free energy here.
So that would be a generic graphical model description of an active engagement with the world. And this would be the equivalent factor graph that tells me the message passing that I would need to do to do the belief updating to realize and integrate the system to simulate
an increasing or active inference and all sorts of great games you can play here in terms of once you know the message passing the belief, the form, the functional forms and schemes of belief propagation that in some way must be under the hood for any given
model you wanted to commit to. And you can start to think about how this would, what this would look like in various systems such as the brain. I won't go into this but there's a, again, a very beautiful and very simple mapping between the time scales of different
updates during say belief updating perceptual inference, slower updating of the parameters of my generative model that we associate with learning and the action selection that ensues from this notion of planning as inference in terms of updating beliefs about what I'm going to do next.
And we can map this, these functions onto different brain areas and functional brain architectures and make predictions about the nature of those architectures which I'm sure we'll be talking about later.
We can simulate behavior with through one example just to sort of demystify and just pick out the most important aspect of this, of this formulation, which is basically this, this balance, this dual aspect, optimal Bayesian design versus optimal Bayesian decision
theory, and show that in fact that balance is systematically deterministically specified by how much I know about my environment. Effectively, it dissolves exploration exploitation dilemma.
And to illustrate that there's a little game we've created here we're simulating this mouse and the game has for this mouse is it has to, in just two moves for secure its reward that could be on the left or the right of the teammates.
And so it could either take a chance and find out where the reward is it has to stay in one of the upper arms.
If it chooses one so you're half the time it may be right and spend two bits moves with its reward the other half the time, it will be wrong, and has to stay with the non rewarded in the non rewarded location for its two moons.
So this is the choice and this is the important choice from the point of view of this little demo.
We've also equipped this universe with adiante queue and instructional queue that if it uses one of its moves to actually go and see the color of the queue down here, this queue tells it where the reward is.
It can choose to use the first move to explore to resolve uncertainty about where the reward is, and then the second move to go with a degree of confidence to secure its reward.
So from an expected utility point of view these are equally viable options but from the point of view of self evidencing with this epistemic affordance in play, it's going to be much more attractive for this little mouse to go and resolve uncertainty by going straight down to the informative
to simulate these this mouse under a particular giant in model using those update equations I showed you a couple of slides ago. That's exactly what happens.
So we start off with the little mouse going down finding where the reward isn't then sitting on its reward for the final move.
But as time progresses, these details are unimportant. The only thing that we're trying to convey here is, as time goes on.
The only thing that we're trying to convey here is leave the reward always on the left hand side. So that this mouse now accumulates the belief that the reward is always in the same place.
And as it gets more and more confident accumulates more and more evidence that the reward is always in the same place. The epistemic affordance or attractiveness of the instructional queue starts to weigh.
The epistemic value, the epistemic affordance will at some point become smaller than the expected utility or the expected cost of continuing to forage epistemically.
And at which point the behavior reverts to explore it to explore your behavior where it will go straight to where it now knows the reward is and stay there for two moves and we can see that occurring after about 20 trials in this particular example.
So that switch in behavior affords the opportunity now to do all sorts of interesting in silico experiments and make predictions about either neuronal responses to be chosen versus chosen option.
Look at the encoding of behaviors and locations in terms of place selectivity. And indeed we can simulate mismatch negativity paradigms or ball paradigms by just comparing trials where at the beginning, there was lots of uncertainty.
So everything is novel. Everything is an audible with trials with exactly the same behaviors and sensory stimuli or sensory outcomes.
But now after the mouse has become familiar and was bored with its environment. So everything now becomes a standard and again we can reproduce with some market remarkable fidelity the timing and the the forms of simulated
electrical physiological responses. So I'm giving these examples very very quickly just to show that they are both grounded in and to a certain extent inspired inherit from the neuro biological implementation of these this sort of belief updating scheme.
So I'll finish now with a with an important nod, but it'll be no more than a nod to what does this kind of formulation bring to the table when we're thinking about lots of agents acting together, all succumbing to the same kinds of imperative.
And one really interesting sort of thing that emerges technically from this is that we move from a world in which we're trying to think and explain the behavior of a creature or a particle that's making influences about it, the states of the world out there the environment through observable things and
over which it has control through active things and we refer to the these two kinds of variables as a Markov blanket. We move from a structure where there's a Markov blanket separating the internal states that do the brief updating
from the outside world that is supplying sensory evidence and is subject to our actions upon which we act. If we now think about two of these agents putting together, then if the external states disappear because you become my external states and I become your external
states, so there are no more external states are just lots of internal states, my brain states your brain states mutually trying to infer each other, and to cut a long story short, the, the, the natural consequence of this in terms of both of us trying to minimize our
surprise is that we will learn to make ourselves more predictable, and we will query each other in a way that we will resolve uncertainty about you are I will resolve uncertainty about you.
But I can only do that if we have the same shared narrative model.
I'm going to close with an example where we use the intrinsic motivation the epistemic affordance to compel two agents, one of which can see a scene and the other one can't, but they can talk to each other.
And because there's this epistemic affordance or incentive to resolve uncertainty, one agent knows because it shares the same language with the other agent that she knows what questions to ask to resolve uncertainty about the scene that she cannot see but the other agent can.
And it's quite simple just to write down the generative models and the equivalent graphical factor graphs for a very simple world where two people are basically playing, well, they're playing 20 questions and one person asked is, is, is something to is something like that.
They will say yes or no, and they will slowly share their beliefs. So in this example, the, the blue woman can see two shapes, two blocks as a red square below and there's a green square above this lady the tea lady cannot, but she can ask questions you can ask this lady is there a triangle
this lady says no there isn't. And then this lady knows that there is a square, but does not know the color. And through this process of making the right moves, engaging in policies that resolve the most uncertainty to buy maximizing the expected free energy.
She can ask the right question about the color of square below, and fans that it's not green so it has to be read and then she can drill down on the, the nature of the upper object until after about four questions.
She knows exactly what the other person's looking at it but they can switch roles and the other person can check that the first person has got the right idea.
And so the reason that I bring that to the table is to fold, want to emphasize the importance of this epistemic foraging in the context of interpersonal interactions and
the mechanics of exchanging beliefs. And the other thing is what underwrites this, which is the shared generative model that is necessary to actually communicate beliefs.
Things that are very necessary if you want to simulate belief updating in a distributed way that has to go across a Markov blanket in some ensemble or collective.
Now, what's on the other side of the Markov blanket doesn't always have to be the same, it could actually be, you know, an eco niche, but still there's going to be an exchange of information and querying when lifted to the cultural level.
And then we could actually talk about these, this formulation as a one way of understanding cultural niche construction. And then finally, if we now apply these principles over different time scales, we can also now think about sort of optimizing model
evidence through model selection as a mathematical image of Bayesian of natural selection and evolution.
So with that, I will close just to the final world to Einstein, everything should be as simple as possible, but not simpler with a not this important notion of complexity.
There is an important part of model evidence. So with that, I just want to thank those people whose ideas have been talking about the course. Thank you for your attention. Thank you very much indeed.
Thank you very much, Carl, for this brilliant presentation that get as simple as this candy presenting that complexity. And I really also appreciate that you squeeze our webinar series into your busy schedule after the one we have with
Konstantin. I'd like to ask a twin question before we get to the panel.
And that question is trying to my, I could say, let's say sophisticated lay woman or even just lay woman on understanding that I think I'm getting to get to of your Mark Marko blanket that is kind of a core aspects, which I believe is kind of the statistical
space that come that combine or differentiate what is in and out system. If I understand correctly, my two questions are the following. As you most likely don't know I was trying 30 years ago at Cornell at the time that be a
kind of mix was starting with with Dversky kind of man and the Taylor that was very descriptive vision of the non rational side of human behavior in moving toward precision convergence, I progressively get much more around along the work of
Herb Simon that is defining essentially bonded rationality as being resource rational within the context of your system. So my first question is to I'm intrigued to hear you connect your work with herb Simon view of bonded rationality and his approach and within that context also on the
panel that we were having on on one consenting was presenting is only a static view of self promotion and there was this debate between this model whether the psychological self promotion promotion and the the biological protection for
immunity and no one was trying to seize their cause, a common causal mechanism. So I would be intrigued in my question, my second question, if you could briefly comment on advancing the discussion we were having between the psychological, the mind and the self and the
biological basis within the context. And the challenge is answering this in three minutes.
I'll spend the first two on the first questions excellent questions so there's your Simon's and bounded rationality I love that because of course.
There's a nice semantic clash with an evidence bound. So if you go to machine learning and ask how does a variation auto encoder work. It optimizes its weights with respect to this elbow.
It's all in the bound so bounded rationality approximate Bayesian inference defined operationally as optimizing a variation free energy literally is bounded optimality.
The other thing that's nice about it, you know, is this notion of constraints, and I'm using that in the in the sort of James in the sense, but of course an important constraint is the uncertainty under which you're operating.
So, you know, that's why I was so keen to emphasize that you can tease apart your objective function into two bits. But remember, one bit is contextualized by the other so the base optimal rational decision making the unexpected
is always contextualized by a degree of uncertainty so there's a value of information that provides if you like a space or a constraint or a context for and that you know I'm reading that in the spirit of boundary rationality, where this is a computational
resource or an information resource. With respect to the homeostasis yeah I mean I love constant in stuff because you know it's just lifting this basic imperative for generalized homeostasis a generalized synchrony with your environment.
And from the physiology to the self modeling to the things that underwrite our behavior and speak to this really important notion that if it is the case that we behave in order to secure or realize the kinds of outcomes that we prefer.
And just to be base optimal in this generalized sense is basically to have that sampling bias and just to be to be intrinsically biased and if you weren't you wouldn't exist you know if you bring about the things that you a priori or expect.
Then that is a sufficient explanation for why you can serve yourself over time and of course that's just a statement of that in spaces.
Thank you so much. And so I would like to ask the panelists to all open their videos shifting to Sean, my co chair who can ask his privilege question also and stop the panel go ahead Sean.
No I'm not I'm not going to take that privilege because I'd like to hear what our panelists I'm very interested here with our panelists have to say so we'll go ahead and get it kicked off with our first round commentary.
I'd like to introduce Mark Bailey who is the vice president for vice president for research at CIFAR and a professor in the department of computer, the departments of computer science statistics and actual science.
Electrical and computer engineering mathematics epidemiology and biostatistics and biology at Western University so welcome Mark and looking forward to your commentary.
Thanks Sean and thank you Carl for an absolutely beautiful presentation. So you asked in your opening slide is there some information theoretic imperative that renders inference and decision making two sides of the same coin.
And then you've given us that this elegant and complex model of how precisely that that might be implemented.
And I'm a simple person so I needed a simpler questions I said I'm going to step back and ask how might such a mechanism arise. Why might we end up in this place.
And I suspect most of us here in the audience have had the experience of you're using your laptop, and it's doing a computation or programs crashing and suddenly it gets very hot.
And you know what why is that why is your laptop getting hot when it's working harder it's not making toast it's just moving bits around.
And so there's a fundamental disconnect because we still tend to think of the physical world in terms of mass and energy.
But, you know, since the time of John Archibald Wheeler, we know it's actually mass energy and information. So information is a fundamental, fundamental core object in our physical universe.
And so Wheeler had a tagline it from bit, you know from a bit of information, we can create something physical and in fact that bit of information is something physical.
And then in the 1960s roll flounder at IBM Watson labs, conjectured that erasing information is thermodynamically dissipative.
So if you take a bit of information and you erase it you clear that bit, that will release energy. This is controversial at the time. There's literature both supporting it and not supporting it but there's been a spate of experimental results recently over say the past two
decades that have really strongly confirmed that yes erasing of information is dissipative. So information processing and thermodynamics are linked.
So if you take this framework and what we know in theoretical computer science, I'm going to say something is not quite true so forgive me. The harder the computation, the more deletion is involved in that computation, and the more energy you need.
We could spend hours talking about what I mean by hard what I mean by needing deletion but just trust me on this one for now.
So the question I come back to is, would evolution select against unbounded computational complexity, which is to say unbounded energy requirements.
And I think that that that's reasonable to say would be parsimonious for for evolution to not prefer systems that require unbounded amounts of energy.
And so this question has been asked in the literature, Martin Novak, who is a famous evolutionary game theorist has a nice publication and PNAS about 10 years ago on the computational complexity of evolutionary dynamics and more recently and on topic for
Kazanichiv at Oxford as a beautiful paper in genetics with title computational complexity as the ultimate constraint on evolution.
Love it. Definitely worth reading. So back to the brain and to the presentation we the keynote we've just had.
The brain is Turing complete, of course, any computable function I can compute with my brain, give me a pen and and this big enough stack of paper, I can simulate a Turing machine.
But the real question is, during my my everyday life, whether I'm a mouse in a maze or or me giving a response to a keynote here. Do I spend most of my time in that regime of super linear recursive recursively innumerable functions.
Am I really doing hard computations most of the time. And the answer is, I'm not right we've all had that experience of driving on autopilot, and there you know there's a lot of systems that are working with very little energy there's there's not a lot of
of of hard computation going on. And so what if the strongest constraint, as Kednichiv suggests, on the evolution of brains is computational complexity.
Where does that get us well to push it I'm going to take an extreme absurd position. What if what psychologists call attention. What if Carl sentient calculus I love that term by the way.
What if consciousness, what we perceive as consciousness are actually primarily mechanisms for controlling complexity.
They're mechanisms for controlling computational complexity, and everything else, all of the richness I experience in my conscious life is a happy side effect of that imperative that is selected for through evolution.
And you know, I think those are actually pretty good mechanisms attention is a great mechanism for for controlling computational complexity. Of course that you know that was an absurd position to propose but I did it to invite conversation.
In reality there's going to need to be a balance between this need to keep computational complexity low, while also providing a robust behavioral repertoire because you do exist in an environment and you must behave in that environment so computational complexity is not the only
mechanism on which evolution is performing selection, but I think it's an important one. I think the role of thermodynamic complexity of neural computation has been understudied, and I, I can't prove this, but I'm going to think about it harder.
I think if you pull on this thread, you end up with a model that looks very similar to to what Carl has just presented here. Thanks john.
Thanks. Yeah, Carl, do you have a response? Just maybe a minute, a minute response.
A minute. I'd like to respond for about half an hour. There's so many great points there. Okay, in one minute.
Right. First of all, selection, your natural selection, the evolutionary pressure, attentional selection, all one theme, I agree entirely. It's complex. I mean, it's only half the story because you've got the accuracy, but under, under constraints that the accuracy about the same, it's all about the complexity.
So, you know, you can see that in statistics through Bayesian model selection, that you're trying to get the simplest architectures that will generalize so you don't want to over parameterize so even the pragmatic agenda of evolution and machine learning is driven by getting the simplest explanation,
possible. So, you know, there are all sorts of ways in which one could celebrate that. You know, I'd like to talk about Commonwealth complexity and universal computation, Janinsky Equalities, you know, so many right things have been said there but I've only got one minute so I'll shut up.
Thank you.
I didn't mean to stifle you I just we have a lot of other perspectives to get to.
I'll turn it back over to Laura.
Yes, and I'm very pleased to introduce Conrad.
Conrad, I apologize in advance for my awful French accent, according, I discovered Conrad's work in a paper on foraging with the idea of looking at dopamine mechanism that are finding where to go and then where to have to have your, your choice.
So that's, I'm extremely pleased to have him with us with his lifetime work on sensory motor and various aspects of nomads in model in Bayesian statistic and other is a German neuroscientist and professor at the University of Pennsylvania.
Yeah, so I will keep it here Conrad to leave you more time. Go ahead.
Three to five minutes. I know you have a lot to say.
So let's, let's zoom out just a little bit no like there's a broader endeavor in our field which is called normative modeling.
It is, it is used in important parts of neuroscience. It's also the basis of machine learning arguably what we do in that field as we say, let's try and derive the optimal solution to an existing product.
It usually works like that we where we look at the world at the problem maybe that the brain is solving or at the problem that we want to solve as as machine learners.
And we write down what our goals are it's usually like a cost function this is what we want to optimize not like get it to be precise most of the time, whatever your definition is.
And then we will usually have a set of constraints those things that we cannot deviate from so my category example compute is one of them there's many of her many others of them.
And then there's prior knowledge that we have about the world aspects of the world I think Carl, Carl related to pump the piece earlier.
And then what we do is we take this prompt statement and we kind of derive the optimal solution and if we're neuroscientist we compare behavior maybe to that we look for representations in the brain.
If we're machine learning people we basically just see how good we are at solving the technical problem that we want to solve so in that sense I think we're all sitting in the same boat.
Now, where I think there's a rather large disconnect between the way the keynote was thinking about it in the way I think about that field is from me.
Most of what happens is in the details of that interface so when I say go.
It's not that simple get it right. It's more like on which data set will we get it want to get it right under which circumstances, what will be the future tasks that we will have to solve with that.
It's not strange it's not just like you only have x computations, it's complex, or but the world not like it's not just a pp world it's a world with structure and causality and like all the different pieces.
So, from my perspective, all the interesting things kind of live within these components that we put into that. So for example, if we want to talk about about curiosity, you can say well, what are we curious about what we curious about causal
things because we believe that in this specific world in which we are causal things will generalize to future problems. We will talk about which things are relevant for us but I'm not interested in information generally.
I'm just interested in the kind of information with which I can give a great talk.
So, so, basically, for an hour and curiosity within machine learning which is a huge field with like hundreds of thousands people of people working on is all built based on different people making different assumptions that that interface.
So, here comes kind of like my difficulty at some level with the keynote know like at some level we've seen an ansatz and was it talk about ansatzology.
Now the answers isn't different to like how anyone in machine learning users said this, we can write the same problem in a few different ways, for example, it's been known since the 60s that we can convert optimal control problems into equivalent inference problems and vice versa.
Interesting thing. So that's just like the same answer it's just written slightly differently.
So, so, so from my perspective, it's not the answers per se that is important because we're utility maximizers expected value for the future. We all agree on the answers.
We agree on the pieces what can we put in what are efficient algorithms and so forth. So, at some level the thing that is missing for me in the approach is pushing these things at the interface what are the assumptions about the world what are the assumptions about the cost functions.
What is what exactly can I say about computation and bits and so forth. And that then in a way like no in that talk.
Dr. Fresden kind of relates to kind of anything everything that is close to my heart now let's let's let's let's let's be positive about that now like curiosity, sentence planning self evidence the shared narratives it has like all these things that mean so much to me.
We've only ever seen the answers for all of them we haven't seen. Okay, how much does it bias to make this assumption versus this assumption so everything that computer scientists are concerned with is in a way hidden because we only ever get to see the answers.
And I'm sure that that there will be at least a minute maybe more than that of rebuttal to what I just said. Thank you for letting me comment on it.
Back to you.
In two minutes.
You can take three.
Okay.
I'll split it one and two. So I think I think there are two key points here.
The first one is your the status of normative theories I think there's a very general important point there.
You, for me and normative theory would be your being able to write down an objective function. So, from an engineer's point of view, your if you can write down the objective function, then the game starts and then you try and find the best way to satisfy that under constraints and the the perspective of the free energy principle.
So that sort of backstories the active inference implementations takes is slightly different in fact fundamentally different it takes a point of view of a physicist and say, if this system exists, what is this objective function.
You know, if I was a, you know, a classical physicist I'll be looking for the Lagrangian, and what's the way of articulating the behavior systems that optimize, say, their action or the pathological Lagrangian well that those are just principles of least action.
It's a mistake that the free energy principle is sown as a principle of least action, because what he's saying is that, given the system exists, then it must possess this Lagrangian that has an interpretation given the Markov blanket as, as a sort of
computational free energy that you'd find in, you'd find in, say machine learning statistics generally. So, I think that's a really interesting philosophical issue I don't leave that and not say anything else.
The last minute will be the second I think, I think, pragmatically more important point that Conor was making he was talking about getting the details right, getting the, getting the questions right in terms of an actual application of something like the free
energy principle to a particular problem. I think it's absolutely crucial. And for me, that really is distinction between a generic application of the free energy principle to what, well, it's the generative model.
That's where all the difficult questions lie. The structure of that generative model, the complexity, the ways in which you prune it, is it hierarchical, what kind of message passing scheme is going to be in play after this kind of generative model continues.
So I'm really acquiescing to Conrad's sort of observation that there is something quintessentially vacuous about the free energy principle because it is just a method. It does not give you the answer in the sense that this generative model is fit for
other factors that satisfies exactly marks complexity constraints. Yeah, I was just thinking, you know, Conrad, you're talking about so simplifying making this more efficient, which is exactly, you know, the complexity part that you would use and say basic model selection.
But the point I'm making here, the free energy principle is just a method. It's not a solution. The solution is actually identifying the generative model that is appropriate for this situation.
That's the area of study.
Thank you, Carl. So I think you're the one to introduce a lot.
Yeah, it's my pleasure to introduce Dr. Lauren Ross, who is an associate professor in logic and philosophy of science at UC Irvine. So we look forward to your commentary, Lauren.
Great. Thank you very much.
This was a great talk and it's a pleasure to be here.
I have two questions and the first one has to do with causation. The second one has to do with constraints.
The first question is about the type of causality that's most relevant to this work, in particular the type of causality that's present in the generative model itself, and perhaps also the type that they suggest that humans have in mind when they're
reasoning or an organism has in mind when they're reasoning about the world.
So I'm a philosopher of science and of course there are heated debates about how to understand causality and the account of causation that is relevant to, in particular, these generative models.
There are at least two accounts of causation that are often discussed in the context of generative models. One of them are process accounts and the other are dependence accounts.
A process account of causation is viewing causation in terms of continuity.
Typically, that there's some kind of continuity from cause to effect. This is sometimes specified in terms of a spatiotemporally connected process, and in other cases the transmission of something from cause to effect.
Sometimes it's referred to as a mark, mark transmission. The entity that's being transmitted can be material or it can be energy.
So this kind of flexibility with what is being transmitted, the emphasis in this account of causation is that there's the transmission of something, and often that there's a kind of physical connection through the causal process or from the cause to the effect.
Now the second type of causation is a dependence theory. And in this framework.
The second type of causation is understood in terms of dependency relationships. So effects are outcomes or factors that depend on their causes.
And in this sense, changes in an effect depend on changes that you would make, do make or could make to the cause. And you can be completely ignorant of or indifferent to the transmission of anything from cause to effect.
Physical connection or you don't need a transmission of any kind of material, or perhaps anything else besides causal influence.
The dependency accounts are also related to control in a number of cases, the sense in which we think causes control their effects, or that causes are kind of switch that you could manipulate or intervene on.
So that when you do that you get control over the, over the effect. So the question here is, do these generative models have implications or suggestions for the account of causation that we as humans or that an organism is using when they're reasoning about the world.
Additionally, and distinctly, what type of causality is involved in the model itself. And then the second question is about constraints. I'm very interested in and I very much like the role of constraints in this framework.
There is right dimension of constraints comes up in a number of places.
And they are.
It sounds like often cited as things that are limiting a kind of space of plausible brain architectures. That's how they sometimes come up, and they can include things like real neuro anatomy, maybe anatomical connectivity.
Maybe these are empirical constraints, but then the generative model is also cited as a constraint. So philosophers of science are very interested in, they've become very interested in constraints.
They are interested in clarifying what they are in particular what role they play and explanation and understanding. And part of their interest is motivated by the fact that they seem like a unique explanatory factor.
They are not always causal, it seems like, maybe, in some cases, they are mathematical, or they involve a kind of law of nature, their thought to explain limitations and a possibility space which is different from how we standardly think of causes or kind of run of the mill explanatory factor.
So the second question is about constraints in your framework and in this model, if you consider them to be causal non causal, or if they have some other character, and just for you to say a bit more about the role that they play in the model and perhaps also an explanation and understanding.
Or however much time you want.
That's dangerous.
To the three to five minutes no more.
So it was a brilliant question and also artfully described to the majority of us who probably don't know what philosophically cause and constraint means but that was very limiting thank you.
So, while it's fresh in my mind, the notion of constraint in this instance, I would read that in, you know, with my physicist hat on as exactly the way you described which is a way of describing disallowed or highly unlikely states of being in some space possible
states of being that just is the generative model.
So, I've been talking about sort of active inputs and self evidencing with it with a strong theological license and cheek, basically.
But from the physics point of view that teleology is just not there. You're all you're trying to describe is the conservation laws or the implications of certain conservation laws that lead to principles of usually similar principles like principles of least action.
And what that basically means for describing something.
So, to say, you know, demarcate the thing from everything else and that induces Markov blanket but once you've done that, then the shape of that thing and the nature of that thing is just basically described by the sets that are the kinds of states that that that thing can occupy the things that he can't
So the whole thing is predicated stipulatively on a constraint specification of the thing that's and then that from that you then interpret put a theological interpretation on that constraint as a generative model.
And if you've got a generative model you've got model evidence and then you can tell a story about self evidencing, but in reality, it's, you know, it's just a mathematical as a calculus just to write down the shape of things.
So the shape is the constraint, and you can describe that probabilistically in terms of a generative model is literally the probability of causes and effects joint distribution over causes and effects, where you're very careful to specify what you mean by cause and effect.
And that's why you need the Markov blanket, because, you know, what's causing things in me, well it has to be stuff that's not me. So then you have to have a separation so we know that.
I'm just saying, introducing that distinction between cause and effect as something which is quite a big move that you've made as a physicist.
Because you have to actually write down how would you discriminate to in a cause and effect, you know, then you get into the game of the Markov blanket.
And the second thing was really interesting, I'd love to hear more about it.
From my perspective, it is a sort of local and possibly limited perspective, it sounds as if the two different kinds of causality are essentially those that are of the kind that a control theorist or dynamicist or dynamical systems theory or a physicist
can talk about where it's not so much important about space that set the temporal continuity aspect that is the, if you like the defining feature of dynamics of dynamics is holds causality.
So, as soon as you write down a launch of an equation or a state space model or a Markov decision process or a hidden Markov model, anything that involves time, any mathematical process that involves time.
You've got causality baited in exactly the way that you describe it. I have in terms of Mark transitions or transition probability matrices or sort of, you know, flows implicit in state space models as differential equations, all of them.
At the end of the day, our core causal structures in the, in the control theoretic sense, simply because you have described the system of interest in terms of either discrete or continuous time equations that you're talking about dynamics.
There's no other, you know, causality.
You know, your geratin model, if it's a geratin model of a process that unfolds in time, it is by definition, a causal model. In version of that model, basically it makes inferences, you know, of a causal sort, selecting the right model to account for that causal
model that you're talking about in terms of evolution or Conrad finding, you know, getting the, getting the, doing it under the right kinds of constraints. That is just basically using this principle of least free energy as a way, a method of getting out
of a causal architecture by inverting a causal model, which begs the question, what's the other kind of dependency causality. I can certainly see a sort of a glimpse of a connection with sort of pearl like causality for static models.
But that kind of do calculus and that kind of model that you'd find in things like structural equation modeling and, you know, his interventional calculus.
That's unashamedly about static systems and unashamedly about usually directed a psychic grasp that don't exist in the, in the, you know, in the real world.
And I'm not quite sure about that dependency status. And of course, if you do use that dependency sort of causality, which we all do when we report a correlation coefficient as scientists.
I think you're a very shaky ground from the point of view of the physicists who just deals with the processes that are not background free because they have time in them.
If you give a physicist time, then you've also given him causality. So that would be my take on that.
I'm not sure about those answers. It'd be great to discuss that.
Thank you, Carl. Before introducing Randy.
Just a quick comment on the, on the, the first three commentary, I was just finishing a reading a book last weekend. It's an historical novel from the Turing Alan Turing.
He is, he was a theoretical mathematician physician, then moved to do whatever he did in his life in his young life.
And one of his regret that the engineer had taken over the basic mathematician in moving forward in understanding decision making.
So hearing Mark saying that everybody, not everybody, but solving the Turing puzzle by computer is, is doable.
You are saying that you, as soon as you have a production, a production function, you can go ahead and optimize.
What I'm asking to everyone is what if you were having the same at the same time your heart of philosopher, mathematician and physician and understanding trying to understand this real world behavior in real world context.
So that's kind of my commentary as we get to the second round of this in mind. And also that commentary allow me to introduce very directly my Canadian colleague Randy Macintosh who is the director of the Institute of neuroscience and the whole technology and
at Simon Fraser University and very much as a neuro computational neuroscientist very much moving in that direction of trying to see how we can bring this precision convergence. Randy, go ahead.
Thank you.
Hi Carl.
This is almost like a PhD dissertation defense I'm thinking.
So we have to decide at the end whether you pass or fail, or there's a heavy revision.
I'm appreciating discussion because I'm getting different perspectives on the whole collective of optimization and objective functions and generative models, and how we can think about using them to address different phenomena.
And what I want to try and maybe throw out there for for discussion is, and you just mentioned this Carl about time being a factor in building these generative models so let's take into consideration for instance,
different time scales over which, which events that affect us can happen. So there's this event now which is happening over this the time scale of seconds.
And then our interaction which you illustrate at the end of your talk.
So our slower time scales that evolve that are sort of circadian rhythms we have very, you know, salient events, for example, we're all sort of sitting in the middle of this pandemic, which has a scale over several years and probably going to have a cascade
effect that happens over subsequent years.
So there are social cultural factors that obviously impact our own interactions impact our physical wellness but also our social dependencies and
this gets into the notion of multi scale models and how to envision these dependencies in some sort of generative model which could I guess use free energy as one of the methods.
I guess the question is whether you can build in that time scale interaction where free energy has to span both within temporal scales, but also between temporal scales.
And whether we start getting into this issue kind of like it's turtles all the way down then because we're kind of looking at using exactly the same objective function for different types of generative models.
But the general model we're talking about one case is more biological, whereas another case might be more sort of psychosocial. And I'm trying to sort of reconcile how to think about these various time scales in the framework that we're sort of
evolving over the course of our discussion today. So I'll stop there and we'll let the candidates reply.
Sorry.
He can say that. I don't actually I haven't actually got my PhD yet.
Ah, seriously, I haven't quite gone round to it. So, so I get one for the physical version so that will be quite nice. Yeah, you have time for all your time.
That's an interesting question. I can see, you know, that that's that that question is also going to come up in the context of sort of natural selection and, you know, later on.
So I think a really key point, and I'm sure you know the answer that the so the importance of the deal.
The commitment to a first principle account is that very much like most of these principles that they're fairly scale free or scale invariant, which does not mean to say the same free energy, or whatever other functional you want to use is
to optimize at all scales. What we're saying is the functional form, the method, you know what it looks like in terms of, you know, what is a function of what and the shape of those functions that strictly speaking Lagrangian is actually conserved over different scales.
Another lovely example of that is statistics is the fact that if you're trying to optimize, let's take a, let's take, yeah, let's take a say a variation or to encode it from machine learning that, you know, is built to change its weights to optimize the elbow the
free energy. So it's a canonical state of the art 21st century free energy optimizing machine.
But now say I come at this, I've got a choice of four or 400 very short encoded some of them with six levels some with different kinds of rectilinear, you know, activation functions, you know, different kinds of parameters.
So this was a model selection or a selective or sort of, you know, using the differential evolution or genetic algorithm. So what I would then do is score the quality of each of these variation auto encoders, and then I would apply genetic algorithm and do some sort of your selection.
So that's a simulated evolution natural selection. The key thing is I'll be using the same objective function in the sense that I will be adding together the elbow or taking the path integral over the time that I've the temple scale that I was doing my selection.
So that's a Bayesian model selection in statistics. So the key point here is not the same quantity that's being optimized in the moment as I'm inferring, or I am learning.
And, but by taking the path integral or you're lifting that to the next scale, necessarily has the same functional form, and therefore gives the scale freeness to the next level of optimization which in this instance, could, you know, I framed as Bayesian model
destructivism. If I was a biologist, this would look like natural selection, where I'd now be reading the free energy is adaptive fitness is simply self evidencing maximizing the likelihood of getting this kind of phenotype this structure in this eco niche, you know, doing this, doing this kind of
self evidencing or
behavior.
So that
formally speaking, what that requires you really to write down is a renormalization group and practically speaking I think I think the question has the answer in it that
whatever principle you bring to the table has to be composable with a separation of temple scales in the sense that, you know, I can do, I can be an artifact that optimizes, extremizes my free energy over the next 200 milliseconds and I could be doing sort of deception.
It could be over the next week as I learned to ride a bicycle I could do procedural learning it could be over the next three years and I could be doing neuro development or it could be transgenerational I would be doing, I would be doing natural selection, but at every point.
Each of the objective function has has if you like a mapping, via the notion of this scale freeness.
So that the optimization at one level is essential for the optimization of the level below and vice versa. So if I was like George Ellis and be talking about top down and bottom up causation now, which means I have to have the right
heritage as a phenotype in the right species in order to have the right kind of brain in order to do the right kind of learning in order to make the right kinds of inferences.
But on the other hand, I need to do the right kind of inferences to learn the right things in order to do the right development so that so I have about adaptive fitness so I can be selected.
So there's a circular causality that couples each of these levels.
Now your question is, can you usually write that down in silico and start to actually grow intelligent artifacts or simulate and model systems that are subject to these mutual constraints and the circular causality.
That's a really difficult problem, particularly when any one level starts to show things like curiosity and the like, you know, the computational complexity is is is, you know, has to be met but it is great.
This is a challenge and then we come back to complexity minimization keeping things as simple as possible.
Right.
Thanks Carl.
Great. And before I introduce our next panelist, I want to remind all the committee members to return your form about the dissertation so that we
turn that into the administration.
Now, my pleasure to introduce Dr. Martin Paulus, who is the scientific director and president of the Laureate Institute for Brain Research in Tulsa, Oklahoma.
So, look forward to your commentary Martin.
Thanks, Sean, and thanks Carl.
And I really actually quite enjoyed the discussion.
I have to comment on all the other comments before I get to my point. I really enjoyed what Conrad was saying and I think that resonates very strongly with me that Conrad was highlighting that, you know, the free energy principle and active inferences and
answers. It's a start to think about something. The difficult work is actually trying to find what's the right, what Carl, what you were saying, what's the Lagrangian for the things that we're interested in, how is it being implemented.
I'm going to say something before I ask the question, because I also want to make sure that I understand what you're saying.
Let me say a couple of sentences and then I want to formulate my question.
So, I understand active inference is a theoretical and computational prescription for normative behavior in complex environment that is fundamentally based on Bayesian inference.
And by you, prior probability distributions are used within the constraint of the free energy principle to arrive at posterior distribution that quantified the behavior to be observed.
That is most consistent with the underlying belief about the nature of the environment.
So that's my understanding, as I would say.
So now I'm putting my psychiatry header, you know, I'm interested in alteration or dysfunctional behavior.
If I'm correct with my premise of what I just stated and you can correct me and I could be wrong and that's what I want to, but if that is sort of the correct premise.
I think that this dysfunctional behavior can be conceived in this framework in this ansatz as stemming from an altered and possibly inaccurate belief about the state of the environment, or by a faulty integration of available evidence to form adaptive
distribution.
And so my question that I wanted to kind of just get your input on how might one arbitrate between those two different alternatives.
And maybe more in the biology how might these different differences be implemented but we probably won't have time to discuss this.
Because I'd also actually think and now I'm becoming becoming sort of more real practical.
Because A and B could mean different things for remediation or intervention.
Right, because one of the things that we want to do if people, because I'm a psychiatrist or animals, if you're an animal person, who behave in such a way that we would agree in some form is non adaptive we want to help them to become more adaptive.
And so the dysfunction is based on fixed prior probabilities about their environments or faulty computation, the remediation might be actually quite different.
And therefore, you know, we need to find almost ways of arbitrating between those two alternatives and so I'd like to get sort of your perspective on that and how would you go about doing that.
Yeah, well you know how I do I do it like you're doing it.
A tricky question.
So, the way I resolve this mathematically as it's a really interesting question but I sort of deflate the question the problem is, you know, is this broken inference, or just bad prize.
Yeah, bad prize with great message passing schemes or broken message passing schemes with good prize.
And that's a really interesting distinction but practically I don't think it's terribly, it's terribly meaningful from a practical point of view.
So I dismiss that with something called the complete classroom and the complete classroom just says for any pair of observable behavior and some loss function.
There are some prior beliefs and render that behavior base optimal. So what that allows me to do is say that everything that's broken can be cast as a, as a, an unusual prior.
This prior is so unusual that it looks as if it's a faulty message passing system so say I had a prior belief about attentional deployment and precision.
And while I'm here just open brackets because this is a lovely connection between precision convergence and precision in the sense of Mark and deploying attention.
And indeed, from Martin, your point of view, you know, deficits of attention and deficits of consciousness are implicit and deficit of attention.
So precision here getting the precision right these sort of inferences about the variability or the, the width of probability distributions in terms of the encoding of uncertainty.
So the way that I had bad prize about that what would that look like when it would look like I had an attention deficit disorder that would look as if I had faulty evidence accumulation and assimilation that would look as if I basically had a broken imprint scheme.
So what I do is, I don't worry about it.
What I'm going to say is the real job. And then you come back to your second I think key point. The real job is to find out what part of this deeply structured hierarchical generative model that is a generative model of dynamics is broken.
Sorry, not it is broken is that is described by, you know, by its priors and formal prize in terms of its structure. What part is abnormal. Yes, broken in this kind of person on this disease relative to another another disease.
I think the question you're asking, of course, it is exactly the practical question the Conrad was referring to. And just to pick up on one, you know, your excellent summary of active inference.
If you want to make an application, the free energy principle, you're absolutely right. It is just a prescription. It's a prescription as a method of weighted things. So just like Hamilton's principle of least action, it's a method.
So you give me a particular ball and a particular sort of cannon, and I'll tell you the trajectory. But before you do that, it's nothing for me to do.
But all the heavy lifting and all the hard work is basically finding the generative model that under the assumption that, you know, you can now apply the free energy principle variational inference of an inactive sort or active inference.
And then that provides a good account or explanation of this person's behavior. Then then the job is just a normal clinical neuroscience problem of finding what part of this narrative model is different.
And by this part, I mean the particular prize and usually hyper priors. And they usually, as you will, you can't respond because it's my turn to talk but what Marty would like to say.
There are these higher order priors whose biological substrates are probably tied up with neuromodulators and synaptic excitability and all the physiology that goes hand in hand with that sort of fussed, you know, synchronous interactions and the like.
So a very specific class of part of neurobiology that deals with modulators and particular kinds of neuropharmacology. And this I think is a perfect example of actually sort of say what of the universe of this deeply structured generative model that, you know, is not only describing a person but in an encultured context.
What's remarkable about this prior relative to this neurotypical prior. And that's a really difficult game, obviously.
Yeah, I actually had a second. So, so even though this is already been a very fascinating discussion for me, I actually had a second question because we were asked to have two questions I do.
I want to, that's a little more edgy, not edgy for you but edgy maybe in the sign of times, which is the fact as you, you actually have now expanded which is very, I just can't wait to see what you're going to do with the message, a message passing scheme between active inference agents,
because now we're talking more than one agent. And you gave the nice example of the 20 questions and how one can develop a common framework to then arrive at the correct solution.
But of course, again, coming back to the fact that I'm interested in when behavior goes wrong. We live in a time. For first of all, let me just say that, you know, even psychiatric disorders, or abnormal behavior, individuals are not islands.
They're embedded in, in communities. And these communities interact heavily, and therefore doing the kind of message passing that you're talking about.
But coming back to first principles, you know, we live also in a time where it seems like that some agents together are very good in message passing, and other agents are very good in message passing and arriving at some consensus.
But then there is nucleations that seem to be occurring and they seem to be getting more prominent and more severe in, in the current time, where there is very little message passing, going between those nucleations.
So the question would be, how, how would you understand that as emerging from sort of an active inference principled agent that interacts with other active inference principles agents.
That's a great question. And I suspect it probably will probably just use that question when we come back to sort of round summarize ways forward.
So the way that I would do that is basically by numerical analysis, and I would write down a little world and see what emerges when each member of a population or ensemble is described or its behavior is prescribed by by active inference.
And what I would imagine would happen is that this dialectic between trying to make your world as predictable, predictable, and at least surprising as possible, whilst at the same time trying to share your generative model with as many artifacts like you that are generating the things you're trying to predict,
that will be resolved by this enucleation and this sort of blanket building isolationism, if you like, sort of partial isolationism that characterizes so many aspects of our sort of political and geo and your familial cultures and structures that there is, again,
a real invariance that does entail sort of a mark off blanketing or blanketing or segregation or partial segregation that fulfills that within your blanket within your family within your ethnic or political or religious, you know, culture.
There is a shared narrative, and everything is nice and predictable.
And that that that sort of encapsulation is underwritten by something we were referring to before, which is this base optimality in terms of bias sampling.
If I act to realize the kind of world I think I'm built to self evidence in, then I'm going to construct to the extent that I'm going to be successful in so doing, collectively, then those boundaries will will will be maintained.
The question now is of course you know what is the scale and the structure and the number of these of these communities that are emergent properties, very much in the spirit of Turing pan formation that you know we're talking about.
We're talking before about Alan Turing. I mean this is exactly the kind of segregation and we actually diffusion like phenomena.
He wanted to understand. And I would imagine the same techniques will be used a century later, and by which I mean he started off by basically doing by hand Excel spreadsheets and looking at sort of local exchanges to look at yourself organization and
pattern formation, we would do it in computers but actually probably just at the same speed because we'll be actually modeling active inference agents that actually are modeling everybody else in the eco niche.
And who knows what will happen, but hopefully for a sufficient number of scaling variant aspects to this procedure, you'll see what tends to happen in your own brain that you actually get functional specialization in the context of integration.
So you get the structure processing but there is segregation of function. And, you know, you can chase that right down to the structure of a new one right up to the structure of your body.
There are symmetries but there's also specialization. So I imagine that there will be the similar kind of structures emerging that will be describable in terms of the dependency kind of causality through Markov blankets.
That one could be looking for in silica.
Thanks, Carl. In the interest of time, you're right. What you just said is essentially the overall one table. But in the interest of time, I would like that we into, we get the commentary of the, of the last two presenter, and you integrate your reply into, we will start our
round table with you, Carl, for the last. So if it's okay, and Daniel will be presenting their commentary and then we'll start the round table with you and you'll answer to those two at the same time of the general question.
How do we go for this real precision?
So back to you to introduce a line.
That's my pleasure to introduce Dr. Elaine dagger who is associate professor at the Montreal Neurology Institute and McGill University and neurologist specializing in movement disorders and functional brain imaging and little known a fantasy American football aficionado.
Thank you. So thank you, Carl for a really inspiring talk. And I have two related questions or comments. One has to do with what you refer as your day job functional neuro imaging.
We've been able to measure blood flow in the brain for a few decades now. And in the early days, we just ask questions like what does what.
One interesting prospect is to try to measure information in the brain, and specifically to ask how is information processed in the brain. And one way that you have promoted is to try to quantify cognitive processes and then relate them to brain activity and I wonder what you think the
aspects are for for this area. And specifically I'd like to ask you about dopamine, which encodes information in multiple ways.
And I'd like to specifically ask you about tonic dopamine, because it seems to be related to motivation and to the willingness to act in the world.
There's low, low tonic dopamine. So that's the dopamine that basically bathes neurons. These patients experience apathy, despite the fact that they still enjoy life and they still have the ability to experience pleasure, they're not willing to act.
And conversely, when we increase their tonic dopamine with medications we can trigger states of maladaptive action, such as pathological gambling and compulsive sexuality.
And I was trying to think where tonic dopamine fits in the free energy principle and I was wondering whether it could encode expected information, and that that alone could explain its effects on apathy and motivation.
Quite interesting. Do you have another question, Alain, before I introduce Daniel?
No, that's it.
Very good. Let's shift to Dan Belsky, who was wondering why we had invited him on the panel and it was very, very carefully selected for Dan is working from the genes to society.
He is working on issue of aging, life course, socioeconomic status that are so important if we are to create this next generation science of real-world behavior in real-world contexts in a more adaptive manner.
I'm his assistant professor of epidemiology at Columbia University, Mailman School of Public Health, and also at the Robert Butler Columbia Aging Center.
So Dan, why don't I let you through all the space we can get. Go ahead, Dan.
So thank you so much for having me for the invitation. It was a fascinating talk and equally fascinating questions from the panelists. I've learned a great deal.
And in fact, the questions that I initially wrote down to pose to Carl, which were about how this model speaks to the development of individual differences and in particular how that happens across time scales of human development.
across the lifespan, have been partly answered in the responses in particular to Randy regarding time and to Martin regarding the genesis of individual differences.
So in service of transitioning us toward that overall panel, let me pose the more general question to Carl from the public health perspective since I'm here from the Department of Epidemiology and I know you've been thinking about public health and epidemiology in the context of your models recently.
How should the free energy principle inform the kinds of strategies that we are developing to shape public behavior, private behavior in service of cultivating health for individuals across their lives and that's a,
you know, maybe the opposite end of the spectrum from Alan's question, which is, you know, very specifically about a particular mechanism that you've thought about. And I guess I'm asking more generally, not what does this say to, you know, sort of public health in general but what are applications that you might envision
of the free energy principle that could guide us toward a more precise approach to public health and social policy.
Then where we stand today. And perhaps, you know, we can have some further interaction in the subsequent Q&A.
Thank you very much, Dan. And I would like to make sure that all the panelists are having their video on. We have 12 more minutes in those 12 more minutes we will have Carl addressing those first to
last two commentaries, but within the broader perspective, Carl, of your words of wisdom, how can we as a worldwide community move toward this next generation precision convergence, convergence science.
So, Carl has two or three minutes and each of you will have one minute after and then Sean and me will be concluding. Go ahead, Carl.
Thank you very much. In fact, Dan, your question and Elaine's question are actually, I think of the same milk and like really important and also speak to what we were discussing with Martin, which is, you know, you've got a principle, a recipe.
How are you going to make it work in a useful way at different scales.
So, you know, in answer to Daniel's question, one can drill down on the individual on the single person in terms of their neurology and their psychology as an individual and try to understand the functional
objectives and the physiology of the belief updating and the, the message passing neural message passing very much along the lines that we're discussing with Martin.
So this basically would be understanding neurology, psychiatry and indeed behavior as a barren to belief updating from the point of view of active inference.
And because you've got this process theory underneath the hood that grounds it in your biology, you've also now got a way of articulating psychopathology in terms of pathophysiology things that are, you know, that are a barren highly unlikely in terms of the physiology and the dynamics and the structure
of your anatomy of a particular brain. So that, you know, that is, if you like, you're already an active research part of say computational psychiatry and computational neuropsychology you just build things in silicone, you see you make
sure you optimize them so they behave exactly like your patient, you do a bit of precision medicine, just by nuancing and adjusting and optimizing the prize of this twin digital twin if you like.
So that complies with the method of the free energy principle. Then I think you have to move up to the next level which we already talked about which is the, the exchange between agents and issues of data foraging not data.
The information foraging epistemic trust static interactions therapeutic engagement. Martin, if you have time would speak to this, you know, how can you use these principles to predict the kind of priorities that people will have it will commit to the ensures a therapeutic
interaction with the doctor say or with the public health service these are really important issues. To understand the mechanics of the belief updating in terms of interpersonal inference, maybe really important in just supplying public health care.
That alone, you know what that care, the content of that care. And then at the next level we're talking about epidemiological modeling.
I haven't thought about that yet in the sense that when we model viruses, and model people, the vectors of those viruses, the people are just like points are like this is a physical cow.
And in themselves have any intentions or priorities. That's an outstanding issue and I repeat, I think that's where we're going to end up.
I won't finish that because I want to just comment on Alan's. That's a great idea. I haven't thought about that.
Sorry, dopamine may well be the encoding of the expected information gain of the, you know, the epistemic value.
I think it's entirely licensed because the speaking to the exchange with Martin.
What we're talking about here is prior beliefs about the precision or the confidence or the certainty with which you subpersonally your brain holds certain beliefs.
And that's for every probability distribution, every probabilistic representation in the brain, there is a precision that may or may not be encoded implicitly or explicitly with a neurotransmitter.
So that certainly speaks to a plurality of things that things like dopamine might encode. We certainly know something to do with plans and action.
And beyond that, I think that the, the hypothesis it could be expected information gain is exactly the side kind of hypothesis that you would then use simulations to test by, you know, playing off expected costs, for example, against expected
information gain. Factorily, and seeing which was the best account for dopamine in this region as opposed to the responses in that region. So that's an excellent idea.
But we'll be part of this basically building a brain in silicone optimizing using the principles we've been talking about to, you know, to drill down, you know, in a precision medicine kind of fashion, which would be one end of the scale from a public health perspective.
The scale being where I'm going to finish, which is the agenda that days.
You want to go with your one minute commentary on the where we go from here.
Well, in a sense, I but the last, the last 30 seconds was meant to be, you know, I think we've identified, you know, where the challenges are and it's basically something that everybody's spoken to at some level from Conrad, you know, the complexity of it all from Randy,
how do you get the multiple time scales from, you know, Daniel, you know, how do you how do you get this to the public health it's all about basically self organization of sentient artifacts at different scales.
So it is exactly collective agency. So that's the big challenge.
The last thing I'd like to say is, I hope I get the PhD.
Absolutely.
When you get many postdocs. So let's let's have this 30 seconds to one minute commentary for each of you, you can you can vote on the PhD, but most importantly, I want to hear from you.
What do we go as a capacity building world wide community in the next six months of a year.
Mark, I will follow the same order than we are in the panel. Mark.
So Alan Turing keeps coming up in the conversation today and I don't think that's an accident Turing's worldview included the theory of computation. And I don't mean digital computers in your laptop I mean the theory of information dynamics as a unifying theory of everything and it permeated all of his
laptop. And I, you know, I don't want to talk my own book here but but I think the theory of computation offers fertile basic ground just like set theory did a couple of centuries ago for mathematics to create a common language and a common set of models.
Thank you.
Well, you're muted.
I think building this, the underlying theory for machine learning is very, very important and more broadly from your science, which means if we're in these normative spaces, oftentimes our algorithms do a lot of arbitrary things and I think there's a lot of wonderful recent
developments in the area where you can then prove that say we'll generalize for a certain set of phenomenon like packed and thinking of ideas like pack base.
And I think, like in the interface with theories of how the world was that they depend on all these pieces that we come in that we put in. And I think there will, there's this a lot of development that we need to make these ideas be concrete.
Thank you.
Lauren.
We may have lost Lauren. Okay, Randy.
I mean, what would I take away from this is that there is a lot of work to be done and thinking about how to build the generative models that we need to answer some of these questions.
Conrad posted some of the examples of the application of free energy in the domain of COVID modeling.
We've done something similar using merging agent based models with some sort of sIR models. And that's also a generative model is the question of the objective functions.
I think this is a domain that kind of extends beyond just thinking about the, the tool itself but thinking about the application and where the application can take us and that's, I think, been a constant theme.
So it's not just so much thinking only about free energy per se but also, you know, what do we want to understand and can we use free energy to engender new understanding of these problems.
Thank you, Martin.
I want to keep it very brief. I want to thank Carl for challenging all of us to think harder about, you know, how behavior emerges and how it breaks down.
Again, I'm excited to see what multi-agent active inference will look like and what, you know, what patterns emerge and how we can potentially use the insights to make the world a better place.
Thank you.
Okay, thank you. Well, I think that the challenge is to see what is a vulnerable brain at least in medicine and understand how genes and environment conspire to create a vulnerable brain and to understand that vulnerability at a computational level.
So that's the interesting challenge of the next few years.
Done.
I mean, I think I would just echo Alan, I think, I think I was inspired by Mark's initial comments about the idea of information as a fundamental element in the universe and genetics is an encoding of that information and so the way in which that information interacts with the
information of our experience to produce variation in behavior is the frontier. And I should also just say I'm grateful that Lauren has returned and so we can give the last word to the philosopher.
Go ahead, Lohan.
Oh boy. Well, yeah, so I, I mean, Carl's work is already bringing together many approaches and I think this is particularly useful for many questions, especially ones that are kind of on the edge of discovery and in some sense the edge of science.
So where do we go from here? I think we can kind of capitalize on this and do more of this, having more discussions, asking more questions and more attempts to answer them.
Thank you. Thank you. Shifting to you. In fact, Lohan, shifting to you, Lohan almost used the expression you were using. What we are doing is bridging the edge of science. I will let you speak for yourself or your commentary but also introduce our next precision convergence.
That is, in fact, a very nice build up to what we were having today. Go ahead, John.
Yeah, I gotta do it quick because I have to go meet my Dean. But thank you for the wonderful talk, Carl, and thank you all the panelists for a mind expanding conversation. I feel like, you know, you're talking Lagrangian's message passing free energy.
It's been a quantum chemistry talk again, which is my background, but it was really interesting and stimulating talk and our next presentation will be, I believe, on May 24.
Carl, Kitty is putting the poster in the chat.
And Satya Ghosh from MIT will be talking about basically how neuroinformatics is facilitated by infrastructure development. He is the lead on an infrastructure project called Dandy, which is a microservices architecture for bringing together neuroscience pipelines and data.
So a very different talk, but still interesting nonetheless. But thank you again, Carl, for what a very wonderful presentation.
Yes, indeed. Thanks to Carl.
We very, very much appreciate you placing this in your business schedule and each of you on the panel, and those in the audience also.
We all know what two hours mean, and we appreciate very much. Not only the time, but the brain investment in the last two hours is indeed building this capacity for precision convergence, which is this edge of science connecting all of the above for real world transformation in as real time as possible.
Thank you, everyone. We will end here. And we hope to have you on the next one, but most importantly, it is, it is a community we are building so we will be getting in touch in that in diverse manner.
Thanks to all of you. Bye bye.
