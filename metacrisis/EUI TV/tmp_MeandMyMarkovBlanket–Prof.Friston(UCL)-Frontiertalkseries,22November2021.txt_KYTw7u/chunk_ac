data you know visual data that is caused by things out there people words cars visual objects
you have to make sense of that visual data through this hierarchical message passing usually in the
posterior part of your brain initially but there's another kind of data we also have to make sense of
and that's the data that reports the states of our actuators the levers that we can pull
to influence the outside the emails that we send the instructions that we give to those people who
have to interface with things beyond our mark off blanket either institutionally or in terms of our
embodied brains and these kinds of data come in and we could have we could form a prediction
error we could use that prediction error to update our beliefs about the deployment of our
actuators our our secretion organs our muscles or indeed the people that we we send out to
deliver our goods for an institution but there's another a much simpler way to eliminate those
prediction errors and that's the the action part I can simply send these prediction errors back into
the outside world to cause movement until the signals from my organs of movement by organs of
my actuators are reporting yes they're in the predicted position so this is how our bodies
work these are known as motor reflex arcs in our bodies but also I would I know in robotics this
is how robots work you predict what the robot should feel in terms of its senses that are attached to
its activators actuators and I would contend this is basically how anything works that basically
is fulfilling the predictions where you set targets for what you'd expect to get feedback
from what's out there in terms of things you can influence and all of this is just another way of
leveraging the other way of minimizing prediction errors which is this active
acting upon the world to produce the right kind of sensory input
so I've got to conclude now just by drilling down a little bit more on that sort of free
energy functional because I want to connect it to economics and choice and the way that we
forage in the world for particular outcomes and the emphasis I want to place now on the
the final few slides is it's not just about minimizing surprise in terms of maximizing
the probability of encountering the outcomes that characterize me it's also about minimizing
expected surprise namely uncertainty it's also about exploring to resolve your uncertainty
about the world in which you operate so the first example is just to illustrate a very fundamental
way of eliminating surprise which is the surprise that my sense organs and my actuator organs are
not actually in the position that I predicted and you can use this very simple kind of active
inference prediction error minimization both in the perceptual and the active domain to simulate
a lot of quite high end behaviors and neurophysiological phenomena that we see in the in the brain
sciences or the neurosciences this example is just to simulate handwriting so all we've done here is
equip this little synthetic brain with a central pattern generator that is generating predictions
of both proprioceptive or motion sensations and sensory and visual sensations so
specifically here the synthetic agent has a particular belief that there is some motion
in the outside visual world that controls the position of an invisible point
and that invisible point is connected by a spring to the tip of her finger so what she expects to
see and feel is her finger being pulled around in a particular orbit that is prescribed by this
central pattern generator so that she will reflexively fulfill the expected proprioceptive
or motor signals by reflexively moving to produce exactly the predicted movement sensations whilst
at the same time generating the predicted visual sensations so what one has done here she can
author her own sensations and do that actively so it looks as if she's writing and here's a very
simple bit of writing here I won't go into the physiology that or the synthetic or simulated
physiology this color simulation offers other than to say there are some cardinal features in
terms of place selectivity in terms of direction selectivity in terms of a dual role of these
internal dynamics these internal states in generating the consequences of action whether
it's you or not you so you can use the same brain machine with the same genetic models
to explain the action not only of yourself and to actually emit those actions reflexively
but also to explain the action of other people and for what their intentions were and what they
what they are actually doing that's known as the mirror neuron system so mathematically the
architecture that we've just used to produce this very simple demonstration of active inference
is summarized in in this slide here where we have the outside world supplying these sensory
signals that are used to minimize to optimize our expectations and beliefs about the state of
the world the external states here by minimizing this prediction error or technically by doing a
gradient descent on the prediction error or the gradients of the free energy minimizing the free
energy and then when we got the right expectations the best expectations at hand given our sensory
input we then can generate predictions about what we would feel and sense in terms of our actions
and our actuators and then we minimize those prediction errors by acting upon the world
and we get this reflexive kind of behavior engagement and synchrony with the outside world
but there's a much more sophisticated and I think plausible way of prescribing action
that involves equipping these narrative models with a model of the consequences of my behavior
and I think it's these kinds of model which are much better for explaining the way that we
decide what to do next through planning so this is a more sophisticated generative model
it relies upon exactly the same Bayesian mechanics but the difference now is we're talking about
generative models that have acquired a temporal depth in the sense that they can see or predict
into the future and in particular the consequences of action on the world and as soon as you do that
you now basically have machinery to plan and what does that mean well it simply moves as before
we are taking sensory inputs where building through this minimization of prediction error
of free energy a leafs about states of the world then we're going to roll out into the future
under different courses of action and evaluate the free energy that we would expect if we committed
to this course of action and then we're going to use that free energy to score the quality or the
goodness of any particular action and then take the best action that has the minimum expected
free energy and we then prosecute that action as part of a plan so this is much more a long term
intentional kind of action in the sense that I have to select amongst a number of different
actions each of which is scored by the expected free energy so again forgive me for the equations
this is the last slide with heavy equations and again that you don't have to know what they mean
all I want you to know what this slide is meant to convey is a beautiful symmetry between the free
energy that's that if you like underwrites our sense making and our perception and the free energy
we expect under a given action aid here in the future so they have exactly the same functional
form but these are just expected versions of these so what are these well it just allows me to
qualify why you know the why this is called a free energy it basically comprises an entropy
term and an energy term that can be rearranged and understood from a number of different perspectives
if if I was in machine learning I would read it as a mixture of this thing that we want to
maximize which is the probability of sensory outcomes given my majority of model or me
that is equipped with something that can never be less than zero which means that this thing
is always a bound on the thing that I want to optimize I phrase it like that because if you
did we went to machine learning then this would be the objective function used in things like variation
autoencoders or deep learning it's a very it's an evidence bound you know and if the free energy is
a negative this free energy it's an evidence lower bound known as an elbow in machine learning
if it was a statistician I'd have a different reading of this just by switching around these
terms I would interpret this as trying to find the most accurate account of my sensory information
in the simplest way possible by minimizing the complexity and what is the complexity
well it's just the the degree to which I change my mind the degrees of freedom I use up we're
moving from my prior beliefs to my posterior beliefs after observing the sensory data so
it's the complexity scores the literally the degree to which I change my mind and that has
a computational cost and an energetic energetic cost so these are just two ways of carving up
this fundamental objective function here that survive when you take the expectation of it
and they basically survive in the sense that the expected complexity of any move on the world
becomes risk and the expected inaccuracy becomes ambiguity in the same way the expected bang
becomes an intrinsic value and the expected log evidence becomes an extrinsic value so at this
point I'm hoping that many of you will recognize what these things are when formulated in the language
of of your field so if you were well let's just look at intrinsic points ignore extrinsic value
for the moment and ask well what is this intrinsic value and what it is mathematically is a measure
of the change in my beliefs my expectations afforded by the sensory consequences of action
so it's the information game that I get if I did that and in the visual search in the
neuroscience this is known as a Bayesian surprise more generally it is also known as a mutual
information between cause and consequence between the external states the causes and the sensory
consequences that would ensue if I made this action so I'm always acting in a way to maximise
the information transfer between me and what's going on out there also known as intrinsic
motivation in robotics it's the part of this free energy minimisation that expels artificial
curiosity and base optimal design and in statistics and another way of looking at this as well let's
make life a little bit simpler let's assume that I can see everything in the sense that my sensory
impressions just are the causes so there's no ambiguity there's no uncertainty about the
states of the world the sense states of the world and the actual external states so this
term here just disappears the accuracy becomes unimportant and I'm just left with the expected
complexity and the risk so what's that well it's another what's known as K. L. Kolbakliber
divergence between two beliefs beliefs about the outcomes of action and my prior preferences
the the states that I prefer the attracting states a priori I would expect to encounter
so it's just a qualitative measure of the difference between what I think will happen if I do this
and what I want or expect to happen or prefer to happen a priori and in engineering this is known
as KL control in economics you might recognise this as risk sensitive control that includes
the kind of uncertainty that attends taking this action or this investment with respect to
another one I can make a final move here and come take all uncertainty off the table other than
that which is bound up in terms of the the extrinsic value so I assume that I I know everything
that I need to know all the reduced blood certainty about the world has been resolved
through epistemic foraging or responding to epistemic affordances I've watched the right
news channels I've read the right books I've checked out the right Wikipedia pages I know
everything I need to know no more active information seeking behaviour is required
then what am I left with but I'm just left with this in extrinsic value what is this
it's just what where we started it's the expected utility or value of an outcomes under a particular
action and that again in economics could be read as basically expected utility theory
that inherits simply from this generic form and this perspective on planning as inference
under deep generative models trying to minimise this expected variational free energy
a final simulation to show you how this works in practice in neurobiology
this is the visual search if you like illustration of this this kind of epistemic foraging
that acknowledges that when we actually look around the visual scene we can actually sample a
very very small part of it we think we see everything because we've done an internal
scene construction through this active inference but in fact we actually see tiny little points
or little circles here of information at any one time we have to build a picture by deploying
our visual palpation to the right parts of the visual scene to build up a confident
belief or expectation that I am actually looking at this kind of thing and you can score that in
terms of the expected free energy on the salience if you like of centering my visual sampling here
or here or here or here build up a map that in the visual neuroscience is known as a salience map
that empirically does seem to attract our actions and our eye movements in the sense that we always
look and deploy our active vision in a way that resolves the most amount of uncertainty
or has the greatest information gain and we can simulate that this is the generative model
that we use it's very simple little hierarchical model that just uses that
Kalman filter or that gradient flow that inherits from the the Bayesian mechanics
and we can simulate visual origin so this agent had a very simple universe that could either
be in three states with an upright face a sideways face or an inverted face and yet the
agent she can only look at little parts of this visual scene and has to decide where to move and
look and she does so sequentially every 250 milliseconds or so looking here then here then here
then here and at each point changing her beliefs about what would happen in terms of the salience
of the expected free energy namely changing the salience map as she resolves uncertainty and
exhausts various parts of the the visual scene in terms of what information is available as scored
by these progressive salience maps here that's driving the eye movements that are soliciting
these inputs here and dynamically down here and here is the belief updating this the evolution
of the internal states that are enjoying informative information that minimizes the
expected prediction error the uncertainty building up confident beliefs about the
the state of the outside world which was indeed an upright face so we see that here with confidence
intervals and about the blue hypothesis as she rejects the the other two hypotheses so to my
mind a really nice example of the importance of epistemics and getting information result being
being curious creatures and that kind of behavior being prescribed from the basic
physics of self-organization under these attracting sets given I can be separated from my world
with a Markov blanket but put much more simply by Helmholtz in the following description every
movement we make by which we alter the appearance of objects should be thought of as an experiment
designed to test whether we've understood correctly the invariant relations of the
phenomenon before us that is their existence in definite spatial relations and with that
I'll close it only remains for me to thank most people whose ideas I've been talking about
and of course to thank you for your attention thank you very much indeed
Thank you very much Professor Fiston that was amazing and if we were with a live audience
physical audience we would probably have a big round of applause for a very insightful
and deep presentation so I myself didn't follow everything and certainly not the technicalities
but I took a ton of notes and I have questions but I would like to hand over to the audience
in the room to ask you some questions or make comments so is there anyone willing to to take
the first the first question
Check over
Yes Nicola and so many people say that rather than a round of applause at least on my side
would we really stand up and thank you for you for this amazing presentation that really
for me as an economist has been a really a discovery of many things that
somehow were disconnected in my knowledge and that you were really able to bring together
to unexpectedly to bring together so thanks a lot it's I can really say it's a mind-blowing
presentation thank you I really have many probably too many questions and so I would
like just to start to break the ice and certainly give the floor also to my colleagues and friends
so I really don't know where to start so let me start with something that probably
in the audience is maybe of some interest to some extent I would say that you base the
a substantial part of your presentation in terms of focusing on descriptions
of the systems now as political as social scientists we are often
involved into policymaking we are asked to suggest
in policy improvements or new policies just thinking about for example the
European Commission draft on regulation of artificial intelligence
and so and the first question I would ask I would have for you is
what would you suggest if you were to move more into the prescriptive
implications of what you presented us or as economists would say the normative part
of what you presented us so the little answer I would say myself is that curiosity
is of the essence here right but I really would like to hear from you
well you've you've just answered your own question I think that's absolutely right it's
curiosity and interestingly that one thing that curiosity is really what is missing in
much of artificial intelligence research and machine learning generally you know because
people have committed to a focus on utility and reward functions that do not come along with
