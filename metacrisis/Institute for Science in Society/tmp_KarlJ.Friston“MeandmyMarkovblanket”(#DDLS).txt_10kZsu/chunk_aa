Okay, so I would like to ask everybody except for the speaker to mute themselves.
So welcome to the third session of the Dutch Distinguished Lecture Series in Philosophy
and Neuroscience.
It is my enormous pleasure today to welcome Professor Carl Friston, who is one of the
godfathers, if not fathers, of many developments that happened in neuroscience and imaging,
near imaging since the 80s and 90s.
And so I will just give you some brief highlights of who he is, if you don't know already who
he is, but given the number of people who attend this talk today, I assume a lot of
people are familiar with his work and significance for the brain research.
So Professor Friston is a theoretical neuroscientist and authoritative in brain imaging, invented
statistical parametric mapping, voxel-based morphometry, and dynamic causal modeling.
These contributions were motivated by schizophrenia research and theoretical studies of value
learning formulated as this connection hypothesis of schizophrenia.
So mathematical contributions by Professor Friston include variational Laplacian procedures
and generalized filtering, or hierarchical Bayesian model inversion, and his e-corner
works on the model of functional integration in the human brain and the principles that
underlie neuronal interactions.
His main contributions to theoretical neurobiology is, of course, a free energy principle for
action and perception.
And among other things, he received the first Young Investigator Award in human brain mapping
already in 1996, and he was elected a fellow of the Academy of Medical Sciences.
In 2000, he was president of the International Organization of Human Brain Mapping.
In 2003, he was awarded the Venerable Golden Brain Award, and he was elected a fellow of
Royal Society in 2006.
In 2008, he received the Medal College de France and an honorary doctorate from the University
of York in 2011, and he became the fellow of Royal Society of Biology in 2012 and received
the Walden Memorial Prize and Medal in 2013 for Contributions to Mathematical Biology.
And then he was also elected as a member of the Excellence in the Life Sciences in 2014
and the Academia Europea in 2015.
He was the 2016 recipient of the Charles Branch Award for Unparalleled Breakthroughs in Brain
Research and the Glass Brain Award, a Lifetime Achievement Award in the field of Human Brain
Mapping.
He holds honorary doctorates from the University of Zurich and, of course, from our own university,
Radbaude University.
And so without further ado, I would like to give floor to Professor Freiston, who will
talk about he and his mark of blankets.
So Professor Freiston, floor is yours.
Well, thank you very much for that lovely introduction.
I've forgotten about my young investigator brain award, I was very nostalgic.
It's a great pleasure to be able to speak to you today.
I should apologize in advance, I know very little about philosophy.
So what I've done is try to take some of my favorite formalisms that may be unpacked in
terms of philosophical implications, but I'm afraid you are going to have to do the unpacking.
So I'm going to be talking about the nature of sentient behavior, the self-organization
of sentient systems, and I'm going to do it in two parts.
I've got to tell two stories.
The first story is from the point of view of a physicist, and now we're talking about
the statistics of life, especially focusing on mark of blankets and the implications for
a Bayesian mechanics that attends the mark of blanket.
And I'll try and illustrate what that means from first principles using toy simulations
of a primordial soup.
And then I'm going to repeat the story mathematically, but using a different semantics, that of a
neurobiologist, and talking about predictive processing and coding in the brain and how
neuronal networks may enact the same kind of dynamics that we saw in the physics part
of the talk.
And then we'll conclude if we have time with a closer look at action perception.
The purpose of this is to introduce a distinction between fast reflexive action of a sort you
might see in a very simple system, like say a virus, from the kind of deliberative planned
actions that systems or curious creatures like you and me tend to engage in.
So I start this talk usually with a question from Schrodinger, how can the events in space
and time, which take place within the spatial boundary of a living organism, be accounted
for by physics and chemistry?
And now we're not going to answer that question, what we are going to do is focus on this notion
of a spatial boundary and just acknowledge that in order to talk about anything, any
system, any particle or person, you need to define or it is defined almost stipulatively
in terms of what demarcates it from everything else in the universe.
So Schrodinger would be the first person to acknowledge that this blanket, this separating
boundary is itself a statistical object, a random object.
And I'm going to read that, that boundary as a mark of blanket introduced by people like
Pearl.
For those of you who don't know what a blanket is, it's relatively simple.
Imagine that I could describe some universe with a whole set of states that influenced
each other to where these little arrows denote an influence of this state over this state.
And then we elect to identify a particular set of states and let's call them internal
states and these are going to be states that are internal to a particle or a person or
any system that we can identify.
So the mark of blanket comprises the parents, the children and the parents of the children
of the internal states.
And the role that they play is to supply a statistical separation between the internal
states and the external states in Sean here in the sense that if I wanted to predict my
dynamics of the internal states, given the rest of the universe, I would only need to
know the blanket states.
There is no further information available beyond the blanket states.
So when conditioning the outside on the blanket states, I have everything I need to know to
tell me about my state or the internal state.
And I'm going to make a further move which will become more intuitive in the next slide.
I'm going to just split the blanket states into sensory states in magenta and active
states into red and do that on the basis of the fact that internal states, external, sorry,
sensory states influence internal states but are not influenced by internal states, whereas
active states are influenced by internal states but do not influence internal states.
So that may sound a little bit arbitrary but if we think about all our favorite systems
then that separation into sensory and active states that together constitute a particle
or person's blanket states and together the blanket states and the internal states that
they enshroud constitute the states of a particle or a person or a brain or a cell.
So here's a brain and here's a cell.
So under this construct, we can associate all internal brain states such as synaptic
efficacy, activity, everything that I would need to know to describe the state of the
brain at the moment.
We can associate those with the internal states that influence our active states or actuators
or muscles or genomic nervous systems that then cause changes in the external milieu
or indeed the internal milieu of our body that then generate sensory states that are
registered by sensory epithelia that feed back into the brain, cause evoked responses,
induced responses and changes in the internal states.
And exactly the same conditional dependencies and sparse influences exist in any system that
you can imagine.
So for example a cell will have intracellular states being the internal states, we can associate
the active states with the active filaments that support the sensory states that are
pushed into the external environment, changing the external states that couple back to the
cell surface sensory receptors, the sensory states that get back into the game either
directly or precariously to influence the internal states.
So that's the basic construct upon which everything else is based.
What I want you to do for the moment is just to forget about Markov blankets, what we're
going to do is just look at the physics of self-organising systems and then we're going
to put the Markov blankets back into the game and see what special implications there
are for dynamical systems that you can find a subset of that you can identify with a system
or a particle.
So this gets a bit technical but I think it's worthwhile drilling down on this formally
because it underwrites as we'll see nearly all that we know or use to describe the physical
world.
So I'm going to start with something called a large van formulation of any given universe
so that rates of change of some states that can be anything can be written down as some
flow function which is state dependent plus some random fluctuations.
And if I trace out the flow of a trajectory of a given state of a given system, say this
is me and I get up in the morning and I have my cup of coffee, I do my emails, I have my
lunch, then for systems that attain a non-equilibrium steady state that exists over some period
of time, in other words that have an attracting set, what you will see is almost by definition,
by definition, this trajectory will always revisit after a certain period of time the
neighbourhood of any given state.
So the trajectory is captured or unfolds on this attracting set or manifold.
This can be at any time scale, it's exceedingly high dimensional, it could be my heartbeat,
it could be a time course over the year with Christmas and Easter and birthdays and summer
holidays.
The same basic structure is in plain.
It's a useful formalism because it is described or affords an interpretation in terms of the
probability that I will or any system will be found in a particular state if sampled
at a random point in time.
So we can also interpret this trajectory in terms of a probability distribution of the
kinds of states to which I am attracted.
We can write that down with a very generic equation, the Boca Planck equation in terms
of the density dynamics, the way this dot denoting the change with time, the way in
which this probability density changes as a function of the amplitude of the random
fluctuations gamma and the flow.
So why is that important?
We've just said that this attracting manifold exists over enduring periods of time, which
means that the probability distribution doesn't change with time, so that means it's zero,
which means in turn I can write down a fundamentally important equality, which is a solution to
the Boca Planck equation, which just expresses the flow, the dynamics, the way I move through
my state space as a function of this probability density and the amplitude of random fluctuations
and something else called solenoidal flow.
So this is called the Helmholtz decomposition.
It's very important in physics, it breaks detailed balance and it describes real world
systems that are open to their external states.
To give you an intuition, imagine that I placed a drop of ink in a cup of water.
Now what would happen for non-equilibrium steady state systems that didn't have an
attracting set is that the random thermal motion of the water molecules would knock
and disperse, dissipate the ink molecules and the ink would diffuse throughout the solvent.
And I can cartoon that in terms of the diffusion away or down concentration gradients as the
ink dissipates or disperses through its medium.
That's not the kind of system that we're trying to describe, we're trying to describe
this kind of system where it gathers itself up again and then attains and maintains this
non-equilibrium steady state and in so doing you can see immediately that the kinds of
flow required to assemble, self-assemble this steady state require somewhat counterintuitively
a flow up the concentration gradients, a reverse diffusion if you like.
And that's basically what that decomposition describes.
All it's saying is that for any system that gathers itself up there must be two separable
components of the flow, one which is a gradient flow flowing up the concentration gradients
and another circular or solenoidal flow which circulates around the isoprobability contours.
So that's all that this equation is basically saying.
Even more intuitively we're all very familiar with this kind of decomposition when we watch
water flowing in a circular fashion down the plug hole.
Going down the plug hole is a gradient flow and the circulating flow is the solenoidal
flow.
So that's nice but what about the Markov-Rankit?
Well this equation has to be true for any system that exists in the sense that it has
attained some form of non-equilibrium steady state and I can therefore write it in a non-trivial
way in terms of the states of a particle, the internal states and in particularly active
states, the autonomous states that don't depend upon the external states.
And if I do that what we have because the internal and active states don't depend upon
the external states I can limit this to a function of probability distribution over
the blank state.
So what we're saying is something quite interesting.
We're saying that the internal states are something say my neuronal states and my action,
the way that I move or the way that my autonomic reflexes work, come both or must be describable
in terms of a gradient flow on the log probability of something that doesn't actually include
the states that are hidden behind the Markov-Rankit and I'm going to try and sell that formalism
as a mathematical image of perception and action respectively and in so doing suggest
that this key quantity, this log probability emerges in many different guises, in many
different disciplines.
So for example if I was in behavioral reinforcement learning as a psychologist, if I was an engineer
using optimal control theory, if I was an economist dealing with expected utility theory,
this quantity, this potential if you like of a probability or a Bayesian belief as we'll
see later, of the kinds of states to which I am attracted, these are the attracting sets
that we were talking about before.
This quantity, this log probability would emerge as value.
So all of these things are trying to maximise or describe systems that maximise utility,
minimise cost or loss or maximise utility.
That's interesting and it's also important from the point of view of the free energy principle
that we heard in the introduction because the free energy is just a way of writing down
this value and this is the negative free energy.
I should note if you're a physicist you want small free energies, if you're a machine learning
