At the same time, we humans might be losing that skill. Deepfakes are an obvious example. What began as a disturbing trend in politics and pornography has spread to other forms of entertainment. It’s an entirely new kind of collaborative, active media.

In 2018, researchers at the University of California, Berkeley, developed an AI motion transfer technique that superimposes the bodies of professional dancers onto the bodies of amateurs, lending their fluid movements to your normal cha-cha-cha. This means anyone can become Fred Astaire, Ginger Rogers, or Missy Elliott. It’s a full-body deepfake, with a key difference: democratization.

Deepfake version 1.0 used AI-driven frame-by-frame image transfers that required multiple sensors and cameras. To pull off these dance fakes, all you need is your smartphone’s camera.

These fakes bring a host of opportunities to entertainment—like raising the dead. How long before Hollywood studios start reanimating Robin Williams, Marilyn Monroe, or Tupac Shakur? How long before we get new movies with old stars? Our hunch, not very long.

And then there’s the real fakes side of the deepfakes discussion: the use of computers to create alternative versions of ourselves. We already have AI-driven personal assistants: Siri, Echo, and Cortana. Imagine, you just got in a spat with your significant other and could really use some advice. Yet saying, “Hey, Siri, my boyfriend is mad at me,” only produces “I don’t know how to respond to that.” But what if your personal digital assistant were the renowned life coach Tony Robbins.

You don’t have to imagine. In 2018, Robbins teamed up with Lifekind, a company that specializes in creating AI “personas” of real people. These are audio- and photo-realistic simulations indistinguishable in everything from manners to memory. To recreate Robbins, Lifekind blended over 8 million images, with the complete library of his work—books, videos, blogs, podcasts, and tapes of live events. The result, according to Robbins, is “an operating AI, not a bot. It won’t be able to do therapy [yet], but I’ll be able to make that happen [eventually].… Already, the audio’s so good that my wife can’t tell it’s not me. But the most interesting part is the actual AI. The opportunity to capture how a person thinks, feels, or creates is extraordinary. It has a memory capacity that dwarfs anything I have. And it has all my [self-help] models, so it can look at you… and using [a particular] model decide that 20 percent of you is concerned, 30 percent is excited, 40 percent is engaged. It can literally do this in real time.” Not only is content becoming more active than ever before, those activities are blending human and machine intelligence to expand the entertainment industry into wild new terrain.

The Holodeck Is Here
Jules Urbach went to high school with Rod Roddenberry, the son of Gene Roddenberry, the creator of Star Trek.The two became close friends. They talked almost every day. What did they talk about? “The holodeck,” says Urbach. “A lot of the time, we talked about the holodeck.”
Introduced in Star Trek: The Next Generation, the holodeck uses holograms to produce almost any experience a user desires. It’s a fully immersive environment, functionally indistinguishable from real life. Urbach’s obsession became a mission—build a holodeck in the real world.

That mission took him into video games, then 3-D gaming, and finally 3-D rendering. Urbach cofounded Otoy, a company that figured out how to move rendering off the desktop and into the cloud. Before they came along, an effects-heavy film like Planet of the Apesrequired hours of supercomputer processing to produce a single frame of film. With Otoy’s software, this happens in real time, on a tablet phone, connected to the cloud via WiFi.

Next, Urbach cofounded LightStage, a company that specializes in photo-realistic 360-degree image capture. This technology is also what turns people into holograms, providing Otoy with the basic images needed for special effects.

But there are still two hurdles to clear before a holodeck can become reality. The biggest one is light. When we see an object, we’re seeing trillions of photons bouncing off that object. So if you can artificially project trillions of photons toward the eye, at just the right angle and intensity, you can recreate reality, any reality.

Enter Light Field Lab, a California-based startup manufacturing the first-ever display technology able to generate those trillions of photons. While their initial displays are only four- by six-inches across, they’re capable of projecting a holographic image two inches thick, viewable from an area thirty degrees wide. By combining these cubes into eighteen-inch displays and then combining those displays into wall panels, Light Field Lab can fill an entire room with these cubes: walls, floor, and ceiling. And every one of them can project a hologram ten feet out. It’s the Star Trekholodeck.

Or almost. Since objects in the holodeck feel like real objects, the final hurdle is touch. And here too, Light Field has made progress. In the same way that they use light to make you see, they use sound to make you feel. When ultrasound—yes, the same technology used by doctors—is projected into the room, sound waves themselves can give objects a physical presence. It’s not quite the heft of real objects, but it’s tangible. And by combining Otoy’s software, LightStage’s image capture, and Light Field’s projector, we have all the basic components for a holodeck—the most immersive form of entertainment yet.

Immersion marks the third shift in content, a transformation that has everything to do with attention. When it comes to attention, active trumps passive, and immersive trumps active. The reason is sensory input. The more senses engaged in an activity, the more attention we pay to that activity.

That’s why companies are devising all sorts of ways to drag our senses into the virtual. Haptic gloves that engage our sense of touch are already here, and increasingly sophisticated. There are also scent-emitting devices to bring Smell-O-Vision into television, and 3-D audio systems that provide concert-style experiences from the comfort of our living rooms. Haptic chairs now pitch and yaw and shiver and shake, while Ready Player One–style omnidirectional treadmills allow us to dance, dance, dance in any direction we want.

Stanford neuroscientist David Eagleman wants to take this further. He’s teamed up with Second Life creator Philip Rosedale, to expand haptic sensation from the hands to the entire torso. Rosedale’s latest creation, High Fidelity, is a fully-immersive VR world. Eagleman’s startup, NeoSensory, has built an “exoskin” that’s designed to work in that world. It’s a long-sleeved shirt with micro-motors placed every few inches along the arms, back, and stomach. “If it’s raining in the VR world,” explains Eagleman, “you can feel the raindrops. Or if you’re touched by another avatar, you can feel their touch.” And the signaling is so fast that the wearer feels that touch instantaneously.

Further still, the Los Angeles–based Dreamscape combines haptic-sensing with immersive VR to allow whole groups to share exotic encounters—such as swimming through the deep ocean beside blue whales or petting creatures in the Alien Zoo. And since Dreamscape has partnered with AMC theaters, we’re not long from the day when participatory moviegoing replaces big-screen blockbusters as the summer’s best thrill ride.

Urbach’s holodeck is the next giant leap forward. The use of ultrasound to provide a sense of touch will provide this same level of haptic sensing, but without the need for fancy gloves. The AI powering that holodeck will also be emotionally aware, and the environment it will create will be incredibly interactive, meaning all three of our major shifts in entertainment will be bundled into one experience. This will mark a sizable change in entertainment, but it’s not the end of this story.

Things are also about to get personal, really, really personal.

This Time It’s Personal
It’s 2028, the end of a long day, and no rest for the wicked. You have less than forty-five minutes to get ready for dinner, but first you want to sit down, have a drink, and be entertained. Do you pick up the remote and flick through channels? Doubtful. Are you interested in a holographic CNN floating above the coffee table? Nope. But the good news is that neither of these questions matter—because your AI already knows what you need.

Not only has your AI been with you all day, it now has the ability to monitor and understand your emotions. It’s followed the highs and lows of your mood in considerable detail. It caught your grimace in a smart mirror early this morning, heard the angry conversation you had with your wife around lunch, and was there on the ride home when you ignored a call from your brother. This last bit is especially telling, because your AI has been around long enough to know you only ignore calls from your brother when really stressed. Plus, sensors have been tracking your neurophysiology along the way, so not only does the system understand the details of your emotional life, it knows how those details impact your body and brain.

And it can act on all this information.

The moment you walk into the living room, projected onto the walls are your favorite clips from Owen Wilson comedies. What makes this unusual is you didn’t actually know you were an Owen Wilson fan, but over the past five years, you’ve caught a handful of his old movies, often without realizing it. And while the movies might have been less than memorable, each one had that scene that cracked you up. Your AI noticed. It also knows, because it’s been tracking your emotional history, that laughter—in 78.56 percent of all prior high-stress situations—was your fast track to feeling better.

So now you’re treated to a parade of Owen Wilson scenes, plus a few from other movies with the same style of comedy. Near the end of the session, your AI also inserts a few videos from your phone—movies of you and your wife laughing together. Happy memories that remind you of what’s actually important. And the mash-up works perfectly. By the time you’ve finished your drink, that bad mood has lifted. You talk through the argument with your wife and are off to dinner feeling more energized than you’ve felt all day.

The crazy part: Most of this technology is already here, found under the label of “affective computing,” or the science of teaching machines to understand and simulate human emotion. It’s a tale of convergence, a new field sitting at the intersection of cognitive psychology, computer science, and neurophysiology, combined with accelerating technologies like AI, robotics, and sensors. Already, affective computing is seeping into e-learning, where AIs adjust the presentation style if the learner gets bored; robotic caregiving, where it improves the quality of robo-nursing; and social monitoring, like a car that engages additional safety measures should the driver become angry. But its biggest impact is on entertainment, where things are getting personal.
