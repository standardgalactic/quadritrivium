Economic Risks: The Threat of Technological Unemployment
When it comes to dangers heading our way, the environment gets top billing, but it’s lately been sharing the limelight with automation. Robots and AI, our headlines increasingly declare, are coming for our jobs. In recent years, major consultancies such as McKinsey, Gartner, and Deloitte have all issued reports saying technological unemployment is unavoidable. One Oxford University study found 47 percent of all US jobs are threatened over the next few decades, and that number could be as high as 85 percent in the rest of the world.

Yet the facts tell a different story. Consider the job market, which is one of the first places to look for signs of this coming robopocalypse. Except, as journalist and author James Surowiecki wrote in a 2017 article for Wired:
Unemployment is below 5 percent, and employers in many states are complaining about labor shortages, not labor surpluses. And while millions of Americans dropped out of the labor force in the wake of the Great Recession, they’re now coming back—and getting jobs. Even more strikingly, wages for ordinary workers have risen as the labor market has improved. Granted the wage increases are meager by historical standards, but they’re rising faster than inflation and faster than productivity. That’s something that wouldn’t happen if human workers were on the fast track to obsolescence.

History tells a similar story. Theoretically, workers have been on the fast track to obsolescence since the Luddites took sledgehammers to industrial looms in the early 1800s. In 1790, 90 percent of all Americans made their living as farmers; today it’s less than 2 percent. Did those jobs disappear? Not exactly. The agrarian economy morphed, first into the industrial economy, next into the service economy, now the information economy. Automation produces job substitution far more than job obliteration.

Even when there’s automation, this doesn’t always create the dire results we expect. Consider automatic teller machines (ATMs). When they were first rolled out in the late 1970s, there were serious concerns about bank teller layoffs. Between 1995 and 2010, the number of ATMs in America went from one hundred thousand to four hundred thousand, but mass teller unemployment wasn’t the result. Because ATMs made it cheaper to operate banks, the number of banks grew by 40 percent. More banks meant more jobs for human bank tellers, which is why bank teller employment actually rose during this period.

The same thing is true for textiles, as journalist T. L. Andrews pointed out in Quartz: “Despite the fact that 98 percent of the functions of making materials have now been automated, the number of weaving jobs has increased since the nineteenth century.” The same thing is true for paralegals and law clerks, two professions predicted to suffer job loss as a result of AI. Yet discovery software, introduced into law firms in the 1990s, has actually led to the inverse. Turns out, AI is so good at discovery that lawyers now need more humans to sift through the deluge—so paralegal employment has increased.

Productivity is the main reason companies want to automate workforces. Yet, time and again, the largest increases in productivity don’t come from replacing humans with machines, but rather from augmenting machines with humans. “Certainly, many companies have used AI to automate processes,” explained Accenture’s James Wilson and Paul Daugherty in the Harvard Business Review, “but those that deploy it mainly to displace employees will see only short-term productivity gains. In our research involving 1,500 companies, we found that firms achieve the most significant performance improvements when humans and machines work together.” BMW, for example, saw an 85 percent increase in productivity when they replaced their traditional—that is, automated—assembly line process with human/robot teams.

Also worth pointing out is that every time a technology goes exponential, we find an internet-sized opportunity tucked inside. Taking advantage of these opportunities requires adaptation—which demands workforce retraining—yet the end result is a net gain in jobs. Look at the internet itself. According to research done by McKinsey, in thirteen countries stretching from China and Russia to the US, the internet created 2.6 new jobs for every 1 it destroyed. Overall, in each of these thirteen countries, the Web’s rise contributed 10 percent to GDP growth, and that number is still increasing.

Make no mistake, certain jobs are heading for extinction. While experts predict technological unemployment will have a greater impact in the 2030s, the next decade could see whole categories of work start to become memories. Robots are coming for everyone from truckers and taxi drivers to warehouse workers and retail employees. Amazon Go might not be the end of all cashiers, but in grocery stores, convenience stores, and gas stations, people will be absent more than present. The real question is will there be enough time to retrain our workforce before these effects go wide.

The answer appears to be yes. For example, Goldman Sachs recently made headlines with a study showing that autonomous vehicles will claim three hundred thousand driving jobs a year. What garnered less attention was their claim that we have twenty-five years to make this transition. Equally important, every educational advance—from VR-accelerated learning environments to AI-directed learning curricula—will make retraining easier, quicker, and more effective. Finally, as artificial intelligence becomes our user-friendly interface with technology, we’re going to see a shift in the skills required for retraining. For a host of jobs, technological fluency and agility will replace deep skills mastery.

Once again, it comes down to whether we can collaborate or not. In July 2018, with 6.7 million unfilled jobs in the US, labor shortages reached a record high. Not only are the jobs there, they’re there in record numbers. The ability to rapidly retrain our workforce to fill those jobs—that’s the challenge we have yet to meet.

Existential Risks: Vision, Prevention, and Governance
In 2002, a relatively unknown Oxford philosopher named Nick Bostrom published a paper in the Journal of Evolution and Technology. In a few years, Bostrom would vault to geek-fame because of his “Simulation Hypothesis,” which convincingly argues that we’re living inside the Matrix. However, this earlier paper also caused a stir, mainly because it scared the crap out of nearly everyone who read it.

Bostrom’s paper described a new kind of threat, which he dubbed an “existential risk,” aka a “global catastrophic risk,” but of a slightly different variety. Traditionally, “global catastrophic risks” have referred to everything from planet-killing asteroids to all-out nuclear war. But Bostrum wanted us to know that there was a new terror in town. Exponential technology, in his opinion, had a bad habit of becoming existential risk.

Nanotech run rampant—aka, Eric Drexler’s “grey goo”—is one familiar example. Another is a pissed off AI waking up, hacking NORAD, and going DEFCON 666 on the entire world. There’s also genetically modified organisms overrunning ecosystems, cyberterrorists playing good night New York with the power grid, or biohackers playing goodbye San Francisco with weaponized Ebola. These are the horrors that go bump in the expo-tech night. And this is Bostrum’s dark point: We’re in for a bumpy ride
Yet, are we certain?
This is a contentious question. Sure, thought leaders like Elon Musk and the late great Steven Hawking have been exceptionally vocal about existential dangers, and institutions as august as Oxford and MIT have formed departments devoted to their study, but opinions remain all over the map. Trying to find accurate odds about our odds of survival is an exercise in futility. Despite this mess, a handful of consensus opinions have started to emerge. These are less solutions than categories of solutions: Vision, Prevention, and Governance.

Vision
Vision is about time horizons, how far we choose to look into the future. Our brains emerged in an era of immediacy, so we’re a shortsighted species. How to avoid being eaten by a tiger—today. How to find enough food to feed my family—today. If there was any long-term thinking, it was of the how do I find someplace warm to wintervariety. In other words, evolution shaped our time horizons to see about six months into the future.

Of course, we evolved ways to extend this perspective. Delayed gratification is the psychological term, and one distinguishing characteristic of our species is the ability to delay gratification beyond the limits of lifespan. Religions that shape behavior today by promising an afterlife tomorrow rely on this mechanism. No other animal can do this.

But we seem to be losing this talent. “Civilization is revving itself into a pathologically short attention span,” writesStewart Brand in an essay for the Long Now Foundation. “This trend might be coming from the acceleration of technology, the short-horizon perspective of market-driven economics, the next-election perspective of democracies, or the distractions of personal multi-tasking. All are on the increase. Some sort of balancing corrective to the short-sightedness is needed.”
The corrective Brand came up with was his aforementioned Long Now Foundation, an organization most famous for constructing a clock that’s hidden in a cave in the hinterlands of Nevada’s Great Basin National Park. The clock’s designed to keep time for ten thousand years, but its real purpose is psychological. It’s built to make us think about ten-thousand-year time horizons. The organization’s ultimate goal is to get people to understand that if you’re trying to protect against existential risks—then you need to think long term.

Prevention
So how does thinking long term work in the real world? Prevention, our second category. One example is the Netherlands. Much of the nation sits below sea level, so it’s Europe’s most climate change–threatened locale. But rather than seeing rising tides as a problem in need of a quick fix—like larger seawalls that will, in turn, require short-term maintenance and eventual replacement—the Netherlands is being long-term proactive. “From a Dutch mindset,” explained Michael Kimmelman in the New York Times, “climate change is not a hypothetical, or a drag on the economy, but an opportunity.… [The] Dutch are pioneering a singular way forward. It is, in essence, to let water in, where possible, not hope to subdue Mother Nature: to live with water, rather than struggle to defeat it. The Dutch devise lakes, garages, parks and plazas that are a boon to daily life but also double as enormous reservoirs for when the seas and rivers spill over.”
Another example sits at the convergence of AI, networks, sensors, and satellites. Here, we gain the ability to develop global threat detection networks far more sophisticated than anything that exists today. Suggestions run the gamut, such as global food web monitoring to protect against catastrophic famines or terrorist attack; atmospheric sniffers that hunt for everything from plague-causing pathogens to the scent of nuclear materials; and rogue-AI detectors—essentially AI built to hunt for rogue AI.

And while all of this may seem outlandish, consider planet-killing asteroid detection. Two decades ago, this idea seemed somewhere between conspiracy theory and Hollywood thriller. Today, it’s the “Sentry System,” designed by NASA’s Jet Propulsion Laboratory for “earth impact monitoring,” and NASA’s DART project, our first asteroid deflection mission designed for planetary defense.

Less futuristic but no less otherworldly, we’ve been using satellite imaging to track wildfires for a little while now. In 2018, NASA started training AI to interpret the data. After a year, their neural nets could detect forest fires from space with 98 percent accuracy.

Other researchers are working on what to do about the fires we detect. Firefighting drones are already in development. Before decade’s end, it’s not ridiculous to assume that space-based forest fire–spotting AIs will be communicating with autonomous fire-fighting drones down here on Earth—or an early step toward the dematerialization of emergency services.

This kind of thinking is mandatory. Even without technological advancement, the Earth is a living system where change is a constant. Originally, our atmosphere was a delightful combination of methane and sulfur, until a poison gas called oxygen came along and ruined everything. The dinosaurs enjoyed a spot as the uber-dominant creature on our planet until the dinosaurs enjoyed a spot in our museums, celebrating their onetime uber-dominance. In a turbulent world, unless we’d like to join the dinosaurs, we need to master this art of prevention.
