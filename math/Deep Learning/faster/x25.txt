
Facial expressions, hand gestures, eye gaze, vocal tone, head movement, speech frequency and duration are all signals thick with emotional information. By coupling next generation sensors with deep learning techniques, we can read these signals and employ them to analyze a user’s mood. And the basic technology is here.

Affectiva, a startup created by Rosalind Picard, the head of MIT’s Affective Computing Group, is an emotional recognition platform used by both the gaming and the marketing industry. The technology tells a customer service chatbot if a user’s confused or frustrated, provides advertisers with a way to test the emotional effectiveness of their ads, and gives gaming companies a means to adjust play in real time. In the thriller game Nevermind, Affectiva’s tech hunts anxiety via facial expressions and biofeedback. When the system discovers the user is scared, the game doubles down—adding challenging tasks and surreal content to up the thrill factor.

Lightwave, another emotional-computing startup, can capture not just the emotional state of an individual, but that of a whole crowd. It’s already been utilized by Cisco to judge a startup pitch competition, helped DJ Paul Oakenfold increase listener engagement at a concert in Singapore, and measured viewer reactions during a pre-screening of The Revenant
Affective computing is also going mobile, which means our phones are starting to serve up content based on what’s going on in the real world—our mood, location, companionship, the mood of those companions, and on and on. Startups like Ubimo and Cluep are providing everything an emo-entrepreneur needs, from affective app development platforms to highly personalizable emotional-content delivery services.

As convergence continues, a new assortment of personalized possibilities arrive—not just content that’s been presorted to fit our mood, but content that’s been individually created to fit that mood.

The same AI-driven Choose Your Own Adventure–style storytelling invading video games has started making its way into traditional media. In May of 2018, 20th Century Fox announced they were turning the 1980s Choose Your Own Adventure book series into a big screen event. Throughout the movie, the audience used their smartphones to vote on which direction they wanted the film to go, choosing storylines, plot twists, and more. Unfortunately, having to make smartphone choices was an engagement killer—the Hollywood Reportercalled it a “major disruption to the movie theater experience in the worst way possible.” But that smartphone interface is only a temporary solution. Soon, with in-theater sensing and affective computing, emotionally driven storytelling will become just another part of the moviegoing experience.

Our AI will also know our plot preferences better than we know them ourselves. Consider that you may remember enjoying a movie, but your AI knows why you enjoyed it. With semantic analysis and biofeedback, it understands how a line of seemingly innocuous dialogue became a deeply powerful nostalgic moment. It knows your heart rate, blinking rate, pupil dilation response, where you’re looking, where you’re not. It’s this kind of data deluge that means that even in choose-your-own-adventure, before long, we won’t be the ones choosing. Our AIs will match our mood to our history, neurophysiology, location, social preferences, and desired level of immersion and then, in an instant, customize content to match them all.

And this brings us to our final shift, not a change in whois making the content or whatkind of content is being made, but rather a revolution in wherewe’ll be experiencing that content.

Here, There, and Everywhere
The story of story: a brief overview. Historians believe that storytelling itself began around the campfire, but they are certain that storytelling to the masses began with print. Books, newspapers, magazines were our first big-league carriers of information, earning them a four-hundred-year run at the center of our entertainment world. Radio came next, offering a level of intimacy and immediacy never seen before. Silent films and talkies were powerful additions, but radio was the very first technology that allowed an entire nation to tune in together.

Then black-and-white TV created an era of instantly shared visuals. Next came color television, which was less upgrade than conquest. These massive boxes earned a half-century monopoly on the center of our living rooms. Then plasma screens. At consumer electronics show after consumer electronics show, those screens got thinner, cheaper, and higher resolution. Next, cords got cut and those screens went everywhere, and what could possibly disrupt this disruption?
Enter the augmented reality of Magic Leap and the company’s stated purpose: to eliminate the screen altogether. Sure, their first generation AR glasses were a piece of hardware so geeky we can only conclude they had unintentional prophylactic effects. But things have gotten sexier since, as have reasons for wearing those glasses. Magic Leap wants to dematerialize the screen, putting it anywhere you want it to be—the wall of your bedroom, the palm of your hand, the side of the Brooklyn Bridge.

So what could possibly disrupt that? How about an AR smart contact lens. No longer would you have to wear headgear, since the screen is mounted on your cornea and projects onto the back of your retina. And what could possibly disrupt that? How about the holodeck—no eye gear required. And what could possibly disrupt that? Well, as Steve Jobs already told us, wait, wait, there’s one more thing.…
Let’s take a closer look:
First, for those who still want screens, the technology itself is morphing. OLEDs (organic light emitting diodes) have begun replacing LEDs. The initial draw was image resolution, but the ultimate advantage is flexibility. LG already has a nineteen-inch OLED display that can be rolled into a cylinder, and other companies aren’t far behind. We’ve seen something similar in phones, where swapping rigid silicon for malleable graphene has allowed Chinese researchers to develop a smartphone that can be wrapped around your wrist and worn as a bracelet. Added to this flexibility, we’re seeing touch screens that provide tactile feedback. By flowing ultra-low-level electric current back into your skin, our touch screens can now touch back.

But screens have an inherent limitation: place. Screens mean watching entertainment in a fixed location—your living room or your local movie theater. Certainly, we get mobility via our tablets and smartphones, but the trade-off is size and, by extension, engagement. If we’re watching content built for screens the size of billboards on phones the size of postage stamps, then we’re much more likely to get sucked out of the story by distraction. With augmented reality, though, we’re beginning to transition away from screens altogether.

This transition is coming quickly. Over the next five years, AR is projected to create a $90 billion market. “I regard [AR] as a big idea like the smartphone,” Apple CEO Tim Cook recently said in an interview with the Independent. “The smartphone is for everyone. We don’t have to think the iPhone is about a certain demographic, or country, or vertical market. It’s for everyone. I think AR is that big, it’s huge.”
And what this growth buys us is an information layer projected atop regular reality. The world becomes the screen. If you want to play AR Star Wars, you’re battling the Empire on your way to work, in your cubicle, cafeteria, bathroom, and beyond.

We got our first taste of this in 2016, when Nintendo released Pokémon GO and the greatest cartoon character turkey shoot in history ensued. With 5 million daily users, 65 million monthly users, and over $2 billion in revenue, the popularity of that experience remains one for the record books. In the years since, apps have exploded. Glasses that were once thick and bulky have become thin and light. They’re also about to become a whole lot smaller. Companies like the well-funded startup Mojo Vision are developing AR contact lenses that will give you heads-up display capabilities and more, no glasses required.

And for those who have no interest in AR contact lenses, Urbach projects that the first versions of his holodeck will start to show up in Disney-style theme parks and possibly in rec rooms for the extraordinarily wealthy by decade’s end. But who needs a holodeck when you can start tinkering with nature’s own reality projection system—the human brain.

This brings us to the world of brain-computer interfaces (or BCIs). With AR contact lenses we get a nearly seamless interface with an information layer. Add in haptic gloves and the simulation starts to feel real. Take that simulation off the street and into a room, layer in both photons and ultrasound, and the experience gets even more immersive. But with BCI, we’re creating reality in the exact same way that we normally create reality—with our brains.

Originally developed to help patients with “locked in syndrome” communicate, some versions of BCI use electroencephalogram (EEG) sensors to read brain waves through the scalp, enabling a hands-free, mind-controlled content interface. Already, we’ve seen EEG-based BCI devices start to invade gaming. Studies have demonstrated the technology in traditional arcade titles such as Tetris and Pac-man, and multiplayer games like World of Warcraft. As a result, we now have new BCI-specific games like MindBalance and Bacteria Hunt.

In 2017, researchers at the University of Washington took this farther, announcing BrainNet, the first brain-to-brain communication network that allows multiple parties to interact via their thoughts. By using electroencephalograms (EEGs) to “read” brain signals and transcranial magnetic stimulation (TMS) to “write” brain signals, participants were linked together to play a modified game of Tetris. They communicated and collaborated only through EEG, TMS, and a set of blinking lightbulbs, marking the birth of a new kind of “hive-mind gaming” and a frontier we’re only beginning to explore.

We’re also seeing BCI venture out of gaming and into traditional cinema. In May of 2018, English artist and director Richard Ramchurn released a twenty-seven-minute short film entitled The Moment. Custom-built to be watched while wearing a relatively cheap ($100) EEG headset, the contents of the film—the scenes, music, and animation—change every time you watch it, based entirely on what’s going on in your head.

BCIs mean that entertainment can be not just customized for our mood, it can be customized for our brains. This is a direct computer-to-cortex connection. While the development of this connection probably exceeds the ten-year time horizon we’ve been focusing on, it’s worth pointing out what this means for content creators. Sooner or later, media companies and neuroscience labs will begin to merge, the product of converging exponentials leading to converging markets leading to a completely unrecognizable entertainment landscape.CHAPTER EIGHT The Future of EducationThe Quest for Quantity and Quality
The same technology converging on our movie theaters is heading for our classrooms—and just in time. From a macroscopic perspective, education has two main issues: quantity and quality. On the quantity side, we face a catastrophic shortage. In America today, we’re in need of 1.6 million teachers. Globally, the picture is worse. By 2030, UNESCO estimates that the number of teachers needed will be a shocking 69 million. As a result, 263 million children worldwide currently lack access to basic education.

On the quality side, we face equally difficult challenges. Our modern educational system is anything but. It’s an institution created in another time for the needs of a different world. In the mid-eighteenth century, in many ways riding the railroad across America, we spread an industrialized educational system designed to produce a standardized product. Heralded by the bell, students moved from one “learning station” to the next, while standardized tests ensured quality control—young minds well prepared for the needs of society. What were those needs? Back then, obedient factory workers.

Consider that hallmark of industrial education: the sage on the stage. This one-size-fits-all model dates to an era when great teachers and good schools were a scarce resource. While economical, a teacher preaching to a classroom filled with students tends to divide pupils into two dispirited groups: those who are lost and those who are bored.

