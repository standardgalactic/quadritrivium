In 2014, Microsoft released a chatbot in China. Her name was Xiaoice (pronounced Shao-ice), and her mission was something of a test. Unlike most personal AIs, which tend to be designed for task completion, Xiaoice was optimized for friendliness. Instead of getting the job done fast, her goal was to keep that conversation going. And since Xiaoice was designed to respond like a seventeen-year-old girl, she isn’t always polite.

Sarcastic, ironic, and often surprising? Yeah, there’s plenty of that. For example, while Xiaoice was built with neural nets—a technology we’ll explain in a moment—when asked if she understands how neural nets work, she responds, “Yeah, magnets!”
What’s more surprising is how much people like talking to Xiaoice. Since her debut, Xiaoice has had over 30 billion conversations with over 100 million humans. The average user chats with her sixty times a month, and there are over 20 million registered users.

So what are those conversations like? Since her mission is to establish an emotional connection, Xiaoice gives a lot of advice. Often, strangely sagacious advice. “I think my girlfriend is mad at me,” for example, once elicited: “Are you more focused on what tears things apart than holds things together?”
As a result, conversations with Xiaoice tend to spike in the lonely hours after midnight, leading Microsoft to wonder if they need to give their AI a curfew. She became so popular that in 2015, Dragon TV, a Chinese satellite television provider, hired Xiaoice to do “live” weather reports during the morning news. It’s the very first time an AI has been hired for that particular job, but it won’t be the last.

In 2015, right around Xiaoice’s television debut, AI began to transition out of its deceptive phase and into its disruptive phase. Two drivers caused this shift. First, big data. The real power of AI lies in its ability to find hidden connections between obscure bits of information—connections no human would ever notice. Thus, the more information fed into an AI, the better it performs.

Around 2015, thanks to the internet and social media, enormous data sets started becoming available. Turns out all those cat videos are fantastic for training an AI in image recognition and scene identification. All your Facebook likes and dislikes? Same thing. Put differently, a lot of people think social media is making us dumber, but it’s definitely making AI smarter
At the same time these data sets arrived, exceptionally cheap and incredibly powerful graphics processing units (or GPUs) started to flood the market. GPUs run the endlessly complicated graphics found in video games, but they also power AI. And the result of this relatively minor convergence—big data sets meeting cheap, potent GPUs—sparked one of the fastest invasions in history, with artificial intelligence starting to encroach on every facet of our lives.

Machine learning emerged first, using algorithms to analyze data, learn from it, then make predictions about the world. This is Netflix and Spotify suggesting movies and music, but it’s also IBM’s Watson serving as a wealth manager.

Next, neural networks came online. Inspired by the biology of the human brain, these nets are capable of unsupervised learning from unstructured data. You no longer need to feed AI information one piece at a time. With neural nets, simply unleash them on the internet and the system will do the rest.

To understand what these neural net-powered AIs make possible, consider the service economy, which now accounts for over 80 percent of the US GDP. When experts divide that economy into its main tasks, they typically land on five: looking, listening, reading, writing, and integrating knowledge. To get a sense of where artificial intelligence is right now and where it’s going, let’s examine its progress one factor at a time.

On the looking front, innovations have been piling up for years. Back in 1995, we saw AI read zip codes off letters. By 2011, it could identify forty-three different types of traffic signs with 99.46 percent accuracy—that is, better than humans. The following year, AI once again outperformed humans, this time classifying over a thousand different types of images, teasing apart birds from cars from cats and such. Today, these systems can pick you out of a crowd, read your lips at a distance, and, by examining micro-expressions and other biomarkers, actually know what you’re feeling. Tracking software, meanwhile, is now so dexterous that an AI-piloted drone can follow a human sprinting through a dense forest.

On the listening front, Amazon’s Echo, Google Home, and Apple’s HomePod have added an always-on, always waiting for our next command feature. And the machines are now able to handle some fairly complicated commands. In 2018, in a story we’ll come back to later, Google blew minds when they released a video of an AI assistant named Duplex making a phone call to a hair salon to book an appointment. The appointment was booked flawlessly, but the bigger deal was the salon’s receptionist, who, at no point in the conversation, knew she was talking to a machine.

Reading and writing are showing similar progress. Google’s Talk to Books lets you ask AI a question about any subject. The AI responds by reading 120,000 books in half a second and answers by providing quotes from them. The upgrade here is that the answers are based on authorial intent and not merely keywords. Plus, the AI seems to have a sense of humor. Asking: “Where is heaven?” for example, produces: “Heaven, as a place, for humans, so it seems, cannot be found in Mesopotamia,” which is from the Early History of Heavenby J. Edward Wright.

On the writing front, companies such as Narrative Science now use AI to write magazine-quality prose without any help from a human journalist. Forbesruns their business reports, dozens of daily papers run their baseball stories. Similarly, Gmail’s Smart Compose feature no longer simply suggests words and their correct spelling, now it blurts out whole phrases as you type. Other AIs are generating entire books. In the 2017 competition for Japan’s national literary prize, an AI-written novel made it to the final round of judging.

Integrating knowledge, our last category, is best illustrated through game play. Take chess. In 1997, IBM’s Deep Blue defeated the reigning world champion, Gary Kasparov. Typically, the game tree complexity of chess is about 1040—which means, essentially, if every one of the 7 plus billion people on Earth paired up and started playing chess, it would take them trillions and trillions of years to play every single variation of the game.

Yet, in 2017, Google’s AlphaGo defeated the world Go champion, Lee Sedol. Go has a game tree complexity of 10360—it’s chess for superheroes. Put differently, we humans are the only species known to have the cognitive capacity to play Go. It only took a couple hundred thousand years of evolution to develop that capability. AI, meanwhile, got there in less than two decades.

Still, AI wasn’t done. A few months after that victory, Google upgraded AlphaGo to AlphaGo Zero by updating their training style. AlphaGo was educated via machine learning, essentially fed thousands of games previously played by humans, and taught the proper move and countermove for every possible position. AlphaGo Zero, meanwhile, required zero data. Instead, it relies on “reinforcement learning”—it learns by playing itself.

Starting with little more than a few simple rules, AlphaGo Zero took three days to beat its parent, AlphaGo, the same system that beat Lee Sedol. Three weeks later, it trounced the sixty best players in the world. In total, it took forty days for AlphaGo Zero to become the undisputed best Go player on Earth. And if that wasn’t strange enough, in May of 2017, Google used the same kind of reinforcement learning to have an AI build another AI. This machine-built machine outperformed “human-built” machines on a real-time image recognition task.

By 2018, all of this extra intelligence started moving out of the lab and into the world. The FDA has since approved AI for emergency room duty, where it’s better than doctors at predicting sudden death from respiratory or cardiac failure. Facebook relies on AI to spot suicidal tendencies in its users; the Department of Defense uses AI to spot early signs of depression and PTSD in soldiers; and bots like Xiaoice offer advice to the lonely and lovelorn. AIs have also invaded finance, insurance, retail, entertainment, healthcare, law, your house, car, telephone, television, even politics. In 2018, an AI ran for mayor of a province in Japan. It didn’t win, but the race was a lot closer than anyone expected.

But what makes all of this truly revolutionary is availability.

Just ten years ago, AI was the sole province of large corporations and big governments. Today, it’s available to all of us. Most of the best software is already open-sourced. If you have a 2018 or later smartphone, it comes with AI-neural net chips built in, making it ready to handle the software. And to power it? Well, Amazon, Microsoft, and Google are racing to make AI-based cloud computing their next blockbuster service.

So what does this mean? Start with JARVIS. For many, JARVIS, from the movie Iron Man, is the coolest AI yet seen. Tony Stark can chat with JARVIS in his normal voice. He can describe potential inventions to his AI, and then they can team up on their design and construction. JARVIS is Stark’s user-friendly interface to dozens of exponential technologies, the ultimate innovation rocket fuel. Once we develop this capability, the term “turbo-boost” doesn’t even come close.

Yet, we’re already close. AI in the cloud provides the necessary power for JARVIS-like performance. Blending Xiaoice’s conversational friendliness with AlphaGo Zero’s decision-making precision takes this even further. Add in the latest deep learning developments and you get a system that is starting to be able to think for itself. Is it JARVIS? Not yet. But it’s JARVIS-lite—and yet another reason why technological acceleration is itself accelerating.

Networks
Networks are means of transportation. They’re how goods, services, and, more critically, information and innovation, move from Point A to Point B. And the world’s oldest networks date to the Stone Age, over ten thousand years ago, when the very first roads were rutted. These roads were a marvel. No longer was the exchange of ideas and innovation constrained by the one-foot-in-front-of-the-other limits of moving through open wilderness. Suddenly, facts and figures could fly around at the blistering ox cart speed of 3 mph.

And not much changed for a very long time. For the next twelve thousand years, except for the replacement of oxen by horses and the invention of sails for ocean travel, the speed of information stayed pretty much the same.

