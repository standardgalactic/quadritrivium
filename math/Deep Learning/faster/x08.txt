These trends will only continue. We’re moving from the world of the microscopic to the world of the nanoscopic. This has already led to a wave of smart clothing, jewelry, and glasses, the Oura ring being one example. Soon, these sensors will migrate inside the body. Take smart dust, a dust mote–sized system that can sense, store, and transmit data. Today, a “mote” of smart dust is the size of an apple seed. Tomorrow, nanoscale motes will float through our bloodstream, collecting data, exploring one of the last great terra incognita—the interior of the human body.

We’re about to learn a whole lot more, about the body, about everything. This is the big shift. The data haul from these sensors is beyond comprehension. An autonomous car generates four terabytes a day, or a thousand feature length films’ worth of information; a commercial airliner, forty terabytes; a smart factory, a petabyte.

So what does this data haul get us? Plenty.

Doctors no longer have to rely on annual checkups to track patients’ health, as they now get a blizzard of quantified-self data streaming in 24-7. Farmers can know the moisture content in both the soil and the sky, allowing pinpoint watering for healthier crops, bigger yields, and—an important factor during global warming—far less water waste. In business, since lithe trumps lumbering during times of rapid change, agility will be the biggest advantage. While knowing everything about one’s customers presents an alarming privacy concern, it does provide organizations with an incredible level of dexterity—which may be the only way to stay in business in these accelerated times.

And these accelerated times are already here. Within a decade, we will live in a world where just about anything that can be measured will be measured, constantly. It’s a world of exceptionally radical transparency. From the edge of space to the bottom of the ocean to the inside of your bloodstream, our electric skin is producing a sensorium of endlessly available information. Like it or not, we now live on a hyperconscious planet.

Robotics
In March 2011, an earthquake in Tokyo triggered a tsunami in the Pacific, flinging a wave the size of an apartment complex at the Fukushima Daiichi nuclear plant. In the ensuring chaos, first the emergency power supply went haywire, then the pumps failed to pump, finally the cooling systems failed to cool. These three meltdowns were followed by a series of midair hydrogen explosions and a catastrophic mess. A month later, on a scale designed by the International Atomic Energy Agency to measure radiation levels after an accident, sensors were off the charts.

Getting cleanup crews on site quickly was fundamental to containment, but Fukushima was too hot for humans. Yet Japan has long been one of the world leaders in robotics, so they sent in droids. And the droids failed miserably. It was a national disaster piled atop a national disaster. The rough terrain acted like a minefield, the radiation fried their circuits. Within a few months, Fukushima was a robot graveyard.

The disaster hit Honda especially hard. Since the start of the crisis, they’d been fielding phone calls and emails from thousands of people begging them to send in ASIMO, the world’s most advanced humanoid robot. Looking a lot like a teenager dressed up as a 1950s era astronaut (think big bubble white spacesuit), ASIMO was an international celebrity. He’d rung the bell to open the New York Stock Exchange, conducted the Detroit Symphony Orchestra, and walked the red carpet at a half-dozen movie premieres. Yet there’s quite a distance between strutting across a carpet and handling the complex environment of a nuclear disaster. ASIMO, like the other robots sent into Fukushima, turned out to be too unreliable for disaster mitigation, creating a public relations nightmare for Honda and an uproar in the robotics community.

In response to the roar, a few years later, DARPA launched their Robotics Challenge, a $3.5 million purse for a humanoid robot capable of “executing complex tasks in a dangerous, degraded, human-engineered environment.” This last bit is key. Humanoid robots are critical because we live in a human-engineered world, one built to interface with our interface: two hands, two eyes, forward-facing bipedal posture.

The results of the 2015 Challenge, viewable online, are a robot blooper reel. Robots fall down, robots fail to climb stairs, robots shoot sparks then short-circuit. Even DARPA program manager and Robot Challenge organizer Gill Pratt couldn’t abide his own live event: “Why would anyone sit in the sun and heat, watching a machine take an hour to go through eight simple tasks that you could do in five minutes?”
But progress was swift. A year later, a video released online showed off Boston Dynamics’ robot Atlas, the second place winner from the 2015 DARPA Challenge, hiking through slick, snowy woods, stacking boxes in a warehouse, even regaining his balance after getting whacked with a hockey stick. A year after that, a different video showed Atlas navigating an obstacle course that included a backflip off a wooden crate and color commentary by a sports announcer: “A 360 spin onto the pallet, backflip gainer off…”
Honda also got in on the action. By 2017, they’d created a prototype disaster-response bot that could climb ladders, shimmy sideways, and even get down on all fours and knuckle-walk through rough terrain. In the six years since Fukushima, we’d gone from drunken droids to disaster-ready ninjas.

And not to be outdone by Honda, 2017 also saw the Japanese conglomerate Softbank buy Boston Dynamics from Alphabet (who had purchased the company back in 2013). The reason? A different national disaster facing Japan—a rapidly aging population and no one to care for the elderly.

After decades of rising life expectancies and falling birth rates, Japan entered the new millennium with the bulk of its population edging into retirement, and no one to take their place. The economy was starved for labor, and concerns were rising about who would care for the elderly and how to afford that care. In 2015, in order solve both problems at once, Prime Minister Shinzo Abe called for a “robot revolution.” And thanks to a series of convergences, his call was heard.

Globally.

Robots are now entering nearly every aspect of our lives. Today’s versions are AI-empowered, allowing them to learn on their own, operate solo and in swarms, walk on two legs, balance on two wheels, drive, swim, fly, and, as mentioned, backflip. Today, robots do jobs that are dull, dirty, or dangerous. Tomorrow, they’ll show up anywhere accuracy and experience are key. In the operating room, robots are assisting on everything from routine hernia repair to complicated heart bypasses. Out on the farm, robo-harvesters gather crops from the fields and robo-pickers pluck fruit from the trees. In construction, 2019 brought the first commercially available robo-mason, capable of laying a thousand bricks an hour.

Industrial robotics has seen a bigger shift. A decade ago, these multimillion-dollar machines were so dangerous they were walled off from the workforce behind bulletproof glass and so complicated to program that PhDs were typically required. Not anymore. A slew of “cobots,” short for collaborative robots, are hitting the market. To program them, just move their robotic arms through the desired motion and they’re good to go. Even better, these cobots are jam-packed with sensors, so the millisecond they encounter anything fleshy—like a human—they freeze.

But the real revolution is economic. The UR3, a cobot from the Danish manufacturer Universal Robots, retails for $23,000, which is roughly the average annual global wage for a factory worker. Plus robots never tire, don’t need bathroom breaks, and won’t go on vacations. This explains why Tesla, GM, and Ford are fully automating their plants and why Foxconn (manufacturers of the iPhone) and Amazon, have already replaced tens of thousands of factory jobs with robots.

Amazon’s also been driving the drone segment of this same market. Five years ago, when they announced drone package delivery was in their pipeline, most experts thought it was a pipe dream. Today, everyone from 7-Eleven to Domino’s Pizza has a program in the works. Tomorrow, whether it’s the latest John Grisham novel, cough syrup, or some late-night ice cream, drones are on the job.

For disaster relief and delivery of medical supplies, drones have been on the job for a while, and not just in Japan. They were in Haiti after Hurricane Sandy in 2012; in the Philippines after Typhoon Haiyan in 2013; in the Balkans for flooding; in China for an earthquake. They’re faster than humans at spotting survivors in need of help. Boeing’s heavy-lifting drones can hoist a small car, so they’re often better at providing that help. A company called Zipline uses them to deliver blood and medicine in both Rwanda and Tanzania, and since 50 percent of Africa lacks adequate roads, this development could significantly improve the quality of medical care on that continent.

We’re also seeing drones mitigating a different disaster: deforestation. We lose over 7 billion trees a year to timber harvesting, agricultural expansion, wildfire, mining, road building, and all the rest. It’s an environmental disaster of epic proportion, both a major cause of climate change and species extinction. Yet there are now tree-planting drones that fire seedpod bullets into the ground, allowing a single drone to plant as many as one hundred thousand trees a day.

Of course, we could go on like this for quite some time. Elder care, hospice care, infant care, pet care, personal assistants, avatars, autonomous cars, flying cars—the robots are coming, the robots are coming, the robots are here. But there’s a forest through these trees: It’s not just robots.

It’s the convergence of robots with other exponential technologies. It’s an electric skin of sensors smashing against neural net–powered AIs in the cloud that are bashing into a growing swarm of deftly nimble and increasingly intelligent robots. And the stranger part of this story? As we’ll see in the next chapter—it’s merely half the story.CHAPTER THREE The Turbo-Boost:Exponential Technologies Part IIVirtual and Augmented Reality
In 2001, Jeremy Bailenson, a Stanford psychologist and virtual reality pioneer, packed up most of the gear in his lab, put it on a plane, and flew it to Washington, DC. He was heading to the Federal Judicial Center to host a conference for judges on the power of virtual reality in a courtroom. And since nothing could be more convincing than a demonstration, Bailenson had the judges don VR goggles and walk the plank.

The plank was part of a VR simulation. The program mapped the room the conference was taking place in—down to the fibers in the carpet and the streaks on the windows—which is what the judges saw when they put on the goggles. Until Bailenson hit a button and a chasm opened up beneath their feet. About thirty feet deep and ten feet wide, the chasm had a thin, rickety plank stretching across it. The game is to walk the plank, which is what one of those judges was doing when he took a step just a little left of center.

And slipped.

