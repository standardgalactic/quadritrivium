Varela, however, disagrees with this extension into the social, and so the authors go no further there.51But in his single-authored introduction to Autopoiesis and Cognition,Maturana does go further and describes social systems as autopoietic, giving noticeably human and nonscalar examples: “family, a club, an army, a political party, a religion or a nation” (xxviii). Luhmann follows this extension, with confusing results. This extension grants various kinds of emergent and scalar attributes to structures that do not possess these attributes. Quite simply, claiming that a corporation is an autopoietic system would be equivalent to claiming that the nervous system made and controlled the entire body—another scalar synecdoche.

Thus, we can add to our discussion of autopoiesis (from chapter 8) three observations about the synecdochic nature of this focus on self-reference and self-maintenance. First, these notions are easily mixed up with a kind of scalar colonizing fantasy: that if a group of people claim to be a group (“self-reference”) and work to maintain that group (self-regulation), then they are a system.52Thus, Luhmann’s systems theory emphasizes the synecdochic reference point where organizations “can communicate in their own name.”53This is exacerbated by, second, the stipulation that a system is independent from external factors—that it is constituted by this division between self and other. Luhmann is emphatic on this point: “The lack of operational access to the environment is a necessary condition for cognition.”54This leads, third, to the assertion that those entities within the system are contained and subordinated to that system for the sake of its self-maintenance.55These points maintain a kind of synecdoche and an overextension of principles of organization on one scale to possibilities and diversity on another. Most importantly, they indulge some terrible habits of nonscalar thought: if I consider the nation or the church as a self-referential system that must maintain itself over and against those outside of it (its environment, those from another country, the nonbelievers) and subordinate its components to this maintenance, then we find ourselves with a biological justification for the kind of othering that claims those considered “components” (of a nation, religion, race, etc.) at the exclusion of everything else. But this is not even how cells work, nor is it how we get from cells to bodies.

At the same time, both cybernetics and systems theory are building on one essential opportunity provided by information theory: one way of explaining the revolutionary aspect of information theory is that it scales communication or semantic structures in a way that was not previously possible. By defining mathematically the conditions of communication within a signal, Claude Shannon provided a different scale view of the process of semiosis. Doing so highlights three further questions about scale that remain in systems theory, cybernetics and subsequent developments:
First, how does information make use of lower-scale components to encode means of communication, that is, imparting forms and patterns that serve as structures to be responded to? Does it make a difference if the ground of a communication is selected on another scale, as in the case of a molecule producing a scent or in a series of transistors encoding for computation? This problem is already implicit in Warren Weaver’s separation of three levels of communication: the technical, the semantic, and the effective.56Can these be redescribed in terms of scale, perhaps as the lower-scale medium (the technical), the scalar translation (the semantic), and the large-scale effects (the effective)?
Second, how do forms of communication coordinate together smaller-scale entities to produce larger-scale forms? Most systems theories hold that communication is central to the formation of larger-scale human endeavors. Such claims are not so controversial for social communication, because few would question that humans communicate. But can this question of coordination via communication also be extended to cells, bodies, and ecologies?
Third, how does information pass from one scale to another? What, for instance, is information about climate change, and how does that translate to this-scale such that it can actually make a difference? Or, as is the task of the increasingly popular discipline of epigenetics, how do we get from DNA to bodies? An even more provocative way of posingthis question is to return it to the questions of experience raised in chapter 8: how do we experience information from one scale when it becomes apparent on another scale? For example, how is the experience of hunger a manifestation of information about another scale (the availability of the appropriate molecules to keep cellular metabolic processes running)? What happens when we have scalar descriptions to make us aware of this scalar transfer? We will make some moves in this direction in chapter 12.

Cybernetics, systems theory, and information theory all inherit additional basic problems arising from statistics. Although not all statistics are scalar, some are, and scale helps us understand something about their operation (see 4.21). Clearly, statistics are essential for mapping scalar relations. Lest we presume that statistics are inherently problematic, we must remember that it is also in such aggregation that we can see structures of inequality and ecological effects. But as we inevitably experience in the frustration of being treated as a demographic, the allure of statistics brings a few essential scalar confusions. In terms of scale, statistics provide descriptions of patterns that emerge in the aggregate while still retaining reference to the smaller components. Therein lies the conceptual difficulty but also rhetorical power: statistics provide a means of redescribing how objects on a smaller scale relate to a larger one. Statistics therefore say less about the actualities of those lower-scale items, although they can provide information about possible ranges of behavior, basis for categorization, and tendencies in those categorizations. The major error is thinking that statistically derived principles translate directly to any single component on the lower scale. Statistics do not annihilate the diversity on the scale below; they identify patterns within that diversity. We must be wary of which scale is being privileged as a mode of description, with the understanding that statistics can point to a larger-scale pattern that must usually be translated to smaller-scale particularities. When such patterns are derived across scalar thresholds, it is important to note that these principles, laws, and trends are not explanatory (6.34); they don’t necessarily say how or why behavior has aggregated in the way it has. In fact, it is the beauty of statistics that they don’t have to.

We can see these problems in Geoffrey West’s Scale,which contains many essential descriptions of mathematically derived scalar principles. West, a mathematical biologist, positions mathematical principles as “universal physical and mathematical properties of these networks.”57But these laws are universal in a particular way: in abstracting away from any particular object. Such principles “embod[y] a ‘universal’ quality  .  .  . [by] transcending the specific details of the object that is moving” (77). At times, West acknowledges that this implies that such laws are simply approximations.58But at other points he overemphasizes the mathematical principles, stating that such principles are “quantifiable laws that capture” the essential features of living systems (98). This language might lead one to miss the essential qualifier that such principles are “emergent laws of biology” that apply to the “generic coarse-grained behavior of living systems” (98). In other words, there is a scalar maneuver implicit in these principles, and their universal applicability needs to be understood in this moving to a larger scale to explain behavior on a lower scale. Without this point we find a kind of reversed reductionism blended with systems thinking: the supposition that something might be explained entirely by what can be seen in the aggregate. Such a position will always be pushing one scale larger, using the aggregate of cells to explain cells, the aggregate of organisms to explain an organism, and so on.

The great value of West’s mathematical scaling is that it traces how “effective laws that govern  .  .  . behavior must have evolved at all scales” (103). In doing so, West allows us to separate out three kinds of scalar principles: those involved in any given object getting smaller or larger (what is known as allometric scaling), principles involved in moving across scales (6.32), and principles and constraints available within any given scale domain (6.49). Statistical methods help us identify some of these attributes as long as we understand when and how we are abstracting from the particulars available at any given scale.

The promises and limits of such mathematical tools have been greatly clarified by computational methods that systematically extend small-scale, localized attributes into larger patterns, such as Benoit Mandelbrot’s fractal geometry, Stuart Kauffman’s Boolean networks, and Stephen Wolfram’s cellular automata. These methods describe and replicate complex patterns of behavior using relatively simple rules or algorithms, using the processing power of computers to compound simple rules in order to create complex behavior.59They demonstrate and mimic how simple constraints and tendencies of lower-scale entities create complex behavior in the aggregate. They also imply that one does not need fully descriptive or predictive principles to produce the diversity of the world
Mandelbrot’s fractals, for instance, use simple algorithms to create recursive repetition with sometimes regular, sometimes irregular changes. The resulting patterns are noticeably lifelike (see Figure 10), and Mandelbrot frames his work in terms of creating a geometry of Nature.60Many of his examples deal with objects that can be understood as large-scale patterns of lower-scale phenomena including galaxies (84), turbulence (97), cell membranes (114), and Brownian motion (232). Fractal geometry thus describes a kind of aggregation, particularly aggregation that results in repeated patterns across different scales.

Similarly, Kauffman’s early Boolean network simulations of evolutionary processes demonstrated the thresholding that can occur in the process of self-organization. These computer simulations of evolution generate “fitness landscapes” that show discernible patterns of change based on simplified variables, models that he describes as a “God’s-eye view” that show “surprising large-scale features.”61From these he derives his concepts of autocatalytic closure and reaction networks, in which a network of components finds itself interconnected in such a way that they begin to function as a unit (50). To illustrate this, he provides a scalar “chemical creation myth.” This imagines a series of disconnected buttons that one then begins to tie together.62As one creates more connections between the buttons, within a small number of iterations one finds a critical threshold where if you pick up any button, the whole network would be lifted together. Once such crystallization occurs, Kauffman argues, a new level of organization emerges wholesale.63He applies these notions to the genesis of life from molecules (61), to the creation of multicellular organisms (126), and to global evolution, economics, and language.

Wolfram’s work on cellular automata is equally surprising.64Here “cellular” means the cells of a grid that can turn on or off depending on simple rules run many times. In these grids, complex behavior can be created out of a few simple rules and the appropriate starting conditions (see Figure 11). Like fractals, the resulting patterns are eerily lifelike, leading to this line of research being named “artificial life.”65Wolfram systematically catalogs possible cellular automata behavior, noting the extraordinary and diverse patterns created.66He then applies these general principles to a list of scalar aggregates: crystal growth (369), breakages (375; see section 6.38–39 above), fluid flow (376), growth patterns (400), and financial systems (429).

Figure 10:A snapshot of the Mandelbrot set. These complex patterns are generated out of a relatively simple algorithm run repeatedly. This image is created by Wolfgang Beyer and licensed under CC BY-SA 3.0.  
Mandelbrot’s, Kauffman’s, and Wolfram’s methods rework our intuitive notions of how complexity emerges. Thus, Wolfram argues that his work requires that we develop a “new intuition” (47). If simple rules provide the basis for aggregation into complex forms at a larger scale, then many common assumptions about the means and methods of controlling aggregates are incorrect. These theories suggest that the synecdoche is only possible as a post hoc claim that hardly understands its own maneuver: things and patterns emerge across scalar thresholds, and then one claims (to control, to understand, to be) them without understanding the condition of their emergence. We then seek the rule that will produce or reproduce a behavior, but the rules provided here are not controllable in the traditional sense. Thus Wolfram argues that even though such models “capture the essential features of systems with very complex behavior” (750), they are “computationally irreducible—so that in effect the only way to find their behavior is to face each of their steps” (6). In other words, “the only way to find out what they do will usually be just to run them” (750).67Similarly, Kauffman argues that “what we are after here is not necessarily detailed prediction, but explanation” (23). But it’s a specific kind of explanation, a theory of emergence that “gives up the dream of predicting the details” (19).68
Figure 11:An example of Wolfram’s cellular automata. These start with a single line (the top right of the diagram). One then sets rules for what will happen in the next rows, using the three previous cells above any given cell to determine whether this next cell will be black or white (this rule is pictured in the top left). This produces 256 possibilities in the simplest version, most of which are simple or repetitive, but some of which are surprisingly complex. This is Rule 110 run to 250 steps, which demonstrates regular yet complex patterns. Others, most famously Rule 30, produce seemingly chaotic non-periodic behavior. Image copyright by Stephen Wolfram, LLC.
