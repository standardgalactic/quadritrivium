Prashanth, H.C., Madhav, R.: Evolutionary standard cell synthesis of unconventional designs. In: Proceedings of the Great Lakes Symposium on VLSI 2022, GLSVLSI ’22, pp. 189–192. ACM (2022)
59.
Real, E., Moore, S., Selle, A., Saxena, S., Suematsu, Y.L., Tan, J., Le, Q.V., Kurakin, A.: Large-scale evolution of image classifiers. In: Proceedings of the 34th International Conference on Machine Learning, ICML’17, vol. 70, pp. 2902–2911 (2017). JMLR.org
60.
Sarwar, S.S., Venkataramani, S., Ankit, A., Raghunathan, A., Roy, K.: Energy-efficient neural computing with approximate multipliers. J. Emerg. Technol. Comput. Syst. 14(2), 16:1–16:23 (2018)
61.
Sateesan, A., Sinha, S., Smitha, K.G., Vinod, A.P.: A survey of algorithmic and hardware optimization techniques for vision convolutional neural networks on FPGAs. Neural Process. Lett. 53(3), 2331–2377 (2021)Crossref
62.
Schorn, C., Elsken, T., Vogel, S., Runge, A., Guntoro, A., Ascheid, G.: Automated design of error-resilient and hardware-efficient deep neural networks. Neural Comput. Appl. 32(24), 18327–18345 (2020)Crossref
63.
Sekanina, L.: Neural architecture search and hardware accelerator co-search: a survey. IEEE Access 9, 151337–151362 (2021)Crossref
64.
Shafique, M., Naseer, M., Theocharides, T., Kyrkou, C., Mutlu, O., Orosa, L., Choi, J.: Robust machine learning systems: challenges, current trends, perspectives, and the road ahead. IEEE Design & Test 37(2), 30–57 (2020)Crossref
65.
Sipper, M.: Neural networks with à la carte selection of activation functions. SN Comput. Sci. 2(6), 470 (2021)Crossref
66.
Staudigl, F., Merchant, F., Leupers, R.: A survey of neuromorphic computing-in-memory: architectures, simulators, and security. IEEE Design & Test 39(2), 90–99 (2022)Crossref
67.
Stewart, R., Nowlan, A., Bacchus, P., Ducasse, Q., Komendantskaya, E.: Optimising hardware accelerated neural networks with quantisation and a knowledge distillation evolutionary algorithm. Electronics 10(4) (2021)
68.
Sze, V., Chen, Y., Yang, T., Emer, J.S.: Efficient processing of deep neural networks: a tutorial and survey. Proc. IEEE 105(12), 2295–2329 (2017)Crossref
69.
Sze, V., Chen, Y.-H., Yang, T.-J., Emer, J.S.: Efficient Processing of Deep Neural Networks. Synthesis Lectures on Computer Architecture. Morgan & Claypool Publishers (2020)
70.
Vasicek, Z., Sekanina, L.: Evolutionary approach to approximate digital circuits design. IEEE Trans. Evol. Comput. 19(3), 432–444 (2015)Crossref
71.
Velasco-Montero, D., Fernandez-Berni, J., Carmona-Galan, R., Rodriguez-Vazquez, A.: Previous: a methodology for prediction of visual inference performance on IoT devices. IEEE Internet Things J. 7(10), 9227–9240 (2020)Crossref
72.
Venkataramani, S., et al.: Efficient AI system design with cross-layer approximate computing. Proc. IEEE 108(12), 2232–2250 (2020)Crossref
73.
Wang, T., Wang, K., Cai, H., Lin, J., Liu, Z., Wang, H., Lin, Y., Han, S.: APQ: joint search for network architecture, pruning and quantization policy. In: 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2075–2084 (2020)
74.
Wang, X., Wang, X., Jin, L., Lv, R., Dai, B., He, M., Lv, T.: Evolutionary algorithm-based and network architecture search-enabled multiobjective traffic classification. IEEE Access 9, 52310–52325 (2021)Crossref
75.
Wu, Y.N., Emer, J.S., Sze, V.: Accelergy: an architecture-level energy estimation methodology for accelerator designs. In: 2019 IEEE/ACM International Conference on Computer-Aided Design (ICCAD), pp. 1–8 (2019)
76.
Xia, X., Ding, W.: HNAS: hierarchical neural architecture search on mobile devices (2020)
77.
Yuan, Z., Liu, J., Li, X., Yan, L., Chen, H., Bingzhe, W., Yang, Y., Sun, G.: NAS4RRAM: neural network architecture search for inference on rram-based accelerators. Sci. China Inf. Sci. 64, 160407 (2021)Crossref
78.
Zhang, L.L., Yang, Y., Jiang, Y., Zhu, W., Liu, Y.: Fast hardware-aware neural architecture search. In: 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 2959–2967 (2020)
79.
Zhang, X., Wang, J., Zhu, C., Lin, Y., Xiong, J., Hwu, W.-M., Chen, D.: DNNBuilder: an automated tool for building high-performance DNN hardware accelerators for FPGAs. In: 2018 IEEE/ACM International Conference on Computer-Aided Design (ICCAD), pp. 1–8 (2018)
80.
Zhou, Y., Dong, X., Akin, B., Tan, M., Peng, D., Meng, T., Yazdanbakhsh, A., Huang, D., Narayanaswami, R., Laudon, J.: Rethinking co-design of neural architectures and hardware accelerators (2021)©  The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.  2024
W. Banzhaf et al.(eds.)Handbook of Evolutionary Machine LearningGenetic and Evolutionary Computationhttps://doi.org/10.1007/978-981-99-3814-8_13
13.  Adversarial Evolutionary Learning with Distributed Spatial Coevolution
Jamal  Toutouh1, Erik  Hemberg2and Una-May  O’Reilly2
(1)
ITIS Software, University of Malaga, Malaga, Spain
(2)
MIT CSAIL, Cambridge, MA, USA
Jamal  Toutouh(Corresponding author)
Email: jamal@uma.es
Erik  Hemberg
Email: hembergerik@csail.mit.edu
Una-May  O’Reilly
Email: unamay@csail.mit.edu
Abstract
Adversarial Evolutionary Learning (AEL) is concerned with competing adversaries that are adapting over time. This competition can be defined as a minimization–maximization problem. Different methods exist to model the search for solutions to this problem, such as the Competitive Coevolutionary Algorithm, Multi-agent Reinforcement Learning, Adversarial Machine Learning, and Evolutionary Game Theory. This chapter introduces an overview of AEL. We focus on spatially distributed competitive coevolutionfor adversarial evolutionary learning to deal with the Generative Adversarial Networks (GANs)training challenges. A population of multiple individual solutions, parameterized artificial neural networks  (ANN), providesdiversityto the gradient-based GAN learning and increases the robustness of the GAN training. The computational complexity is reduced by using a spatial topology that decreases the number of evaluations and facilitates scalability. In addition, the topology enables diverse hyper-parameters, objectives, search operators, and data. We present a design and an implementation of an AEL system with spatial competitive coevolutionand gradient-based adversarial learning. We demonstrate how the increase in diversityimproves the performance of generative learning tasks on image data. Moreover, the distributed population in AEL can help overcome some hardware limitations for ANN architectures.
13.1 Introduction
Adversarial Evolutionary Learning (AEL) is concerned with competing adversaries that are adapting over time. One division of this perpetual conflict is to search for solutions to a minimization–maximization problem via competitive coevolutionary algorithm  (CCA)  [60], Multi-agent Reinforcement Learning(MARL)  [23], Adversarial Machine Learning  (AML)  [109], and Evolutionary Game Theory  (EGT)  [102]. In AEL, adversarial populations  (e.g., some solution representations or agents) adapt against the learning of each other. The learningrefers to how the adversaries are updated, e.g., training of parameterized functions with some data. A population can help provide robustness and scale to the learning method  [78]. One particular instance of interest of adaptive adversaries is in generative modeling with Generative Adversarial Networks  (GANs)that aims to learn a function that describes a latent (unknown) distribution  [51].
A GAN estimates the distribution from which real data samples from a training dataset are obtained  [51]. GANs provide an example for the capabilities of AEL. In a GAN, two artificial neural networks (ANN), a generatorand a discriminator, compete as adversaries over multiple iterations to address a minimaxoptimization. The generatorproduces data samples from a latent spaceinput, and the discriminator  (classifier) discriminates between generated samples (“fake”) and samples from a dataset (“real”). The objective and loss functionsare defined to train the generatorto synthesize a sample that cannot be discriminated from a real sample by the discriminator. GANs can produce realistic data samples with a few samples due to the generative adversarial interactions. Numerous image applications use GANs, e.g., 3D object generation  [24], image-to-image translation  [89], multispectral and panchromatic images fusion  [41], and medical image generation[112] and data types  [67, 83, 120].
A caveat is that GANs trained with gradient-based optimization methods commonly fail to converge to a stable solution where the generatorand the discriminatorcannot improve their objectives, i.e., to find the Nash equilibrium of the underlying game. Some examples of training misbehaviors, a.k.a. pathologies, that frequently hinder GAN training from converging to this equilibrium are mode collapse  [9], discriminatorcollapse  [64], and vanishing gradients  [7]. There are black-box machine learning  (ML) algorithms that use evolutionary computation  [118] to overcome this when training GANs  [33, 110].
We aim to show how diversityfrom AEL improves performance in adversarial evolutionary learning applied to train GANs. Furthermore, we inspect how convergence, resilience, and robustness are enabled by the diversityof: solutions  (e.g., ANN weights), hyper-parameters, operators, objectives, and data. For this, we use the process of evolutionary learning  (evolutionary computation) as a natural representationto accommodate diversity. In evolutionary learning, multiple solutions and higher performing solutions are selected and varied to create a population of new solutions, i.e., learning is done at the population level. The use of populations provides solution diversity. A spatial topology defines interaction among solutions and provides a diversitytemporally over generations  (training epochs). Furthermore, the nodes in the topology allow hyper-parameter, objective, data, and operator diversity for each node. Finally, the spatial topology provides an asynchronous and scalable system. Figure  13.1shows possible sources of diversity for adversarial evolutionary learning, i.e., coevolution and spatially distributed coevolution.
Fig. 13.1
Overview of possible sources of diversity for adversarial evolutionary learning with Lipizzaner, e.g., population
The research work on spatial coevolutionary GANs is dispersed  [2, 45, 54, 104–108], so this chapter consolidates it. A spatial (two-dimensional toroidal) topology can efficiently control the mixing of adversarial populations in coevolutionary algorithms  [54, 108]. Studies showed that the spatially distributed competitive coevolutionary GAN training mitigates pathologies[54, 105]. For example, each cell contains a generator–discriminatorpair and randomly selects a loss functionto train each cell for each generation  [104]. In addition, using distributed cells allows data decomposition to train GANs in each cell with different subsets of data for datadiversity  [106]. In particular, we demonstrate the impact of diversity on GAN training and different tasks  [107]. In this chapter, we discuss studies of the following important adversarial evolutionary learning factors of population, hyper-parameter, variation operator, data, and objective diversity:
Spatial PopulationCan spatial coevolutionary algorithms improve GAN training? (Sect.  13.4.1)
TopologyWhat is the effect on the GAN trained when changing topology and communication? (Sect.  13.4.1.1)
ScaleHow do large spatial grids impact the accuracy of the generative model trained with a cellular algorithm? (Sect.  13.4.1.2)
Variation operatorDo different loss functioncombinations (applied to optimize the network parameters) result in generative models with better performance? (Sect.  13.4.2)
DataHow does the GAN training quality change in spatially distributed grids when the dataset is varied on the grid? (Sect.  13.4.3)
Task objective transferCan we create high-quality and diverse ensembles? (Sect.  13.4.4)
We focus on Lipizzaner, a system specifically designed to reliably address the degenerate GAN behaviors  [54]. Lipizzanercombines spatially distributed coevolution and GAN training. Lipizzanerconsists of an asynchronous CCA executing on a 2D spatial grid topology of cells with overlapping neighborhoods. Two competitive coevolving sub-populations exist on each cell—generatorsand discriminators—collected from the cell and its adjacent neighbors. Training is done pairwise in the sub-populations. The ANN models follow ML convention and are updated with stochastic gradient descent (SGD) using the minimaxobjective. Some training parameters, such as the learning rate of SGD, also use evolutionary learning. Between training epochs, the competing sub-populations are updated with copies of the best ANN models from the cell’s neighborhood. This communication relies upon the overlap of the cell neighborhoods. The solutions at each individual cell improve because of the training, while the spatial propagation of best solutions in each epoch prevents pathologiesfrom persisting.
The contributions in this chapter are 
1.
An overview and some definitions of adversarial evolutionary learning, with focus on evolutionary GANs.
2.
A comprehensive description of spatial coevolutionary GAN training, as an example of adversarial coevolutionary learning.
3.
An extensive discussion and demonstration of adversarial evolutionary learning for GANs. We focus on the impact of population, hyper-parameters, objectives, data, and operator diversity.
The chapter proceeds as follows. Section  13.2presents background and related work. Section  13.3describes LipizzanerAEL method and its variations. Section  13.4summarizes different experimental analyses performed on adversarial coevolutionary learning. Finally, Sect.  13.5draws the main conclusions and describes the future work.
13.2 Background and Related Work
First, we present an overview of the background and related work for AEL. Note that this chapter is an overview, not a full survey. Figure  13.2shows a categorization that we follow for AEL. Our division is Competitive Coevolutionary Algorithm  (CCA), Multi-agent Reinforcement Learning(MARL), Adversarial Machine Learning  (AML), Evolutionary Game Theory  (EGT), and Generative Adversarial Networks  (GAN). Wehave included GANs in the figure because we understand that GANs can be seen as a specific case of AEL in which two agents (ANNs) compete with each other.
Fig. 13.2
A categorization of Adversarial Evolutionary Learning. To distinguish the categories we use the properties regarding number of adversaries, adversary dynamics, and “Learning”, see Table  13.1. We focus on GANs  (gray box)
Table 13.1
Table of names and key properties in AEL categorization. In the Learning column: SL is supervised learning, UL is unsupervised learning, RL is reinforcement learning. The key evolutionary learning properties are population size, selection, and variation
Name
Adversarial learning
Evolutionary learning
#Adversaries
Dynamic
Learning
Population
Variation and selection
Generative adversarial network
1
Yes
