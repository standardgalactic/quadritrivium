Furthermore, ML can be applied to parent, mate, or survivor selection, considering factors such as population quality, diversity, compatibility among mates, and proximity to the optimum.
Lastly, let’s consider the application of ML methods to the outcomes of EC methods. One obvious application is using ML to synthesize the outcomes of evolutionary runs by filtering or selecting results. Additionally, clustering the outputs of one or multiple evolutionary runs can be valuable, especially in scenarios with multiple alternative solutions, such as multi-objective and many-objective optimization. ML-empowered visualization techniques, such as t-SNE [103], can also be powerful tools for presenting results to end-users and analyzing the evolutionary process. Furthermore, ML techniques, such as Large Language Models, can significantly expedite the analysis of EC processes, offering an important tool for researchers and practitioners in this field.
While there are several noteworthy works, we believe that the use of ML for EC is just beginning to blossom. Many of the areas we have identified remain largely uncharted, and hold potential for exploration. Moreover, even the territories that were explored in the early days of EML deserve to be revisited with the tools and knowledge of modern ML. By delving into these untapped areas and building upon existing knowledge, we can uncover new insightsand advancements in the field.
1.4.3 EC for ML Problems
In our perspective EC can be seen as a form of ML and, as such, several of the problems that are typically addressed by non-evolutionary ML can also be tackled by EC. Fig. 1.6summarizes the main categories and problem types that we identify in this context.
Fig. 1.6
EC for ML Problems
We consider four main categories, and corresponding typical problems: ‘supervised,’ ‘unsupervised,’ ‘reinforcement,’ and ‘meta-learning.’
In the ‘supervised’ learning category, EC has been successfully applied to address classic problems such as Classification, Regression, and Generation. Clustering is a prototypical ‘unsupervised’ learning problem solved by evolutionary means.
The domain of reinforcement learningis particularly well-suited for EC, since it is natural to see fitness as a consequence of the interaction between the individuals and their environment, resulting in a reward or penalization. While reinforcement learningscenarios can be tackled using canonical EC methods, they lend naturally to the application of co-evolutionary approaches, whether they are competitive, cooperative, or a combination of both.
It is worth noting that competitive co-evolution predates what is now known as adversarial AI, including Generative Adversarial Neural Networks (GANs). Furthermore, the competition between evolutionary and non-evolutionary ML approaches, including the co-adaptation of both types of models as a result of this competition, has a long history (see, e.g., [66] and [65] for early and recent examples, respectively).
Meta-learning is a significant challenge for ML and hence for EML. In this area we identify five subcategories: ‘ensemble’ learning, ‘federated’ learning, ‘life-long’ learning, ‘online’ learning, and ‘transfer’ learning. When it comes to ensemble and federated learning, collective intelligence models are an intuitive metaphor.
Life-long and online learningimplies the continuous adaptation of individuals. While this process is not achieved through evolutionary means in nature, computationally we have the freedom to explore EC approaches for these purposes. Nevertheless, if one wishes to follow the metaphor closely, evolving ML models and algorithms capable of life-long and online learningwould be a natural approach. Likewise, in the case of transfer learning, we can promote the evolution of ML models that exhibit knowledge transfer abilities. This can be accomplished by directly assessing this capability in the fitness function or by valuing characteristics that may facilitate transfer learning, such asmodularity, re-utilization of knowledge or pathways, incremental learning abilities, compactness or even physical plausibilityof the model.
Finally, we identify additional dimensions of analysis that are, to some extent, orthogonal to the previously identified categories. These dimensions include:
Single versus Many task—while in single task learning we focus on a specific task on many task learning the models should be able to perform multiple tasks, typically by using shared knowledge and patterns across tasks.
Passive versus Active learning—Passive learning assumes that all data is pre-labelled, whereas in active learning the model prompts the user interactively for labeling data. This type of interaction bears resemblance to certain forms of interactive EC, particularly semi-interactive ones, and can be a valuable opportunity for EC research.
Many versus Zero Shot learning—while in many shot learning we have several labeled training examples by class in zero-shot learning the challenge is to classify objects without any labeled training instances. This is typically attained by associating observed and non-observed classes through auxiliary information, e.g., a model algorithm trained to recognize horses may recognize zebras by using the auxiliary information that ‘zebras look like striped horses.’
1.5 Discussion of Main Branches of the Field
We shall start our discussion with a brief overview of the main branches before delving into more details of each one of them. Many evolutionary and, more generally, selectionist approaches have been studied in the context of brain function [29].
Due to the recent success of deep neural networks, a natural topic for EML is the question of how to improve neural networks with evolution. This field of EML is called neuroevolution. It addresses the question on all levels of the neural network from the synaptic level, through the neuronal level, to the connectivity patterns and higher level structures like neural architectures and even developmental aspects of the nervous system.
A second branch concerns itself with the algorithms for learning. There are various aspects of this, including the evolution of learning algorithms (meta-learning), efficiency aspects of training and learning, considerations of ensembles and competition and cooperation in learning as well as the evolutionary integration of tools into the ML pipeline.
A third branch of EML studies the results of ML algorithms. This refers to whether the results are fair, trustworthy, explainable, interpretable, usable, accessible etc. Many of these aspects require more analysis for measurement and optimization, with some still being under basic research for quantification.
The fourth main branch of EML considers ethical aspects, green EML, rules for using the systems etc. This is a less well-defined branch, that will, however, only gain more visibility over the next couple of years.
1.5.1 Neuroevolution
It is well known from Biology that synapses are complex systems that must have undergone substantial evolution to reach the current status [25]. Therefore, it might be the case that the connections in artificial neural networks should also be subjected to evolution to yield more complex and better organized connectivity in the networks [110].
Moving to the level of the neuron, it has been found that neurons themselves are highly complex and individual entities that require for their simulation entire deep artificial neural networks [11]. It should, therefore, not astonish when multiple aspects of neurons are subjected to various optimization approaches from the domain of evolutionary algorithms. For an example, see [105].
Moving up a further level, to network/architecture level, an entire new field called Neural Architecture Search (NAS) has emerged which benefits from evolutionary approaches [59]. This entails the design of networks with sparse layers, connections that skip over layers or recurrent connections between layers. Multi-objective algorithms have been recruited to pressure evolution toward simple solutions [62].
Finally, developmental aspects of neural networks, like growing neural network structures from small structures, have become an important issue. In Biology, the evo-devo movement has taken firm roots in research examining the brain and its architecture [42]. Taking inspiration from developmental neuroscience, EML addresses the building of artificial neural structures [68].
1.5.2 Learning Algorithms
The evolution of learning algorithms from scratch has recently shown major success for EML. Real et al. have been able to demonstrate the evolution of the backpropagation learning algorithm from data and simple mathematical building blocks like matrices and vectors [82]. Clustering algorithms have for a long time been considered material for evolution [2]. The newest in meta-learning is the evolution of transformers[93].
While much of this work features under the term ‘meta-learning,’ other aspects of learning algorithms, like hyperparameters or the amount and sequential organization of training can also be subject to evolution [28].
A whole different area is constituted by ensemble learning methods [22, 84], where not just one, but a number of models are used to make progress on a learning task. Evolutionary ensemble learning is discussed in one of the chapters here.
Apart from ensembles, which constitute part of cooperative algorithms, competitive or adversarial algorithms are also used to generate different solutions [49]. This whole area in EML is known under the topic co-evolutionary algorithms [63], and contains, in addition to cooperative also competitive systems, like adversarial algorithms that make use of two different populations [39]. Game-like algorithms where players are modeled [112] are also relevant for evolutionary studies.
Finally, we need to mention the evolutionary designof pipelines which might consist of many different learning methods at different stages of the ML process. TPOT[72] is a famous example of this line of work which claims its rightly place in the field of automatic machine learning.
1.5.3 Learning Results
One key aspect of learning systems is whether the results of such learning can be interpreted by humans or explained and communicated to others. Interpretable [23] and explainable [10] models will be required in many fields of application, like business and government, and neural network solutions have some problems with those required characteristics. Evolution can pressure solutions toward such criteria, if they are quantifiable. Also, genetic programming, as one of the major EML methods can be based on symbolic structures and is naturally geared towardexplainability [67].
Further requirements for learning results include trustworthiness, user friendliness for AI novices, usabilityand accessibility. All of these, as long as they can reasonably be defined, can be part of a multi-objective EML approach.
1.5.4 Ethical Aspects
We end this discussion by mentioning a number of aspects of EML that are somewhat less clearly defined, yet merit continued research and investigation. Much of the work currently going on in the field of ML is based on case studies [60, 73]. However, fairnessand biashave been refined gradually to the point where they can be considered measurable quantities [76].
But other aspects of ethical behavior, not subsumed under the previous topics, are ideas about green ML [43, 100]. In this line of inquiry, how to learn as quickly as possible, with as few CPU/GPUcycles as possible, is a quantifiable goal open to EML (see, e.g., [78]).
This also includes the systematic measurement and reporting on energy requirements of ML generated models [44, 57] which can help to design efficient hardware and software approaches [3, 97]. While not yet systematically deployed, multi-objective EML approaches hold much promise for this field.
1.6 Open Problems
Evolutionary Machine Learning is a young field, despite its roots in the 1980s. This is mostly due to the technical progress that allowed powerful neural networks to solve difficult problems. The research questions in EML can be roughly categorized into two classes: Methodological open problems and practical open problems. Below we provide a list of where we see the major open problems in each of these categories. This is not a complete list, but it is our attempt to take stock of the current status and outline the problems we currently see in this field.
We see the following methodological problems:
The need to create generalized and robust solutions. Some research has started to model generalization ability [8], but this area is far from explored.
The need to process multi-modal information, i.e., information from different types of sensing devices [53, 79].
The need to fuse models. This is related to multi-modal information processing. Assuming that each mode develops its distinct models, how can they be integrated into a cohesive picture of the situation [13, 111]?
The need to make use of transfer and ‘lifelong’ learning. The former refers to the need to transfer models from one task to a related task [107, 113]. The latter refers to the problem of avoiding the unlearning of skills when tasks change, with the potential that earlier tasks are necessary to perform later again [74].
Meta-learning—evolution in the natural world has shown its ability to create learning methods used by organisms. How can we recruit evolution to evolve learning methods for entire classes of problems encountered in the world [75, 82, 104]?
Connected to the above questions, but more on the methodological level—how can we evolve modular and hierarchical models of the world? This is a very important question when scaling up to real-world problem dimensions [15, 50, 90].
Models are only approximations of the systems they are supposed to reflect. How to close the reality gap [86]?
There are ethical questions around the use of artificial intelligence: Can we secure its beneficial use while avoiding its misuse? What do we do to control the potential for AI weapons [21, 64]?
Given the growth of applications in AI, how to avoid a development akin to a nuclear explosion, where AI/ML creates a strong positive feedback loop without leaving humans able to control it? Is that a realistic perspective [12]? There are also questions raised around sentience.
The need to set up rules and methods for the interaction of AI/ML systems. How can AI/ML systems be allowed to negotiate outcomes among themselves, to the benefit of human users [19]?
On the more practical side, we see the following questions:
How can we minimize the resource requirements of (E)ML tools? In particular, the need for energy, data storage, data and processing power.
How can we ensure the acceptability of AI/ML suggested solutions? The usefulness of EML must be demonstrated in the context of human needs.
How can we make sure that AI/ML/EML solutions are available to all users, not just the privileged few with large HPC centres?
How can the quality of data be improved, or possibly curated automatically?
How can we use EC to create models that generalize better?
How can a solution for problems in principle be scaled up to the real-world domains it needs to prove to work in?
How can the interaction between humans and machines be organized in a way that prevents us from becoming machines?
There are a number of even more fundamental questions, however, that will possibly remain unresolved for the foreseeable future, but still might be useful to guide inquiry.
The first we see is how we can handle novelty, and the open-endednessthat comes with learning systems in interaction with humans and their societies. While open-endednessis a feature of our natural world under evolution, human-made (artificial) systems do not really possess open-endednessyet. They are closed systems that need repair, rather than open systems autonomously satisfying their needs. Can we manage to keep it that way, or do we need to take precautions to make open-ended systems still fit into a world we can call ours?
The second question sounds very practical or methodological, but it seems to us indeed a fundamental question: How can we understand the interaction between evolution and learning, how can we make use of it in the context of artificial learning and evolving systems? Is there another level of yet to be found phenomena that accelerates adaptation beyond learning?
Finally, we are used to discuss intelligence as a key concept of human existence, with some authors floating the idea of super-intelligence [14]. It is likely that entities with higher intelligence than humans can exist, e.g., humans with augmented intelligence. Early work on the intelligence amplifier by W. Ross Ashby [4] and intellect augmentation by Douglas Engelbart [26] comes to mind. But is there a limit to intelligence [24, 33], and at which level will it enforce itself? We believe that evolution teaches us that there is such a limit, but whether we are close is subject to exploration
Acknowledgements
This work is funded by the FCT–Foundation for Science and Technology, I.P./MCTES through national funds (PIDDAC), within the scope of CISUC R &D Unit–UIDB/00326/2020 or project code UIDP/00326/2020.
References
1.
Ackley, D., Littman, M.: Interactions between learning and evolution. In: Langton, C., Taylor, C., Farmer, J., Rasmussen, S (eds.) Artificial Life II, pp. 487–509. Addison-Wesley (1991)
2.
Al-Sahaf, H., Bi, Y., Chen, Q., Lensen, A., Mei, Y., Sun, Y., Tran, B., Xue, B., Zhang, M.: A survey on evolutionary machine learning. J. Royal Soc. New Zealand 49(2), 205–228 (2019)
3.
Amershi, S., Begel, A., Bird, C., DeLine, R., Gall, H., Kamar, E., Nagappan, N., Nushi, B., Zimmermann, T.: Software engineering for machine learning: A case study. In: 2019 IEEE/ACM 41st International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP), pp. 291–300. IEEE (2019)
4.
Ashby, W.R.: An Introduction to Cybernetics. Chapman and Hall (1956)
5.
Bailey, B.: The Impact Of Moore’s Law Ending (2018). https://​cacm.​acm.​org/​news/​232532-the-impact-of-moores-law-ending/​fulltext(Last accessed Oct 12 2022)
6.
Baluja, S., Pomerleau, D., Jochem, T.: Towards automated artificial evolution for computer-generated images. Connect. Sci. 6, 325–354 (1994)
7.
Banzhaf, W., Nordin, P., Keller, R., Francone, F.: Genetic Programming–An Introduction. Morgan Kaufmann (1998)
8.
Barbiero, P., Squillero, G., Tonda, A.: Modeling generalization in machine learning: A methodological and computational study. ArXiv preprint arXiv:​2006.​15680(2020)
9.
Bedau, M.A.: Weak emergence. Philos. Perspect. 11, 375–399 (1997)
10.
Belle, V., Papantonis, I.: Principles and practice of explainable machine learning. Front. Big Data 39 (2021)
11.
Beniaguev, D., Segev, I., London, M.: Single cortical neurons as deep artificial neural networks. Neuron 109(17), 2727–2739 (2021)
12.
Benthall, S.: Don’t fear the reaper: Refuting Bostrom’s superintelligence argument. ArXiv preprint arXiv:​1702.​08495(2017)
13.
Blasch, E., Pham, T., Chong, C.-Y., Koch, W., Leung, H., Braines, D., Abdelzaher, T.: Machine learning/artificial intelligence for sensor data fusion-opportunities and challenges. IEEE Aerosp. Electron. Syst. Mag. 36(7), 80–93 (2021)
14.
Bostrom, N.: Superintelligence. Oxford University Press (2016)
15.
Callebaut, W., Rasskin-Gutman, D., Simon, H.A.: Modularity: Understanding the Development and Evolution of Natural Complex Systems. MIT Press (2005)
16.
