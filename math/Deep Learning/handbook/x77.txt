University of Coimbra, CISUC, DEI, Coimbra, Portugal
Inês  Valentim
Email: valentim@dei.uc.pt
Nuno  Lourenço(Corresponding author)
Email: naml@dei.uc.pt
Nuno  Antunes
Email: nmsa@dei.uc.pt
Abstract
Whenbuilding Machine Learning models, either manually or automatically, we need to make sure that they are able to solve the task at hand and generalize, i.e., perform well on unseen data. By properly validating a model and estimating its generalization performance, not only do we get a clearer idea of how it behaves but we might also identify problems (e.g., overfitting) before they lead to significant losses in a production environment. Model validation is usually focused on predictive performance, but with models being applied in safety-critical areas, robustness should also be taken into consideration. In this context, a robust model produces correct outputs even when presented with data that somehow deviates from the one used for training, including adversarial examples. These are samples to which small perturbations are added in order to purposely fool the model. There are, however, limited studies on the robustness of models designed by evolution. In this chapter, we address this gap in the literature by performing adversarial attacks and evaluating the models created by two prominent NeuroEvolutionmethods (DENSER and NSGA-Net). The results confirm that, despite achieving competitive results in standard settings where only predictive accuracyis analyzed, the evolved models are vulnerable to adversarial examples. This highlights the need to also address model validation from an adversarial robustnessperspective.
15.1 Introduction
Successful applications of Machine Learning (ML) models are nowadays found across a variety of fields, from computer vision  [16, 23] to natural language processing  [42]. A primary goal while building these models is to have them perform well on unseen data, which is typically referred to as their generalization performance. Since Evolutionary Machine Learning (EML) has ML at its core, this objective also applies to the models evolved by such approaches.
Validating the models before their deployment allows us to estimate their generalization performance and helps us keep any expectations regarding their performance in check.1Appropriate validation methods guarantee that that estimate is as unbiased as possible (e.g., by avoiding leaking test data during training) and help identify issues like overfitting. In addition, we may uncover scenarios under which a model still needs improvement (e.g., samples that are repeatedly being incorrectly classified). On the contrary, adopting incorrect validation practices, or not validating a model at all, can result in overoptimistic expectations that can ultimately lead to significant losses, which may even be fatal in the case of safety-critical systems like autonomous vehicles.
Although most EML approaches are concerned with the predictive performanceof the models they evolve  [54, 61], there are other aspects that should not be neglected when considering real-world applications. Namely, it is likely that a model has to process data that deviates from the one seen during training. Even under those circumstances, a model should be able to provide correct outputs, i.e., it should be robust. Model robustness has many possible interpretations, but we focus on adversarial robustness, i.e., robustness against inputs tampered with small perturbations that cause an attacked model to produce incorrect outputs  [9, 15]. These perturbed inputs are commonly known as adversarial examples[21, 52].
In this chapter, we present general recommendations and methods to conduct model validation in the scope of EML. More importantly, we argue that it is crucial to look beyond the standard setting where predictive performanceis the core concern and also evaluate the robustness of the models, specifically their robustness against adversarial examples. Thus, we conducted a study where we assessed the adversarial robustnessof Artificial Neural Networks (ANNs) designed by NeuroEvolution (NE) approaches. More concretely, we evaluated Convolutional Neural Networks (CNNs)designed by DENSER  [3] and NSGA-Net  [35] for the CIFAR-10image classificationtask  [31].
Our results suggest that models designed by evolution are susceptible to adversarial attacks, much like ANNs designed by hand, whose vulnerabilityto adversarial exampleshas been widely investigated. Nevertheless, we also found that one of the models under evaluation shows some resistance to adversarial examplesin one of the threat modelsconsidered in the experiments. On the one hand, our findings corroborate the need to look at model performance in a broader sense, as even highly accurate models can perform poorly under scenarios that deviate from in-lab environments with ordinary data. On the other hand, these results are also promising when it comes to leveraging NE to search for ANNs that are inherently more robust.
The remainder of this chapter is organized as follows. In Sect.  15.2, we give general recommendations for validating models designed by evolution and present related work on benchmarking Evolutionary Neural Architecture Search (ENAS) approaches. In Sect.  15.3, we introduce key concepts related to model robustness, with a special emphasis on adversarial robustness. Section  15.4is dedicated to our study of the adversarial robustnessof NE approaches, including the methodology, experimental setup, and findings. Open challenges are identified and summarized in Sect.  15.5. We discuss the main findings and future directions in Sect.  15.6, which also concludes the chapter.
15.2 Predictive Performance Assessment
A vast majority of EML approaches focuson optimizing predictive performance  [54, 61]. That is the case of NE approaches for classification tasks, which often use predictive accuracyas the target metric for the fitness of each individual. The experimental design of such approaches must be done with caution so as to be able to get reliable estimates of the generalization performance of the evolved models. Given that EML approaches are, in general, computationally expensive, some standard validation methods used in ML (such as nested cross-validation[44]) may be unviable. Be that as it may, some general best practices should still be followed.
In this section, we present a general framework for the validation of models designed by evolution. The suggested method is particularly suitable for approaches that evolve models for supervised tasks, specifically ANNs. Having said that, the recommendations are general enough to be easily adapted to different scenarios and application domains. We also present a benchmarking platform aimed at ENAS proposals, an important step toward allowing fairer comparisons between models found by different approaches.
15.2.1 General Framework
To better estimate the generalization performance of an evolved model, different sets of data should be used at different stages of the EML pipeline, as illustrated in Fig.  15.1. As in any standard ML application, we suggest a first partitioning of a dataset into a training set and a test set. Common benchmark datasets (e.g., CIFAR-10)usually have pre-defined sets of training and test data. Only the training set should be used within the evolutionary process. It is imperative that the original test set is kept aside and is only used at the end of the evolutionary process, so as to obtain an unbiased estimate of the performance of the best individuals on unseen data. Moreover, by keeping the original partitioning intact, comparisons with other approaches are more likely to be feasible, since the test performance is measured in the same subset of data which, in turn, has not been used during training or model selection. Making the partitioning of the dataset publicly available can further facilitate proper validation and comparisons.
Fig. 15.1
Model validation method for evolutionary machine learning
In addition to this partition, we advise dividing the original training set into three data subsets, as follows: 
1.
Evolutionary training set
It is used during the training phase, e.g., to learn the parameters of an ANN in the context of an ENAS approach.
2.
Evolutionary control set
It is also used during training, but to keep track of the progress made. A concrete example is the use of this subset to control an early stopping mechanism.
3.
Fitness evaluation set
It is used to evaluate the fitness of the trained individuals, thus guiding the evolutionary process.
It is also important to mention that any data augmentationtechnique, where additional data is introduced, should only be applied to the training data. The same applies to computing summary statistics neededfor data pre-processing methods like feature normalization. As such, the data subsets should first be created, and only then should the augmentation methods be applied. Failing to do so can cause data leakage, which jeopardizes the validityof the results. Nonetheless, it is possible to perform test-time augmentation by using and combining the model predictions for multiple modified versions of each sample in the original test set to get a final outcome. Again, the key point to follow is to only perform augmentation after splitting the data.
Due to the stochastic nature of evolutionary approaches, the reported results should also take more than one evolutionary run into account. Different random seeds should be used for the partition of the original training set across these runs. Besides computing average metrics of the best individuals found across evolutionary runs, tracking how fitness evolves through generations can also give important insightsinto the effectiveness and efficiency of the evolutionary process. After the best individual across runs has been selected, a final training can take place with the original training set being merged back together. In an ENAS approach, the last training could use gradient methods to optimize the parameters (i.e., weights) of the best architecture (as measured on the fitness evaluation set).
15.2.2 Benchmarking Evolutionary Neural Architecture Search
Besides looking at an evolved model in isolation to assess how well it performs at the task at hand, it is also fundamental to compare different approaches. By doing so, one can get a better understanding of how they position themselves in the literature, as well as track advances in the field.
Nevertheless, making these comparisons in a fair and reliable manner is not trivial. Firstly, most implementations are not publicly available, making it difficult to replicate the results of the original papers  [61]. Moreover, the approaches are often complex and incorporate specific details and technicalities, which further adds to the difficulty of implementing them faithfully  [34]. Additionally, the original works might not clearly specify the parameters used to obtain the reported results. There is also the issue of comparing the efficiency of the evolutionary processes. Given the differences across approaches, for instance, in terms of population size or the number of generations  [61], it is not straightforward to make this comparison.
Variations in how the dataset is partitioned (as described in Sect.  15.2.1) and, consequently, the size of each data subset, also contribute to it being difficult to make unbiased and fair comparisons between methods. Some EML problems can pose even further challenges. For instance, in NE there are differences between the data preparation and pre-processing techniques adopted by each approach, not to mention the distinct details pertaining to the training of the ANNs themselves (e.g., number of epochs, batch size, and optimizer). Besides predictive performance, another factor commonly considered in this scenario is the search cost (expressed by the number of GPUdays). Given the differences in the hardware used to run the experiments, this is yet another aspect to take into consideration  [61].
BenchENAS[61] addresses these issues by proposing a framework to benchmark ENASapproaches. In particular, the settings regarding the data and the training strategy can be fixed across approaches. The number of function evaluations, which is often used to assess the efficiency of the evolutionary search, is also the same for the different methods. Moreover, BenchENASemploys strategies to reduce the time needed for fitness evaluations, namely by taking advantage of the use of multiple GPUs, when available, to train ANNs in parallel.
In sum, BenchENASpromotes sound comparisons of different ENAS methods by providing a platform under which models are trained and evaluated under the same experimental conditions. Nevertheless, BenchENASmakes these comparisons based on a fixed set of objectives and metrics (e.g., accuracy, number of model parameters, and GPUdays for each evolutionary run). Therefore, it is not suitable for more complex evaluations, e.g., robustness assessments. Furthermore, the existing platform does not seem to offer the option to implement a customized validation strategy in terms of dataset partitioning.
15.3 Beyond Predictive Performance: Robustness
Thesuccess of ML models led to their adoption in scenarios where concerns other than predictive performancemust be addressed. One such concern pertains to how models behave at test time when processing atypical or peculiar data, for instance, as a result of distributional shifts or perturbations. In this context, the robustness of a model refers to its ability to provide correct outputs even if presented with such data.
Given the variety of peculiarities the test data can show, robustness has many facets. In this chapter, we focus on adversarial robustness, more precisely robustness against small -norm perturbations. Moreover, we mainly discuss robustness in the computer vision domain.
15.3.1 Adversarial Robustness
Adversarialexamples[21, 52] are carefully crafted samples that make a model produce incorrect outputs  [15, 28]. Throughout this chapter, we adopt the definition of adversarial examplesdescribed in  Szegedy et al. [52], where the authors found that it was possible to create an input similar to a valid data point xto which a model gives a highly different prediction  [20]. Figure  15.2illustrates this phenomenon.
Fig. 15.2
An adversarial example. On the left is a clean example from CIFAR-10, correctly classified by a CNN; on the right is an adversarial examplewhere perturbations were added to the benign image making the same model misclassify it
Throughout the years, several methods have been proposed in the literature to craft such examples, under different threat models. From a securityperspective, these threat models can be defined based on the goals, knowledge, and capabilities of the adversary  [8].
15.3.1.1 Threat Models
A distinction can be made based on the adversary’s knowledge about the model: architecture and parameters, training algorithm and training data, randomness at test time, and allowed level of query access  [4]. White-box attacks, like gradient-based attacks  [36], need full access to the model. Others, like decision-based attacks  [6], are black-box attacksthat solely need access to the final output, such as the predicted label. Yet another type of black-box attacks, transfer-based attacks  [43], use a substitute model to craft adversarial exampleswhich are then used to attack the real target model. These transfer-based attacks exploit the fact that adversarial examples generated for one model have a high probability of also being misclassified by another model (trained with a different training set or even with a different architecture)  [21, 33, 36, 52]. This property is known as transferability
We can further distinguish between untargeted and targeted attacks. In the case of image classification, the goal of an untargeted attackis simply to make the model predict a class different from the true label of a given instance, while the goal of a targeted attackis for the misclassification to be into a specific class  [9]. Formally, xis an input with correct label yand Cis the classifier under attack. In the untargeted setting, an adversarial exampleis such that . On the other hand, given a target , a targeted attackwould aim at crafting such that 
An adversarial exampleis usually created by adding some perturbation to a benign input  [28]. Constraints are usually imposed on the capabilities of the adversary in terms of the maximum perturbation that can be added, so that remains close to the original input and its true label remains unchanged  [8].
15.3.1.2 Attack Design
In the image domain, a common approach is to use an -norm to bound the perturbations such that , where is the perturbation budget and usually [8, 14]. The -norm limits the number of pixels that can be modified, but does not restrict the magnitude of the changes. The -norm imposes limits on the sum of the magnitudes of the changes. The -norm measures the Euclidean distance between and x, and allows any number of pixels to be perturbed. The -norm limits the magnitude of the changes, but allows any number of pixels to be altered.
While some white-box attackstry to find minimal adversarial perturbations, others try to maximize some loss functionwith respect to the true label (typically the cross-entropyloss)  [4, 14]. We present some of the latter methods assuming an untargeted setting, which corresponds to the threat modelused in the experiments described in Sect.  15.4. One can argue that untargeted attacksare less powerful than targeted ones  [9], but on the other hand, it is harder to defend against attacks with no specific target  [4].
The Fast Gradient Sign Method (FGSM)  [21], a one-step gradient-based attack optimized for the -norm, generates an adversarial exampleas
where is the gradient of the cross-entropyloss with respect to the input image. When this method is optimized for the -norm, we get the Fast Gradient Method (FGM) which generates an adversarial exampleas
In both cases, the perturbed image is clipped to the valid data range. Figure  15.3shows examples of adversarial examplesgenerated with the FGSM and the FGM attacks on the same clean image from  CIFAR-10.
Fig. 15.3
Adversarial examplesgenerated with FGSM and FGM. On the left, the original image without any perturbation
The methods that follow consider adversaries, but these definitions can be easily adapted to the case where bounds are imposed, similar to what is done with FGM.
A straightforward extension of the FGSM attack is to take multiple small steps (with step size ) and clip the result by at each iteration. The clipping operation also takes into account the valid range of data values. This results in the Basic Iterative Method (BIM)  [32] which can be defined as
The Projected Gradient Descent (PGD) method  [36] is another iterative attack which only differs from BIM on how is set. Instead of starting from the original input, a random perturbation bounded by is generated and added to x
In an attempt to stabilize the update directions and escape from local optima, the Momentum Iterative Fast Gradient Sign Method (MI-FGSM)  [14] incorporates momentum  [51] into the BIM method. The Auto-PGD (APGD) method  [11] is a variation of the PGD attack which adjusts the step size in an automated way. The authors of APGD also proposed an alternative to the cross-entropyloss called Difference of Logits Ratio (DLR) loss. In addition to being invariant to shifts of the logits, the DLR loss is rescaling invariant  [11].
One of the most notorious attacks aimed at finding minimal perturbations is the Carlini and Wagner attack with distortions  [9]. The targeted version of the attack searches for wthat solves the following objective:
where a binary search over cis used to find the minimal perturbation that results in a successful attack and where fis defined as
with tbeing the target class and being the output of the layer before the softmax layer, i.e., zare the logits.
Regarding attacks that require less knowledge about the target model than white-box ones, we highlight the Boundary Attack  [6] and the Square Attack  [2].
Another class of attacks, known as Universal Adversarial Perturbations (UAPs), tries to find a single perturbation vector that can fool an ANN on most samples of the input distribution  [38]. Crafting adversarial examplesusing UAPs is more efficient than attacks that operate on an instance basis, which require the computation of a perturbation from scratch for each sample. The fact that UAPs are also transferable across different model architectures poses additional threats to the robustness of ANNs  [38]. However, the fooling rates of methods based on UAPs tend to be lower in comparison to per-instance attacks  [38].
There is also a line of work that uses evolution-based methods to generate adversarial examples. Two noteworthy works are the One-Pixel Attack  [49] and the GenAttack  [1]. The One-Pixel Attack proposed by Su et al. [49] uses Differential Evolution (DE)  [48] to generate attacks where a single pixel of an image can be modified. GenAttack  [1] uses Genetic Algorithms (GAs) to generate adversarial samples in a targeted setting. Both of these approaches are gradient-free methods, but they do require access to the class probabilities (output of the softmax layer).
While the vast majority of the literature studies norm-constrained perturbations, where small norms act as proxies of human perception, other notions and threat modelshave been discussed and deserve to receive attention from the community  [19]. As examples of such alternative scenarios, we have the attack on traffic signs proposed by Eykholt et al. [17], as well as the adversarial patches from Brown et al. [7].
15.3.1.3 Defenses and Assessment
Several defenses against adversarial exampleshave been proposed in the literature. Such defenses tend to be designed to be robust to one specific threat model[8]. Adversarial training[36] and its variants have shown the most promising results when it comes to increasing the robustness of models. The main idea behind these methods is to incorporate adversarial examplesin the training procedure of a model. Another family of defenses comprises methods that, instead of making the ANNs themselves more robust, aim at detecting adversarial examplesduring inference time  [37]. Many more proposals and approaches can be found in the literature, and so, this is by no means an exhaustive review of all existing work on adversarial defenses. Although more advances have been made in recent years, the 2019 survey by Yuan et al. [64] gives a comprehensive overview of the topic.
Adversarial robustnessevaluations typically consist of heuristic approaches that estimate the robustness of a model by performing adversarial attacks  [10]. One resorts to this approximation since computing it exactly is usually intractable  [8, 10]. Nevertheless, there is a line of work regarding provable approaches that compute exact bounds of the adversarial robustnessof a model  [30, 55]. These methods fall outside the scope of this chapter.
It is important to mention that performing a correct robustness evaluation is a difficult and error-prone task, especially when dealing with models that incorporate some kind of defense. Many defense proposals conducted incorrect or incomplete evaluations and, as a consequence, were circumvented shortly after publication  [4, 57]. In particular, some defenses are able to cause gradient-based attacks to fail, but can be broken with the methods proposed by Athalye et al. [4] or with gradient-free attacks, such as the GenAttack  [1]. In this scenario, an evaluation that only uses gradient-based attacks would be misleading.
RobustBench  [10] is a relatively recent initiative to keep track of the progress made in adversarial robustness. It aims at benchmarking adversarialrobustness under different threat models, namely -robustness with . The authors adopt AutoAttack  [11], a heuristic evaluation method which relies on an ensemble of white-box and black-box attacks. Although this method can be used to estimate the adversarial robustnessof ANNs trained in normal regimes without adversarial considerations, the main focus of the work are models that incorporate some kind of defense against adversarial attacks. In addition to a leaderboard aggregating the evaluations of several robustness-enhancing proposals, a Model Zoo containing pre-trained models from top entries of the leaderboard is also available. Nevertheless, some restrictions are introduced on the considered models, mainly to avoid overestimating robustness due to adaptive evaluations not being performed  [10].
In Benz et al. [5], a comparison is made between CNNsand more recent architectures, such as the Vision Transformer  [16] and the MLP-Mixer  [56], which have achieved promising results in computer vision tasks. This work relates to ours in that architectural differences are at the core of the analysis.
The work by Huang et al. [27] focuses on evaluating the impact of network width and depth on the adversarial robustnessof a Wide Residual Network. This ANN is adversarially trained, and its width and depth are modified at different stages of the model. Their results suggest that reduced width and depth at the last stage of the model can be beneficial.
The experiments conducted by Devaguptapu et al. [12] also look at adversarial robustnessfrom an architectural perspective and include both manually designed architectures and architectures found by NASapproaches (such as an NSGA-Net model). However, only -robustness is considered in this work. Surprisingly, the accuracy of the handcrafted models for CIFAR-10does not drop to zero under attack, which raises questions in terms of the adequacy of the evaluation methodology.
There is also a growing bodyof work that uses NAS approaches, including NE, to find robust models  [18, 22, 46, 59]. As pointed out by Devaguptapu et al. [12], adversarial trainingis often incorporated in these studies, making it difficult to assess the role of architectural aspects on the robustness exhibited by the models. In the case of Vargas and Kotyan [59] and Geraeinejad et al. [18], adversarial robustnessis explicitly included in the objective function. Thus, it is difficult to understand if these NE approaches would be able to find inherently more robust models than the ones crafted by humans without specifically driving the search toward that goal.
15.3.2 Other Forms of Robustness
In the examples seen so far, small perturbations are added to benign data samples with the goal of producing labeling errors. An underlying idea is that the perturbed examples remain semantically close to their clean counterparts, meaning that, for instance, we want a perturbed image of a dog to still look like a dog to a human. However, there are other possibilities when it comes to generating images that are capable of fooling a model. Namely, it is possible to generate images unrecognizable to humans or that seemingly look like noise, but that a model classifies as some known class with high confidence. A noteworthy example is the work by Nguyen et al. [40], where such images were generated using evolutionary algorithms and one of two possible representations: a direct encoding of the pixel values as integers, and an indirect encodingbased on Compositional Pattern Producing Networks (CPPNs)  [47].
Adversarially perturbed inputs are not the only cause for concern regarding model robustness. Other relevant data distortions include the common image corruptions of the benchmark designed by Hendrycks et al. [25]. It comprises 15 types of corruption from four categories, namely noise, blur, weather, and digital distortions. Each corruption type has five severity levels. Gaussian noise, motion blur, fog, and brightness are examples of distortions from each category. RobustBench  [10] also covers this type of image perturbation.
One can argue that these common corruptions and perturbations are unrealistic. Bearing in mind this limitation, the 3D Common Corruptions (3DCC) set  [29] has recently been proposed. The 3DCC set is more targeted toward datasets with full scene geometry information, but can also be applied to datasets lacking this 3D information, like ImageNet[45].
An underlying problem of the aforementioned data distortions is that they are synthetic, meaning that the data samples are obtained by applying well-defined transformations that are generated computationally  [53]. In Taori et al. [53], the authors argue that the models should also be robust to distribution shifts that occur naturally in the real world, such as changes in lighting or scene compositions. To assess model robustness against these natural distribution shifts, they then use datasets of unmodified images grouped into three categories: consistency shifts, dataset shifts, and adversarially filtered shifts.
Another problem plaguing the robustness of ML models is the open-world nature of real scenarios, given that a closed-world view is often adopted during training  [62]. Taking the case of image classificationtasks, like the one associated with the CIFAR-10dataset, models will always output a class from a closed set of possibilities. They are often highly confident about that prediction even if the input image does not fit in any of the classes the models are expected to identify  [62].
In real-world applications, it is most likely that changes will occur over time, namely when it comes to the relationships between the inputs and the target outputs of a model  [39]. Such changes, often called concept drift  [39, 60], can cause model performance to deteriorate, in part due to the model typically being static and trained in historical data where the new relationships were not present  [60]. Methods to detect concept drift and address it remain open challenges.
15.4 An Empirical Evaluation of the Adversarial Robustness of NeuroEvolution Methods
Models generated by NE approaches achieve competitive resultsperformance-wise, but their robustness to adversarial exampleshas received limited attention. We can assume that these evolved models also suffer from this vulnerability, but we cannot be sure whether the evolutionary search yields ANNs that offer some kind of resistance to adversarial examples, and, if that is the case, to what extent it goes.
For that reason, we conducted an empirical evaluation of the adversarial robustnessof models designed by two NE approaches  [58]. The experiments and results presented herein are largely based on our previous work  [58]. The main goal of this study was to understand whether these evolved models show any inherent robustness even when that objective is not included in the evolutionary search procedures. We focused on image classificationtasks and CNNs, and performed such an analysis by attacking pre-trained models made publicly available by the authors of the corresponding NE approaches. Two threat models basedon perturbations were considered.
Besides adversarial robustnessnot being included in the fitness functions of any of the selected approaches, no defense mechanism (e.g., adversarial training)is applied to the models in any phase of the process. As such, the models were not specifically designed to be adversarially robust.
15.4.1 Methodology
We followed the approach depicted in Fig.  15.4to assess the adversarial robustnessof the selected models. Only the test set of a dataset is used to generate adversarial examples. Furthermore, the attacks are only performed on test samples that, without being perturbed, are correctly classified by the model being evaluated, as depicted by the steps marked with (1)
Fig. 15.4
Approach to assess the adversarial robustnessof NeuroEvolution methods
Different models expect the data to undergo distinct pre-processing procedures. We generate adversarial examplesin a more general scenario where a single pre-processing step is applied to the images to normalize the pixel values to the interval , as shown by steps (2a)and (2b). In this case, additional pre-processing procedures are only applied after the adversarial perturbations have been added to the images, as illustrated by step (2c). Additionally, we consider a scenario in which the adversarial examplesare crafted after all the pre-processing steps are applied to the images, as depicted by steps (3a)and (3b). Under these conditions, the perturbation budget refers to the distance in the space in which the first layer of a network expects the data to be.
Pixel values are typically represented by integers between 0 and 255, but we model them as real numbers. The discretization of the perturbed images can decrease the success rate of the attacks mainly if the perturbations are too small. Thus, our methodology also accounts for the impact of converting the pixel values of the perturbed images back to valid integers, as represented by steps (4a), (4b), and (4c)
The analysis of the models under attack includes computing their accuracy on the perturbed images (which is closely related to the success rate of the attacks), as well as studying the corresponding confusion matrices.
15.4.2 Experimental Setup
