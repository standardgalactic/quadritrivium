77.
Van Der Maaten, L., Postma, E., Van den Herik, J., et al.: Dimensionality reduction: a comparative review. J. Mach. Learn. Res. 10(66–71), 13 (2009)
78.
Ventura, S., Luna, J.M.: Pattern Mining with Genetic Algorithms, pp. 63–85. Springer International Publishing, Cham (2016)
79.
Wakabi-Waiswa, P.P., Baryamureeba, V.: Extraction of interesting association rules using genetic algorithms. Int. J. Comput. ICT Res. 2(1), 26–33 (2008)
80.
Wang, J., Biljecki, F.: Unsupervised machine learning in urban studies: a systematic review of applications. Cities 129, 103925 (2022)
81.
Wu, S.X., Banzhaf, W.: The use of computational intelligence in intrusion detection systems: A review. Appl. Soft Comput. 10(1), 1–35 (2010)
82.
Xue, B., Zhang, M., Browne, W.N., Yao, X.: A survey on evolutionary computation approaches to feature selection. IEEE Trans. Evolut. Comput. 20(4), 606–626 (2015)
83.
Xue, B., Zhang, M., Dai, Y., Browne, W.N.: PSO for feature construction and binary classification. In: Proceedings of the 15th Annual Conference on Genetic and Evolutionary Computation, pp. 137–144 (2013)
84.
Yan, X., Zhang, C., Zhang, S.: Genetic algorithm-based strategy for identifying association rules without specifying actual minimum support. Expert Syst. Appl. 36(2), 3066–3076 (2009)
85.
Yin, J., Wang, Y., Jiankun, H.: A new dimensionality reduction algorithm for hyperspectral image using evolutionary strategy. IEEE Trans. Industr. Inf. 8(4), 935–943 (2012)
86.
Zhang, M., Lee, D.-J.: Efficient training of evolution-constructed features. In: Proceedings of the 11th International Symposium on Advances in Visual Computing, ISVC-2015, Part II, pp. 646–654. Springer (2015)
87.
Zhang, M., Gong, M., Chan, Y.: Hyperspectral band selection based on multi-objective optimization with high information and low redundancy. Appl. Soft Comput. 70, 604–621 (2018)
88.
Zhao, Q., Bhowmick, S.S.: Association Rule Mining: A Survey, vol. 135. Nanyang Technological University, Singapore (2003)
89.
Zhou, M., Duan, N., Liu, S., Shum, H.-Y.: Progress in neural NLP: modeling, learning, and reasoning. Engineering 6(3), 275–290 (2020)
90.
Zhou, Y., Kang, J., Kwong, S., Wang, X., Zhang, Q.: An evolutionary multi-objective optimization framework of discretization-based feature selection for classification. Swarm Evol. Comput. 60, 100770 (2021)
91.
Zhu, W., Wang, J., Zhang, Y., Jia, L.: A discretization algorithm based on information distance criterion and ant colony optimization algorithm for knowledge extracting on industrial database. In: 2010 IEEE International Conference on Mechatronics and Automation, pp. 1477–1482. IEEE (2010)©  The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.  2024
W. Banzhaf et al.(eds.)Handbook of Evolutionary Machine LearningGenetic and Evolutionary Computationhttps://doi.org/10.1007/978-981-99-3814-8_4
4.  Evolutionary Computation and the Reinforcement Learning Problem
Stephen  Kelly1and Jory  Schossau2
(1)
Department of Computing and Software, McMaster University, Hamilton, Ontario, L8S 4L7, Canada
(2)
Department of Computer Science & Engineering, Michigan State University, 428 S. Shaw Ln., East Lansing, MI, 48824-1226, USA
Stephen  Kelly
Email: kellys32@mcmaster.ca
Abstract
Evolution by natural selection has built a vast array of highly efficient lifelong learningorganisms, as evidenced by the spectacular diversityof species that rapidly adapt to environmental change and acquire new problem-solving skills through experience. Reinforcement Learning (RL) is a machine learning problem in which an agent must learn how to map situations to actions in an unknown world in order to maximise the sum of future rewards. There are no labelled examples of situationaction mappingsto learn from and we assume that no model of environment dynamics is available. As such, learning requires active trial-and-error interaction with the world. Evolutionary Reinforcement Learning (EvoRL), the application of evolutionary computation in RL, models this search process at multiple time scales: individual learning during the lifetime of an agent (i.e., operant conditioning) and population-wide learning through natural selection. Both modes of adaptation are wildly creative and fundamental to natural systems. This chapter discusses how EvoRL addresses some critical challenges in RL including the computationalcostof extended interactions, the temporal credit assignmentproblem, partial-observability of state, nonstationary and multi-task environments, transfer learning, and hierarchical problem decomposition. In each case, the unique potential of EvoRL is highlighted in parallel with open challenges and research opportunities.
4.1 Introduction
Intelligent behaviour emerges in natural systems as a result of adaptation occurring over at least two vastly different timescales: evolution and lifetime learning. Both modes of adaptation are wildly creative and critical to the development of robust intelligence. The cause and effect relationship between evolution and learning is so nuanced and complex that it is a field of study in its own right [22, 66] and is colloquially referred to as the “Baldwin Effect” [11, 49, 119]: that phenotypic plasticity might have a direct effect on inherited genetics and its evolution, which of course can in turn affect phenotypic plasticity.
RL has a long research heritage in psychology and ethology as a model for how animals learn through trial-and-error interaction with the environment over their lifetime [149, 173]. In machine RL, computational agents learn to maximise the sum of future rewards through interactions with unknown environments. A variety of lifetime learning algorithms have been formalised and extensively researched to solve the RL problem [163]. Their integration with deep learning has achieved remarkable success solving complex sequential decision tasks [118]. The construction of behavioural agents has also been a fertile playground for evolutionary computation since its inception. Early work by Holland [75] and Koza [97] describe methods of evolving game-playing strategies and controllersfor complex dynamical systems. Since then, EvoRL, the application of evolutionary computation to RL, has emerged to represent a family of evolutionary search algorithms that consider both individual learning during the lifetime of an agent (i.e., operant conditioning) andpopulation-wide learning through natural selection. While the development of behavioural agents is typically the focus, EvoRL represents a form of meta-learning in which artificial agents (bodyand brain), their environment and the interaction among them might all be subject to adaptation, Fig. 4.1
Fig. 4.1
Adaptation at two timescales in EvoRL. In the inner loop, an individual agent learns to predict the effect of its actions in the environment. In the outer loop, an evolutionary cycle of selection and variation operates on a population of individuals. Any part of the system might evolve including decision policies, reward functions, environment transition functions, initial states, goal states, hyperparameters, and more
In addition to developing strong sequential prediction algorithms for real-world problem-solving, EvoRL has a rich heritage from the field of Artificial Life, seeking to better understand brains, behaviour, and their evolution in the natural world [2, 15, 109]. An underlying theme from this perspective is how knowledge from evolutionary ecology and cognitive science might guide evolution in building intelligent agents that learn and creatively discover solutions to problems [35, 115]. Creativityis characterised as a system’s capacity to generate new things of value. It is inherently a two-part process: (1) Explorethe environment to experience new structures and strategies for problem solving; and (2) Exploitvaluable discoveries, making sure to store that information within the agent. This creative process is abundant in natural evolution, as evidenced by the spectacular diversityof well-adapted species, and is fundamental to animal learning, perception, memory, and action [19]. AI researchers have long wondered how machines could be creative, and the digital evolution community is perpetually surprised by unplanned, emergent creativityin their systems [105]. In particular, creativity is the driving force behind active trial-and-error problem-solving. It is emphasised in parts of this review because it can be a general engineering principal to guide evolution towards building computer programs with the structure and efficiency of the biological brain [141].
This chapter is organised to highlight how the holistic EvoRL search paradigm can be applied to the critical challenges in RL. Section 4.2reviews RL as a machine learning problem and outlines a taxonomy of RL and EvoRL algorithms. The remaining sections discuss EvoRL in context with unique aspects of the RL problem including the computational costof evaluating policies through sequential interaction with large state spaces (Sect. 4.3), the temporal credit assignmentproblem (Sect. 4.4), partial-observability of state (Sect. 4.5), nonstationary and multi-task environments (Sect. 4.6), transformational learning and hierarchical problem decomposition (Sect. 4.7). While these challenges are all interrelated, we use this decomposition as a structure to explore how evolutionary computation has been studied and applied in RL, discussing select historical and state-of-the-art algorithms to illustrate major achievements and open research opportunities. We hope this perspective complements surveys in the field [9, 145, 185].
4.2 Machine Reinforcement Learning
Howcan we build artificial agents that learn from trial-and-error experience in an under-explored world?
Given an environment (typically a complex and stochastic dynamical system), RL is the problem of mappingsituations to actions in order to maximise the sum of future discounted rewards. The problem can be formally modelled as a Markov Decision Process (MDP)with in which represents the state space, the action space, is a transition function that computes a probability distribution over possible next states , a reward function computes , and is the initial state distribution. The agent’s decision-making policyis denoted by where is the policy’s algorithm and is a set of parameters.
Interactions in RL are typically episodic, beginning with the agent situated in a start state defined by the task environment. From there, time unfolds in discrete timesteps (t) in which the agent observes the environment via sensory inputs , takes an action based on the observation , and receives feedback in the form ofa reward signal 1The process repeats in a loop until a task end state is encountered or the episode ends for another reason, such as a time constraint. Rewards are often time delayed. The agent’s objective is therefore to select actions that maximise the long-term cumulative reward from its current state , or where is a discount factor on future rewards (See inner loop of Fig. 4.1). The reward signalis used to communicate what we want the policy toachieve without specifying howto achieve it. Unlike supervised learning, there are no prior labelled examples of associations to learn from. As such, the problem is considered semi-supervised. Given this constraint, the broad challenge of RL is how to discover sequential decision policies through direct interaction with the task environment.
4.2.1 Reinforcement Learning Algorithms
A common approach in RL is to attempt to discover the optimal policy , whichselects with probability 1 the action expected to yield the largest return when the agent continues to act optimally thereafter:
(4.1)
RL algorithms for attempting to discover optimal policies include three general phases: 
1.
Explore: Generate experience by interacting with the environment following policy (inner loop of Fig. 4.1).
2.
Evaluate: Assess the quality of ’s actions using the reward signal 
3.
Update: Generate a newpolicy based on evaluations.
In the context of EvoRL, it is informative to characterise RL algorithms by the timescale(s) over which these phases are computed.
Step-basedRL methods explore in action space at each timestep. In this case, evaluation estimates the quality of state-action pairs, and the gradient of the reward signalis used to guide learning updates, potentially online, i.e., within an episode. A representative example is Temporal Difference (TD) learning, which aims to minimise the error between predictedfuture rewards following a policyfrom a given state, and the actualrewards received post-execution. For example, some form of TD algorithm is used by methods that optimise an action-value function (Q-function) to predict the value of a state-action pair. The policyalgorithm can then query the value function at each timestep to select an action. Q-learning [163] is a well-known example, which has been combined with deep convolutional neural network architectures in Deep Q-Networks (DQN) and many of its variants [80, 118]. Step-based RL requires explorationsteps (eg. random actions) within the episode, which can result in jittery behaviour during learning and produce unreproducible trajectories for an exploration-free policy. Furthermore, while step-based RL with value functions can efficiently learn a high-quality policyunder tabular settings [81], it often has difficulty scaling to large state and action spaces and can even diverge with function approximation in continuous versions of those spaces [188]. It is possible to avoid value functions and directly adjust the policy, , in the direction of the performance gradient, for example, using the PolicyGradient Theorem [163]. This approach is assumed by policy-based algorithms such as REINFORCE [190] and, more recently, Proximal Policy Optimisation (PPO) [143]. Policy-based RL algorithms scale better to high-dimensional state and action spaces, which are common in real-world RL tasks such as robot control. However, Policy-based RL can suffer from high variance in the estimation of gradients and thus requires many samples for an accurate estimate, which can be inefficient. Actor-critic methods represent a hybrid approach that attempts to capture the strengths of value-based and policy-based RL by simultaneously optimising a policyfunction (actor) and value function (critic) [96, 117, 134]. Interestingly, actor-critic methods can achieve greater stability by operating at two timescales: the critic changes rapidly using TD learning, while the actor updates at a slower timescale in an approximate gradient direction based on information provided by the critic [96]. In short, step-based RL methods perform a learning update relative to each decision and the immediate reward within the temporal sequence.
Episode-basedRL methods explore in policyspace at each episode. Evaluation estimates the quality of the policy as a whole using the cumulative reward received after following the policyfor an entire episode. This is traditionally the approach taken in EvoRL, in which each learning update effectively creates a new policy(e.g., by selection and variation operators in the Evolutionary Algorithm (EA), see outer loop of Fig. 4.1), and thus the search process is performed over the space of possible decision policies within a particular representation. Episode-based RL can be characterised as a form of black-boxor derivative-freeoptimisation since policyupdates do not rely on reward signalgradients. Episode-based RL offers several benefits in comparison to step-based RL. It generates smooth control trajectories that are reproducible under test, it can handle non-Markovian reward definitions, and the resulting explorationin policyspace is well-suited for adaptation in environments with sparse reward. The relative challenges and merits of step-based and episode-based RL are discussed in more detail in the context of the temporal credit assignmentproblem in Sect. 4.4.2
In this chapter we focus on model-freeRL, meaning that no prior model of the environment’s transition or reward function is available. As such, trial-and-error interaction with the environment is required for the agent to gain experience (collect data) and learn. However, note that as soon as learning occurs, the agent itself becomes a world model of sorts (i.e., environment dynamics are encoded in learned behaviours). Furthermore, building internal world modelsis an explicit requirement for agents operating in dynamic and partially observable environments. For example, if complete information about the state of the environment is not available from each observation , it may be necessary for the agent to explicitly learn a world modelthat can be queried in order to predict intermediate variables that—when combined with the sensory input —form a complete representation of the state of the environment. Memory models are discussed in more detail in Sect. 4.5
Finally, note that RL algorithms are also categorised as on-policymethods that learn by executing their current policyin the environment, while off-policymethods learn from experience collected by a different policy. For example DQN is an off-policy method in which learning updates are performed based on samples from a replay memoryof many past experiences, i.e., experience = (, , , ).
4.2.2 Evolutionary RL Algorithms
There are two distinct approaches to evolvinga policy : searching in the parameterspace, and searching in the programspace. For example, those that search in the parameterspace might assume a fixed structure for the policy’salgorithm as a neural network often does, optimising only the weight parameters . Those that search in programspace simultaneously build the policyalgorithm andoptimise its parameters . The latter approach is more general and open-ended, since no prior constraints are placed on how the policy works or how complex it should be in order to solve a particular problem. Indeed, any behaviour algorithm could conceivably be evolved, given an expressive programming language and suitable memory [13].
All EvoRL methods for policysearch assume some form of EA in which a population of candidate behaviours are iteratively improved through selection, crossover, and mutation. Most approaches can be further characterised as variants of Evolutionary Strategies (ES), Genetic Programming (GP), or Neuroevolution (NE).
In the context of RL, ES are typically used as continuous black-box optimization algorithms suitable for search in large parameter spaces, for example optimising the weights of a neural network.2They have recently gained significant attention in RL for their ability to scale over multiple CPUs and rapidly train deep neural networks to a performance level comparable to that achieved by traditional gradient descent based optimisers [139].
GP is the special case of an EA in which the individuals being evolved are computer programs. In the most general EvoRL case, GP builds policyalgorithms from scratch starting with basic mathematical operations and memory structures as building blocks. As such, GP performs an evolutionary search in program space, simultaneously discovering a policyalgorithm and optimising its parameters. Early EvoRL experiments established GP’s power to discover general problem solvers in simulated control tasks [97, 98] and build adaptive controllersfor real robots [130]. GP has since been used toevolve graph-structured policies (tangled program graphs [87] and cartesian genetic programming[191]), which rival multiple deep RL algorithms in Atari visual RL tasks while being significantly more efficient to evolve anddeploy post-search/training. GP has also been used to evolve loss functionsfor step-based RL algorithms to optimise [33]. In this case, GP rediscovers the TD method from scratch, producing domain-agnostic RL algorithms capable of learning control tasks, gridworld-type tasks, and Atari games.
NE has a rich history in EvoRL, exploring how evolutionary computation can evolve the weights, topology, learning rules, and activation functionsof neural networks for behavioural agents [155, 156, 187]. In particular, pairing NE with deep RL has significant potential to leverage the best of evolution and stochastic gradient descent [145] learning.
In short, EvoRL represents a diverse universe of algorithms that use digital evolution to address RL problems.
4.3 Complex Evaluations
Machine RL potentially incurs significant computational cost. In complex tasks, the agent is likely to observe the environment through a high-dimensional sensory interface (e.g., a video camera). However, scaling to high-dimensional state observations presents a significant challenge for machine learning, and RL in particular. Each individual input from the sensory interface , or state variable, may have a large (potentially infinite) number of possible values. Thus, as the number of state variables increases, there is a significant increase in the number of environmental observations required for the agent to gain the breadth of experience required to build a strong policy. This is broadly referred to as the Curse of Dimensionality (Sect. 1.4 of [18]). The temporal nature of RL introduces additional challenges: complete information about the environment is not always available from a single observation (i.e., the environment is only partially observable) and delayed rewards are common, requiring the agent to make thousands of decisions before receiving enough feedback to assess the quality of its behaviour [83]. As a result, RL systems benefit from policyalgorithms with low computational complexity and learning methods that are sample-efficient. EvoRL has been shown to address the RL scaling issue through at least three mechanisms, namely, incremental growth, dynamic inference complexity, and sensory space selectivity.
4.3.1 Incremental Growth
Several EvoRL methods leverage evolution to build the policyalgorithm starting from the simplest working devices and incrementally complexifying through interaction with the task environment. This implies that the computational costof inference is low early in evolution, and only increases as more complex structures provide fitness and performance gains. In particular, NE and GP exhibit these properties in a range of RL scenarios.
Neuroevolutionof Augmenting Topologies (NEAT)[156] is an evolutionary algorithm for building neural network architectures and optimising their weight parameters. NEAT uses a variable-length linear representation consisting of node genes and connection genes. Node genes encode input, hidden, and output node types. Connection genes specify the possibly recurrent network links and their weights. Historical markingsannotate connection genes with unique innovation identifiers that are inherited by descendants. These markings facilitate crossover by allowing genes representing compatible network structure to be aligned. NEATuses Speciationto maintain diversityand prevent a single nicheof network architectures (and associated weights) from dominating the population, protecting novel architectures by allowing them to develop within a niche until they become competitive. Populations are initialised containing small neural networks and since increases in complexity must pass natural selection, the search is bound by incremental growthTheinitial version of NEAT wasanalysed on the classic double inverted pendulum balancing control task [156], where it is several times more efficient that previously existing neuroevolutionmethods. NEAThas since been improved and demonstrated in various tasks including building complex policyarchitectures in robotics [157], strategic decision-making in fracturedenvironments where the correct action varies discontinuously as the agent moves from state to state [95], and building minimal policyarchitectures in Atari video games from compact state representations [176]. From these origins, NEAThas been extended in several ways that benefit EvoRL [132], and its various innovations appear in nearly every section of this chapter.
Sample-Efficient Automated Deep Reinforcement Learning (SEARL) [55] simultaneously evolves neural architectures, trains them in an off-policy setting, and optimises their hyperparameters. SEARL uses a population of agents with varying structure and hyperparameters such as the learning rate. During evaluation, each structure-hyperparameter combination gets a fitness score and the environment interactions are stored as state-action-state transitions in a shared replay memory. Based on the fitness score, the best structure-hyperparameter combinations are selected, the hyperparameters are mutated and trained with samples from a shared replay memory and then evaluated again. Shared experience replay memory stores trajectories from allevaluations and thus provides a diverse set of samples for each agent during the training phase. This speeds up training and reduces the potential of over-fitting. Diverse training samples and incremental growthallow SEARL to build and train agents using up to ten times fewer environment interactions as random search or PBT [79] (a closely related, state-of-the-art neuroevolution method) while matching or improving on their final performance.
4.3.2 Dynamic Inference Complexity
Tangled Program Graphs (TPGs) are a relatively new GP framework developed specifically formulti-task RL in large (e.g., visual) state spaces with partial-observability[87, 90]. They have been extended to support large-scale parallelisation [44] and ultra-fast inference on embedded hardware through the generation of optimised standalone C code [43]. The critical contribution of TPGs is to enable the construction of hierarchical agents through compositional evolution: evolving policyalgorithms that combine multiple models (algorithms), which were previously adapted independently [184]. In addition to incremental growth, this allows TPGs to mirror how the neocortex is highly modular, containing many semi-independent predictive models with similar underlying structure. These models are organised hierarchically and compete to form a multi-faceted and efficient prediction machine [8, 68, 122]. The inference time complexity of a Tangled Program Graph (TPG) policy’smany-model architecture is dynamicbecause only a subset of models require execution at each timestep, improving efficiency. Conversely, most deep learning neural networks model the world in terms of a hierarchyof concepts, with each concept defined through its relation to simpler concepts (e.g., [118]). This implies that every level of the hierarchycontributes to every prediction, incurring an excessive computational costfor inference. Such networks do not account for the fact that neocortical regions are organised into columns and layers, and how this structure suggests a theory of intelligence in which thousands of semi-independent world modelsare switched in and out of the overall inference process over time [8, 68, 122]. Recently, sparse deep learning architectures have appeared which dynamically activate subcomponents instead of the full graph [53]. Interestingly, evolution is being used to automatically decompose the task into subcomponent models and combine them into multi-task graphs [57]. The application of these methods to RL problems is an emerging field [110]. Other systems also have this dynamic inference complexity property of much more minimal computation, such as Linear Genetic Programming[26], WireWorld [146], and Hierarchical Temporal Memory [68].
4.3.3 Sensor Space Selectivity
Sensor space selectivity implies that a policyis able to ignore inputs (variables in ) not relevant to the task at hand [7, 160], which potentially improves learning efficiency by simplifying or abstracting the input space [82]. In GP, an evolved policy’suse of sensor data in (eg. image pixels) is entirely an evolved property. In the case of TPGs, evolution builds sacade-like perception behaviour in which the multi-model agent jumps between subsets of sensor data in that are critical to prediction and avoids processing irrelevant inputs [87, 88]. This behaviour can take many evolutionary cycles to emerge, but the result is more efficient than sweeping a convolution filter over the entire input space as in deep RL [118]. The sensory abstraction provided by convolution filters undoubtedly improves learning but does not capitalise on the efficiencies associated with learning when to ignore particular state variables.
4.3.4 Parallel Algorithms and Hardware Acceleration
Scaling through parallelisation is an intrinsic benefit of population-based search and optimisation methods. RL naturally lends itself to parallelisation since, in the simplest cases, each episodic evaluation is independent and can thus be executed in parallel if appropriate hardware is available. Secondly, if there is diversityin the population, then the time for convergence to optima scales with population size, thus making parallel evolutionary algorithms much more effective than their linear counterparts [175]. Using a novel inter-process communication strategy, ES was recently shown to scale to over a thousand parallel workers, distributing parameters such that many unique deep RL models are evaluated in parallel [139]. This framework shows impressive parallelisation results for two benchmarks: MuJoCo 3D humanoid walking, and Atari games. The framework solves MuJoCo 3D humanoid walking in 10 minutes using 1,  440 CPU cores, which takes 18 CPU cores over 11 hours to solve. For Atari games, competitive results were often achieved after just one hour of training. Furthermore, ES is relatively invariant to action frequency and delayed rewards, and tolerant of long episode horizons. These benefits are largely due to adopting episode-based RL, which is not sensitive to temporal discounting as compared to RL techniques, such as Q-learning and PolicyGradients.
In asynchronous, multi-agent RL tasks such commercial Real-Time Strategy video games, the RL interaction loop typically runs asynchronously for multiple interconnected agents. Lamarckian [10] is an open-source, high-performance EvoRL platform that supports an asynchronous Markov Decision Process (MDP) and can scale up to thousands of CPU cores. An asynchronous tree-shaped data broadcasting method is proposed to reduce policy-lag, an inefficiency in distributed off-policy RL that occurs when an actor’s policyfalls several updates behind the learner’s policy. Multiple current EvoRL algorithms are benchmarked at scale such asNSGA-II [42] for Game Balancing in Real-time strategy (RTS) games, i.e., discovering diverse Non-Player-Characters (NPCs) that keep gameplay interesting for humans, and Population-Based-Training (PBT) [79] for general Atari Video game playing. Lamarckian significantly improves sampling efficiency and training speed as compared to the state-of-the art RLlib [107].
Multiple recent platforms build on APIs for GPUacceleration to support neuroevolution EvoRL. EvoJax [168] is a scalable neuroevolutiontoolkit to train neural networks running in parallel across multiple TPU/GPUs. EvoJAX implements the evolutionary algorithm, neural network, and tasks all in NumPy, which is compiled just-in-time to run on accelerators. EvoJax and its sister projects QDax [108] and evosax [103] (JAX-Based Evolution Strategies) include robot control tasks simulated in Brax [56] and natively support evolutionary strategy variants including CMA-ESand Map-Elites, while EvoX [78] supports the complete OpenAI Gym RL testbed and includes implementations of Differential Evolution (DE) and Particle Swarm Optimisation (PSO). The Parallel Evolutionary and Reinforcement Learning (PEARL) Library [169] proposes a novel combination of OpenAI’s Natural Evolutionary Strategy [139] with the Adam optimiser equation [92]. These GPU-accelerated neuroevolutionframeworks can optimise policies in Atari or MuJoCo robotics tasks with significantly less time than using parallel CPUs.
4.3.5 Discussion and Open Challenges
Incremental growthis an intrinsic property of natural and digital evolution. The ability to scale the complexity of problem solvers through interaction with the problem is a practical method of optimising both the model and sample complexity of RL algorithms. Adaptivecomplexificationalso reduces the burden of prior knowledge, which would otherwise be necessary to estimate the correct solution complexity.
Evolutionary computation also naturally lends itself to parallelisation, and most EvoRL implementations scale to many CPUs by performing independent computations in parallel, for example, each episodic policyevaluation can be computed simultaneously in single-agent settings, and asynchronously in multi-agent settings. In addition to scaling the speed of evolution, parallel and multi-population evolutionary models support diversityby allowing individuals to develop in isolated niches, with occasional migration as a means to distribute this diversitythroughout the experiment. There is significant opportunity to further improve efficiency and scaling potential of EvoRL by modelling properties of biological and societal niches. For example, [77] demonstrated how hierarchically organising populations according to fitness level can help scale evolutionary search to complex problems by nurturing a diverse set of intermediate “stepping stones” on the path to the most fit individuals. Biological nichesare also time-dependent. Natural environments change over time (e.g., weather becomes colder or hotter), and individuals that accumulate neutral variations under a static environment may have an advantage if these variations happen to become advantageous when the environment changes. Dynamic environments are ubiquitous in RL settings, where policies are exposed to unique sequences of tasks that change over abrupt or gradual intervals (discussed in more detail in Sect. 4.6). More generally, more research is needed to synergise the speed potential of parallel processing while also leveraging the diversitypotential of parallel evolution in nature.
Finally, artificial neural networks benefit greatly from hardware acceleration because some matrix manipulations required during learning and inference can be computed in parallel on GPUs. While neuroevolutioncan also leverage GPU acceleration, a similar breakthrough for non-connectionist program-search methods such as GP remains an open problem. Field-programmable gate arrays (FPGAs) are programmable computing platforms in which digital circuitry can be synthesised on-the-fly, without physical integrated circuit development. Their potential for accelerating GP is alluring and has a long history, with recent results showing that GP representations can be engineered such than an FPGA executes individual programs in a single clock cycle and transition between separate programs within a single cycle [37, 72]. For EvoRL, simulation of the environment is generally a more significant compute bottleneck than executing the agent policy. However, the target application for RL policies is often autonomous robotics or other low-power edge/embedded applications, where fast and efficient execution is essential. Hardware-accelerated program search, potentially entirely implemented on a single chip [177], is a promising direction for future study.
4.4 Exploration and Temporal Credit Assignment Ambiguity
Creditassignment is the mechanism used to modify policybehaviour relative to information obtained through the reward signalduring exploration. In sequential decision problems, the task environment typically provides the policywith a reward in response to each action taken (i.e., at every cycle in the inner loop of Fig. 4.1). However, it is often difficult to determine which specific decision(s) influence the reward gradient over time. For example, even actions with a neutral or negative stepwise reward may ultimately contribute to a successful outcome. On top of this, the reward signalfrom each interaction is often noisy. This means that many approaches to RL are unstable during learning and overly sensitive to initial conditions of the environment and/or hyperparameter settings. This is known as the temporal credit assignmentproblem [74, 163].
4.4.1 Exploration
Explorationat the evolutionary timescale in EvoRL is driven by variation operators (namely mutation and crossover) that introduce structural and behavioural changes to individuals within a population of policies. This variation causes agents to interact with their environment in new ways and gain new experiences. However, additional effort is often required in order to ensure that mutation (or other diversitygenerating operators) results in new organisms that are measurably different (in a phenotypic and/or genotypic sense) form what exists in the current population. In particular, explicit maintenance of a diverse policypopulation is important to ensure a thorough search of the behaviour space. Indeed, diversitymaintenance is a well-known factor in preventing evolving agent populations from getting stuck on local optima [125], even for behavioural tasks [39]. Furthermore, in multi-agent settings, or if a single agent will ultimately be represented by a group behaviour, then diversitymaintenance might be critical in developing individual group members that complement each other in a group setting that is, producing individuals that specialiseor succeed or fail in different ways. This section identifies two generic approaches to explorationthrough population diversitymaintenance that could potentially be used in combination: quality diversityalgorithmsand competitive coevolution
4.4.1.1 Quality Diversity
Quality Diversity (QD)evolutionary algorithms employ a fitness function that measures properties other than goal-directed problem-solving. At one extreme is novelty asthe objective, i.e., Novelty Search (NS). In an influential study, Lehman and Stanley [106] discover that abandoning the objective (i.e., ignoring the reward signal)and searching for behavioural novelty alone can help policies escape local optima in a deceptive maze navigationtask. NS is based on the observation that the path to a desirable or innovative behaviour often involves a series of divergent stepping stones that may not resemble the final behaviour and cannot be known ahead of time. Interestingly, novelty searchalso potentially leads to increasing complexity because the search will continually exhaust the behaviour space potential of simpler structures. Two important design issues now appear:
For a behaviour to be considered novel, it must be measurably different from other behaviours in the current population and all behaviours that have everbeen discovered during the search. If behavioural novelty is described as the observable characteristics of policies that are interacting with the environment, then how could behavioural novelty be characterised and compared in a meaningful way? Designing a Behaviour Characterization (BC) and similaritymetrics is a difficult task [123]. If multiple BCs are available, combining them in a single run has been shown to increase performance in robotics tasks, and sidesteps the need to choose the single most appropriate metric. Combining BCs by switching between them over time often leads to better results than taking the mean behavioural diversity, while requiring less computational power [24, 46, 86]. Similarly, switching goals during evolution potentially also promotes diversity, speeds up evolution, and leads to better generalisation [84, 133].
How can the EA and policy representation support the continual production of novelty and complexity over time? This is the research question explored in the field of open-ended evolution [12], often studied in EvoRL research (e.g., [87, 183]).
