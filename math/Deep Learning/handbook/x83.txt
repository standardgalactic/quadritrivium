Evans et al.  [12] propose a model extractionmethod using multi-objective genetic programming to construct decision trees that accurately represent any black-box classifier while being more interpretable. This method aims to simultaneously maximize the ability of the tree to reconstruct (replicate) the predictions of a black-box model and also maximize interpretabilityby minimizing the complexity of the decision tree. The reconstruction ability is measured by the weighted F1 score over cross-validation, and the complexity of the decision tree is measured by the number of splitting points in the tree. The overall evolutionary process uses a modified version of NSGA-II[10]. In their experiments on a range of classification problems, they found that the accuracy remained commensurate with other models extractionmethods (Bayesian rule lists, logistic regression, and two types of decision trees) while significantly reducing the complexity of the models produced.
16.3.5 Local Explanations
Insteadof creating an interpretable modelto approximatethe global performance of a black-box model, which may not be possible, these approaches only attempt to approximate the local behavior using an evolutionary algorithm. Much work in this area is inspired by LIME, but enhance the relatively simple sampling strategy and linear approximation with evolutionary components.
Ferreira et al.  [13] proposed Genetic Programming Explainer (GPX), a GP-based method which fits a local explanationmodel for a given input example. Similar to LIME, when given a sample input to be explained, the method samples a set of neighboring data points around the input and fits a local explanation. However, rather than a linear model, GPX uses a GP to evolve symbolic expression trees that best capture the behavior of the pre-trained black-box model over the neighboring data points. The authors tested this on both classification and regression datasets and reported that the GP captured the model’s behavior better than LIME, as the assumption of linear local behavior was not always valid, and also outperformed a decision tree used as an explainer for the same neighbor set.
On the other hand, Guidotti et al.  [14] proposed Local Rule-based Explanations (LORE), which applies an evolutionary algorithm to neighborhood generation rather than evolving the explanation itself. A genetic algorithm generates a set of points near the prediction to be explained, which are either classified the same as or differently from the original prediction while being nearby. A decision tree is then used to fit the local behavior of the black-box model. The use of a genetic algorithm here ensures a dense sampling of points in the local neighborhood which lie on both sides of the decision boundary.
16.3.6 Counterfactuals
Another line of work is in creating counterfactualsusing EC. An EA is well suited to this task as a black-box, possibly multi-objective optimizer, as it allows us to find counterfactuals without knowing the internal workings of the model while also optimizing for multiple desirable criteria in the counterfactuals
CERTIFAI  [50] generates a population of counterfactualexplanations using a model-agnostic genetic algorithm. The initial population is generated by sampling instances which lie on the other side of the decision boundary of the model (i.e., are classified differently from the instance to be explained). Then, the genetic algorithm optimizes the population to minimize the distance (for some notion of distance, depending on the type of data) from each counterfactualinstance to the input instance. The population is then analyzed for robustness, which increases if the best counterfactualexamples found are farther away from the input, and fairness, which is measured by comparing robustness across different values of a particular feature.
GeCo  [45] uses a genetic algorithm with feasibility and plausibilityconstraints on the features, specified using the constraint language PLAF. This allows certain counterfactualswhich would be useless to the user (e.g., counterfactuals where the user changes their gender or decreases their age) to be ruled out. Similar to CERTIFAI, the genetic algorithm minimizes the distance from the input instance to the counterfactualexamples, prioritizing examples on the other side of the decision boundary but also keeping examples which are close to the decision boundary if not enough counterfactualsare available. The fitness function does not consider how many features are changed relative to the input instance (with a smaller number being preferred for ease of understanding), but the algorithm is biased toward a smaller number of changes by initializing the population with only one feature changed.
Multi-objective counterfactuals(MOC)  [9] explicitly use multi-objective optimization to consider multiple desirable properties of the explanations. MOC uses a modified version of NSGA-IIto perform its search. Among the changes are the use of mixed integer evolution strategies (MIES)  [28] to search a mixed discrete and continuous space, and a different crowding distance sorting algorithm which prioritizes diversityin feature space. A total of four objectives are used, optimizing for these four desirable properties: the model output for the example should be close to the desired output; the example should lie close in feature space to the input to be explained; the example should not differ from the input in too many features; and the example is plausible (likely to be drawn from the same distribution as the real data), measured by its distance to the closest kdata points.
16.3.7 Explaining Deep Learning
Thus far, the methods we have covered are general and can be used with a variety of models. However, with the popularity of deep learning methods, we would be remiss not to discuss some specific methods tailored for deep learning models. While deep learning models are large, they are also differentiable allowing for hybrid methods combining gradient information with evolution.
For image classification, the large number of input features (pixels) presents a significant problem for many explanation methods. As such, it is necessary to reduce the dimension first, for example by clustering similar pixels into “superpixels”. Wang et al.  [57] propose using a multi-objective genetic algorithm to identify superpixels of importance for the final prediction, and using this set of superpixels as an explanation. The genetic algorithm uses NSGA-IIto optimize for the least number of superpixels used, while maximizing the model’s confidence in its prediction.
Adversarial examplesare closely related to counterfactuals. An adversarial example is a counterfactual example but with the intent of creating an incorrect prediction  [34]. This is done by applying a small perturbation to an example to change its classification. Most approaches search for examples which are as close to the original input as possible, and perceptually similar to the input. These examples are a method to highlight failure modesof the model as well as a potential attack vector on deep learning models.
Su et al.  [51] propose a method of finding adversarial exampleswhich modify only one pixel in an image. This is in contrast to previous methods which modify multiple pixels in the image and are more obvious to humans. Their method uses differential evolution, where each individual is encoded by the coordinate of the pixel to be modified and the perturbation in RGB space. They find that in many cases one pixel is sufficient to deceive the model.
Adversarial examplesare also present in models built for other domains, such as natural language processing. Alzantot et al.  [2] generate adversarial exampleson a sentiment analysis model and a textual entailment model. In addition, the examples they produce are designed to be semantically and syntactically similar to the original input, making the attack more difficult to spot. A genetic algorithm is used to optimize for a different target label than the original. Mutation is done by changing words in the input to similar words as measured by a word embedding model (GloVe) and filtering out words which do not fit the context.
16.3.8 Assessing Explanations
Finally, rather than using EC to generate the explanations themselves, we will discuss some ways that it can be used to assess or improve the quality of other explanation methods.
Huang et al.  [21] propose two metrics to assess the robustness of an explanation: worst-case misinterpretation discrepancy and probabilistic interpretation robustness. Interpretation discrepancy measures the difference between two interpretations, one before and one after perturbation of the input. It is desirable for this value to be low for a interpretation to be robust to perturbation. They then measure the discrepancy in two worst cases: the largest interpretation discrepancy possible while still being classified as the same class, and the smallest interpretation discrepancy possible while being classified differently (adversarial example). These values are optimized for using a GA. The other metric, probability of misinterpretation, calculates probabilistic versions of the above: the probability of an example having the same classification but significantly different interpretations, and the probability of an example having a different classification but similar interpretation. This is estimated using subset simulation.
It is also possible to perform an adversarial attack on the explanations themselves. Tamam et al.  [52] do this with AttaXAI, a black-box approach based on evolution. AttaXAI tries to evolve an image similar in appearance to the original input and produce the same prediction from the model but with an arbitrary explanation map. In their experiments, pairs of images were selected and they showed that they were able to generate a new image with the appearance and prediction of the first image, but with a similar explanation map to the second.
16.4 Research Outlook16.4.1 Challenges
One major challenge for evolutionary approaches to XAI is scalability. As data continues to grow and machine learning models become increasingly complex, the number of parameters and features to be optimized grows as well. As such, methods which work well on small models and datasets may become too expensive on larger ones. However, large models are the most opaque and most in need of explanation, so improving the scalability of XAI methods is necessary to ensure they can be applied to even the largest models. In particular, producing fully interpretable global explanationswhich accurately capture behavior yet are still simple enough to understand may become too challenging as models become larger—necessitating more local explanationsor a more focused approach concentrating on explaining particular properties or components of the model. We also see here the potential for more use of automated approaches to explainability—for example, by using evolutionary search to find local explanationsof interest and optimize for particular properties. This idea has been explored with counterfactualexamples, but it could be extended to other types of explanations.
Another challenge for explainabilityis the incorporation of domain knowledge. This can include knowledge from subject matter experts, as well as prior knowledge about the dataset or problem. Current approaches to XAI are broad and aim to provide explanations which are independent of the problem setting, or at most are model-specific rather than problem-specific. However, it can be useful to see how well a solution found by a machine learning model aligns with current knowledge in the field to evaluate the quality of the solution, or conversely, to identify areas where the model deviates from current understanding. For example, a practitioner may want to see how well the gene associations found by a genomics model aligns with the literature, as well as which associations are novel. This domain knowledge can be provided in the form of expert rules, constraints, or structured data such as a graph structure or tree. Domain knowledge can also be incorporated into the model-building process to improve interpretability, for instance by constraining the models to focus on associations known to be plausible (e.g., by incorporating causality) or excluding irrelevant features.
16.4.2 Opportunities
We see some additional opportunities for future work employing EC for XAI. One promising direction in current research is the use of multiple objectives to optimize explanations. Explainabilityis inherently a multi-objective problem, requiring the explanation to both be faithful to the model as well as being simple enough to be interpretable. EC is well-suited to explicitly optimizing for this, and we believe introducing these ideas into current and future explanation methods is a straightforward but effective way of improving the quality of explanations.
Along similar lines, the use of diversitymetrics and novelty searchis a unique strength available to evolutionary algorithms, which can help improve the explanations provided. The use of quality-diversity (illumination) algorithms can produce a range of explanations which are both accurate and present different perspectives on the behavior of the model. For example, a quality-diversity approach to counterfactual explanationscould ensure that a range of behaviors are showcased in the examples.
Another opportunity for EC is the incorporation of user feedback, considering the evolution of explanations as an open-ended evolution process. Explainabilityis intended for the human user, and as such explanation quality is ultimately subjective and can only be approximated by metrics. Users may also have their own unique preferences for what constitutes a useful explanation. Incorporating user feedback into the evolution process can allow better tailored explanations that continue to improve. At the same time, better metrics for measuring the quality of an explanation are also necessary in order to not overwhelm the user.
16.4.3 Real-World Impacts
As AI becomes increasingly integrated into real-world applications, developing better methods for providing explanations is essential for ensuring safety and trust across various domains. With this in mind, it is also crucial to consider the practical effects and benefits that XAI research can have. We would like to highlight here a few application areas where work on evolutionary approaches to XAI can have a substantial impact.
Healthcare is a domain where the consequences of errors can be especially high. Untrusted models may be ignored by clinicians, wasting resources and providing no benefit. Worse, seemingly trustworthy but flawed models may cause harm to patients. Even models with few errors may exhibit systematic biases, such as diagnostic models underdiagnosing certain patient groups while appearing accurate  [47]. Explainabilitycan help identify these systematic errors and biases  [3]. In the financial sector AI models are employed for fraud detection and riskassessment. Similar systematic biases in these models can also produce harm, for example, by disproportionately denying loansto certain groups. In addition, regulatory bodiesoften require explanations for these models to ensure compliance and maintain transparency
Explainabilityalso holds significant potential to advance engineering and scientific discovery. AI models are used in various engineering applications, for example, in AI-driven materials design and drug discovery, and to produce scientific insightsin fields such as genomics and astrophysics. Explanations can offer insightinto the underlying mechanisms and relationships, improving hypothesis generation, and validating domain knowledge.
Natural language processing has experienced many recent breakthroughs, with the development and deployment of models of unprecedented size. In particular, there is an emerging paradigm of building “foundation models”, generalist deep learning models which are trained on a wide range of data for general capabilities and which can be further fine-tuned for downstream tasks [5]. These models are capable of tasks which they are not specifically trained for, but it is still unclear how they make decisions or generate outputs. Any flaws in these foundation models may be carried over to application-specific models built on top of them. As these models become more pervasive and their applications expand, understanding them and identifying their failure modesbecomes increasingly important.
16.5 Conclusion
Explainable AI/ML is an emerging field with important implications for machine learning as a whole. With the increasing use of machine learning models in real-world applications, it’s more important than ever that we understand such models and what they learn. Evolutionary computing is well-poised to contribute to the field, bringing a rich toolbox of tools for performing black-box optimization. In this chapter, we introduced various paradigms for explaining a machine learning model and current methods of doing so. We then discussed how evolutionary computing can fit into these paradigms and the advantages of employing them. In particular, evolutionary computing as an optimizer is well-suited for tricky interpretabilitymetrics which are difficult to handle due to reasons such as non-differentiability, as well as for population-based metrics such as diversityand for optimizing multiple of these metrics at the same time. We highlighted a few methods in each category which leveraged some of these strengths, but there is still significant room for more explorationand more advanced evolutionary algorithms.
The field of explainable machine learning is still new, and much knowledge remains locked away within trained models that we still do not have the means to decipher. The use of evolutionary computing for XAI is still uncommon, but there are many opportunities ripe for the picking and we believe that it has the potential to play a key part in the future of XAI.
References
1.
Adel, T., Ghahramani, Z., Weller, A.: Discovering interpretable representations for both deep generative and discriminative models. In: Proceedings of the 35th International Conference on Machine Learning, pp. 50–59. PMLR (July 2018)
2.
Alzantot, M., Sharma, Y., Elgohary, A., Ho, B.-J., Srivastava, M.B., Chang, K.-W.: Generating natural language adversarial examples. In: Riloff, E., Chiang, D., Hockenmaier, J., Tsujii, J. (eds.) Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, 3 Oct–4 Nov 2018, pp. 2890–2896. Association for Computational Linguistics (2018)
3.
Arias-Duart, A., Parés, F., Garcia-Gasulla, D., Gimenez-Abalos, V.: Focus! rating XAI methods and finding biases. In: IEEE International Conference on Fuzzy Systems, FUZZ-IEEE 2022, Padua, Italy, 18–23 July 2022, pp. 1–8. IEEE (2022)
4.
Bacardit, J., Brownlee, A.E.I., Cagnoni, S., Iacca, G., McCall, J., Walker, D.: The intersection of evolutionary computation and explainable AI. In: Proceedings of the Genetic and Evolutionary Computation Conference Companion, GECCO’22, pp. 1757–1762, New York, NY, USA, July 2022. Association for Computing Machinery (2022)
5.
Bommasani, R., Hudson, D.A., Adeli, E., Altman, R.B., Arora, S., von Arx, S., Bernstein, M.S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N.S., Chen, A.S., Creel, K., Quincy Davis, J., Demszky, D., Donahue, C., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Goodman, N.D., Grossman, S., Guha, N., Hashimoto, T., Henderson, P., Hewitt, J., Ho, D.E., Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G., Khani, F., Khattab, O., Koh, P.W., Krass, M.S., Krishna, R., Kuditipudi, R., et  al.: On the opportunities and risks of foundation models (2021). arXiv:​2108.​07258
6.
Breiman, L.: Random forest. Mach. Learn. 45, 5–32 (2001)CrossrefzbMATH
7.
Buciluǎ, C., Caruana, R., Niculescu-Mizil, A.: Model compression. In: Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD’06, pp. 535–541, New York, NY, USA, Aug 2006. Association for Computing Machinery (2006)
8.
Cano, A., Ventura, S., Cios, K.J.: Multi-objective genetic programming for feature extraction and data visualization. Soft. Comput. 21(8), 2069–2089 (2017). AprilCrossref
9.
Dandl, S., Molnar, C., Binder, M., Bischl, B.: Multi-objective counterfactual explanations. In: Bäck, T., Preuss, M., Deutz, A., Wang, H., Doerr, C., Emmerich, M., Trautmann, H. (eds.) Parallel Problem Solving from Nature–PPSN XVI. Lecture Notes in Computer Science, pp. 448–469, Cham, 2020. Springer International Publishing (2020)
10.
Deb, K., Pratap, A., Agarwal, S., Meyarivan, T.: A fast and elitist multiobjective genetic algorithm: NSGA-II. IEEE Trans. Evol. Comput. 6(2), 182–197 (2002)
11.
Dwivedi, R., Dave, D., Naik, H., Singhal, S., Omer, R., Patel, P., Qian, B., Wen, Z., Shah, T., Morgan, G., Ranjan, R.: Explainable AI (XAI): core ideas, techniques, and solutions. ACM Comput. Surv 55(9), 194:1–194:33 (2023)
12.
Evans, B.P., Xue, B., Zhang, M.: What’s inside the black box? A genetic programming method for interpreting complex machine learning models. In: Proceedings of the Genetic and Evolutionary Computation Conference (GECCO), pp. 1012–1020 (2019)
13.
Ferreira, L.A., Guimarães, F.G., Silva., R.: Applying genetic programming to improve interpretability in machine learning models. In: Proceedings of the IEEE Congress on Evolutionary Computation (CEC), pp. 1–8 (2020)
14.
Guidotti, R., Monreale, A., Ruggieri, S., Pedreschi, D., Turini, F., Giannotti, F.: Local Rule-Based Explanations of Black Box Decision Systems (2018). CoRR, arXiv:​1805.​10820
15.
Hancer, E., Xue, B., Zhang, M.: A survey on feature selection approaches for clustering. Artif. Intell. Rev. 53(6), 4519–4545 (2020)Crossref
16.
Hastie, T., Tibshirani, R., Friedman, J.: The Elements of Statistical Learning. Springer (2001)
17.
Hinton, G.E., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network (2015). CoRR, arXiv:​1503.​02531
18.
Hotelling, H.: Analysis of a complex of statistical variables into principal components. J. Educ. Psychol. 24, 417–441 (1933)CrossrefzbMATH
19.
Ting, H.: Can genetic programming perform explainable machine learning for bioinformatics? In: Genetic Programming Theory and Practice XVII. Springer (2020)
20.
Hu, T., Oksanen, K., Zhang, W., Randell, E., Furey, A., Sun, G., Zhai, G.: An evolutionary learning and network approach to identifying key metabolites for osteoarthritis. PLoS Comput. Biol. 14(3), e1005986 (2018)
21.
Huang, W., Zhao, X., Jin, G., Huang, X.: SAFARI: versatile and efficient evaluations for robustness of interpretability (2022). CoRR, arXiv:​2208.​09418
22.
Icke, I., Rosenberg, A.: Multi-objective genetic programming for visual analytics. In: Silva, S., Foster, J.A., Nicolau, M., Machado, P., Giacobini, M. (eds.) Genetic Programming. Lecture Notes in Computer Science, pp. 322–334. Springer, Berlin, Heidelberg (2011)
23.
Karimi, A.-H., Barthe, G., Schölkopf, B., Valera, I.: A survey of algorithmic recourse: Definitions, formulations, solutions, and prospects (2020). CoRR, arXiv:​2010.​04050
24.
Kulesza, A., Taskar, B.: Determinantal point processes for machine learning. Found. Trends Mach. Learn. 5(2–3), 123–286 (2012)CrossrefzbMATH
25.
La Cava, W., Moore, J.H.: Learning feature spaces for regression with genetic programming. Genet. Program Evolvable Mach. 21(3), 433–467 (2020). SeptemberCrossref
26.
Lakkaraju, H., Kamar, E., Caruana, R., Leskovec, J.: Interpretable & explorable approximations of black box models (2017). CoRR, arXiv:​1707.​01154
27.
Lensen, A., Xue, B., Zhang, M.: Genetic programming for evolving a front of interpretable models for data visualization. IEEE Trans. Cybern. 51(11), 5468–5482 (2021)Crossref
28.
Li, R., Emmerich, M.T.M., Eggermont, J., Bäck, T., Schütz, M., Dijkstra, J., Reiber, J.H.C.: Mixed integer evolution strategies for parameter optimization. Evol. Comput. 21(1), 29–64 (2013)
29.
Li, Z., He, J., Zhang, X., Fu, H., Qin, J.: Toward high accuracy and visualization: an interpretable feature extraction method based on genetic programming and non-overlap degree. In: Proceedings of the IEEE International Conference on Bioinformatics and Biomedicine (BIBM), pp. 299–304 (2020)
30.
Lipton, Z.C.: The mythos of model interpretability. Commun. ACM 61(10), 36–43 (2018)Crossref
31.
Lundberg, S.M., Lee, S.-I.: A unified approach to interpreting model predictions. In: Proceedings of the 31st International Conference on Neural Information Processing Systems (NeurIPS), pp. 4768–4777 (2017)
32.
Mahendran, A., Vedaldi, A.: Visualizing deep convolutional neural networks using natural pre-images. Int. J. Comput. Vision120(3), 233–255 (2016). DecemberMathSciNetCrossref
