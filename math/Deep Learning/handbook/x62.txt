Standish, R.K.: Open-ended artificial evolution. Int. J. Comput. Intell. Appl. 3(02), 167–175 (2003)Crossref
60.
Stanley, K.O.: Compositional pattern producing networks: A novel abstraction of development. Gen. Progr. Evolvable Mach. Spec. Issue Devel. Syst. 8(2), 131–162 (2007)
61.
Stanley, K.O., Lehman, J., Soros, L.: Open-endedness: the last grand challenge you’ve never heard of. O’Reilly Radar Online Article (2017)
62.
Stanley, K.O., Miikkulainen, R.: A taxonomy for artificial embryogeny. Artif. Life 9(2), 93–130 (2003)Crossref
63.
Stanley, K.O., Lehman, J., Soros, L.: Open-endedness: the last grand challenge you’ve never heard of. O’Reilly Online. Accessed 19 Dec 2017
64.
Stanton, C., Clune, J.: Curiosity search: producing generalists by encouraging individuals to continually explore and acquire skills throughout their lifetime. PloS ONE 11(9), e0162235 (2016)Crossref
65.
Stiennon, N., Ouyang, L., Wu, J., Ziegler, Lowe, D.M.R., Voss, C., Radford, A., Amodei, D., Christiano, P.: Learning to summarize from human feedback (2020). arXiv:​2009.​01325
66.
Sutton, R.S., Barto, A.G.: Reinforcement Learning: An Introduction (1998)
67.
Szerlip, P., Stanley, K.O.: Indirectly encoding running and jumping sodarace creatures for artificial life. Artif. Life 21(4), 432–444 (2015)Crossref
68.
Taylor, T.: Exploring the concept of open-ended evolution. In: Artificial Life 13 (Proceedings of the Thirteenth International Conference on the Simulation and Synthesis of Living Systems), Cambridge, MA, pp. 540–541. MIT Press (2012)
69.
Taylor, T., Bedau, M., Channon, A., Ackley, D., Banzhaf, W., Beslon, G., Dolson, E., Froese, T., Hickinbotham, S., Ikegami, T., et al.: Open-ended evolution: Perspectives from the OEE workshop in York. Artif. Life 22(3), 408–423 (2016)Crossref
70.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I.: Attention is all you need. Adv. Neural Inf. Proc. Syst. 30(2017)
71.
Video. Available at: https://​y2u.​be/​jeP8Nsulu48
72.
Video. Available at: https://​y2u.​be/​QNyNtvwA9FI
73.
Video. Available at: https://​y2u.​be/​M9pAJuX6dyM
74.
Video. Available at: https://​y2u.​be/​8C2K5fk28HI
75.
Video. Available at: https://​y2u.​be/​-LW2cCwSdRU
76.
Video. Available at: https://​y2u.​be/​XR3L4cZ83xU
77.
Video. Available at: https://​y2u.​be/​e53NwdT4RdM
78.
Video. Available at: https://​y2u.​be/​WEM1dBtLLTw
79.
Video. Available at: https://​y2u.​be/​P9A1ruI3_​tU
80.
Video. Available at: https://​y2u.​be/​l5PVSLDknWM
81.
Video. Available at: https://​y2u.​be/​Mo-rXnFq6vQ
82.
Wang, B., Komatsuzaki, A.: GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model (2021). https://​github.​com/​kingoflolz/​mesh-transformer-jax
83.
Wang, R., Lehman, J., Clune, J., Stanley, K.O.: Poet: open-ended coevolution of environments and their optimized solutions. In: Proceedings of the Genetic and Evolutionary Computation Conference, GECCO ’19, New York, NY, USA, pp. 142–151. Association for Computing Machinery (2019)
84.
Wang, R., Lehman, J., Rawal, A., Zhi, J., Li, Y., Clune, J., Stanley, K.O.: Enhanced poet: open-ended reinforcement learning through unbounded invention of learning challenges and their solutions. In: International Conference on Machine Learning, pp. 9940–9951. PMLR (2020)
85.
Wierstra, D., Schaul, T., Peters, J., Schmidhuber, J.: Natural evolution strategies. In: IEEE Congress on Evolutionary Computation. CEC 2008. (IEEE World Congress on Computational Intelligence), pp. 3381–3387. IEEE (2008)©  The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.  2024
W. Banzhaf et al.(eds.)Handbook of Evolutionary Machine LearningGenetic and Evolutionary Computationhttps://doi.org/10.1007/978-981-99-3814-8_12
12.  Hardware-Aware Evolutionary Approaches to Deep Neural Networks
Lukas  Sekanina1, Vojtech  Mrazek1and Michal  Pinos1
(1)
Faculty of Information Technology, Brno University of Technology, Brno, Czech Republic
Lukas  Sekanina(Corresponding author)
Email: sekanina@fit.vutbr.cz
Vojtech  Mrazek
Email: mrazek@fit.vutbr.cz
Michal  Pinos
Email: ipinos@fit.vutbr.cz
Abstract
This chapter gives an overview of evolutionary algorithm (EA) based methods applied to the design of efficient implementations of deep neural networks (DNN). We introduce various acceleration hardware platforms for DNNs developed especially for energy-efficient computing in edge devices. In addition to evolutionary optimization of their particular components or settings, we will describe neural architecture search (NAS)methods adopted to directly design highly optimized DNN architectures for a given hardware platform. Techniques that co-optimize hardware platforms and neural network architecture to maximize the accuracy-energy trade-offs will be emphasized. Case studies will primarily be devoted to NASfor image classification. Finally, the open challenges of this popular research area will be discussed.
12.1 Introduction
Previous chapters have shown that Evolutionary Algorithms (EA) can be utilized for neural architectures search (NAS)and for solving various hard optimization problems in the scope of machine learning applications. This chapter is devoted to the use of evolutionary algorithms in the task of discovering high-quality implementationsof deep learning algorithms. We will focus on deep neural networks (DNN) and their distinct subclass—convolutional neural networks(CNN)—that are currently employed as machine learning engines in many challenging applications operated on different types of platforms, ranging from ultra-low-power edge devices via mobile phones to high-performance accelerators in data centers  [8, 28, 68]. Hence, in addition to producing high-quality outputs, many of these implementations have to be energy efficient. This is achieved by developing specialized neural architectures and hardware inference accelerators for CNNs(and DNNs in general).
Section  12.2introduces the principles of efficient processing of CNNsin specialized hardware. It describes two fundamental architectures of CNN accelerators that are currently used—temporal architecturewhichis typical for common processors and Graphic Processing Units (GPUs), and spatial architecture, often adopted in application-specific integrated circuits (ASICs) or field programmable gate arrays (FPGAs). Particularly we emphasize the role of mapping, i.e., the strategy determining how a computational graph of (a potentially very complex) CNNis executed on limited hardware resources available on a chip. This chapter also deals withhardware simulatorsand fast predictors of hardware parametersof CNNs
In Sect.  12.3, we start with a fully trained CNNmodel and discuss various optimizations that can be conducted by EAs to obtain its hardware implementation with desired properties. We focus on the evolutionary designof components (such as approximate multipliers and activation functions)of hardware accelerators, optimized precision scaling, evolutionary optimization of the CNN-to-accelerator mapping, and weight compression
Section  12.4is devoted to hardware-aware NAS methodsand NAS methods with hardware co-design. Considering a given task (e.g., image classification)and target hardware, the hardware-aware NASalgorithms try to deliver the most suitable CNN architecturewhose hardware implementation satisfies given constraints, e.g., on maximum latency. Note that the hardware platform is not directly optimized; it can be seen as a series of constraints for the NAS method. NAS with hardware co-optimizationevolves CNNarchitecture and configuration of a configurable hardware acceleratorin parallel, i.e., in addition to the space of CNNarchitectures, it optimizes hardware configuration (e.g., type of used resources, mappingstrategies, buffer sizes, and compiler options). We survey the key evolutionary NASmethods addressing the above-mentioned approaches.
Finally, conclusions and open research challenges are presented in Sect.  12.5. Specifically, we address the problem of benchmarking of hardware-aware NASmethods, security & reliabilityissues, novel unconventional hardware platformsfor DNNs, and design time reduction.
12.2 Hardware Platforms for Efficient Processing of DNNs
The efficient processing of deep neural networks has been addressed in the literature in great detail, including a prominent book  [69] and comprehensive surveys  [8, 46, 51, 61]. This section aims to explain the basic concepts of efficient processing of CNNsand briefly survey hardware acceleratorsintroduced for inference. First, we recallthe principles and terminology of CNNs in Sect.  12.2.1. We distinguish two main architectures—temporal architecture(Sect.  12.2.2) and spatial architecture(Sect.  12.2.3). In Sect.  12.2.4, we will also deal with simulatorsand performance predictors developed to simplify the hardware acceleratordesign process.
12.2.1 Convolutional Layers
Atypical CNNconsists of convolutional layers, pooling layers, fully connected layers, and some other less computationally intensive units. Convolutional layersare responsible for more than 90% of overall computation, dominating runtime and energy consumption of inference  [69]. Figure  12.1illustrates how the output, the so-called output feature maps(O), of a convolutional layer, are obtained. The input feature maps(I), holding either the input image or an intermediate result of a previous layer, are processed by applying a set of filter weights(Weights). As multiple input feature mapscan be processed in parallel, multiple output feature mapsare obtained, where Nis their number; Nalso denotes the batch size. Table  12.1summarizes all symbols used.
Fig. 12.1
Applying a set of filters on the input feature maps to calculate the output feature maps in convolutional layers. Symbols are defined in Table  12.1
Table 12.1
Symbols used to describe convolution layers
Symbol
Description
H
Input feature map height
W
Input feature map width
C
Number of input channels
R
Filter height
S
Filter width
M
Number of output channels
E
Output feature map height
F
Output feature map width
N
Number of input/output feature maps
