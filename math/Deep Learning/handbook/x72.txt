In Lipizzaner, GAN training is a black-box component which applies SGD to update the parameters of the models, Algorithm4(Fig.  13.9)::CoevolveAndTrainModelslines  9, 10, 14, and 15. We can set up loss functions  (operators) with correspondingoptimization objectives and randomly assign one to a model after selection.
13.3.2.1 Dataset Sampling
Instead of using the whole training dataset, each cell of the topology uses a subset of the training dataset which is sampled from the whole training dataset. Figure  13.11illustrates how subsets of data are defined for each cell by sampling the entire dataset (for the cells 0, 0and 1, 0). For each cell of the topology, Lipizzaneralgorithm searches for and returns a mixture of models composed from the models collected from the cell neighborhood. Thus, the mixture is the fusion set of diverse models in the neighborhood trained by different subsets of the training dataset. Next, we describe an implementation of the Lipizzanersystem for AEL.
Fig. 13.11
Example of how the whole training dataset is sampled to generate training data subsets to train the different cells of the grid. In this example, a 33 grid defines the topology, and it is shown how the subsets for the cells 0, 0and 1, 0are defined
13.3.3 Implementation of Distributed Computing of Lipizzaner
Figure  13.12outlines Lipizzanerexecution for a given cell. Along the evolutionary process, communication between the processes is performed to exchange relevant information (such as ANNs parameters that define the individuals).
Fig. 13.12
Scheme of how Lipizzanerruns in parallel
With large grid sizes, training iterations could become out of sync, leaving some clients behind the rest. This can lead to the master process waiting too long to return the results. Thus, the Lipizzaner master process is allowed to end the run as soon as a proportion of the client nodes had successfully finished and a specific waiting time had expired.
Handling errors is critical when running Lipizzaneron large grids because the probability of any client to fail. As most of these errors depend on the HPC system, we make Lipizzaner resilient by using a checkpointing procedure. The clients save their running status (i.e., a checkpoint) and send it to the master process to store or update it. A checkpoint consists of the parameters of the ANNs (generatorand discriminator)in the center of the cell and the values of the hyper-parameters that drive Lipizzaner. These checkpoints allow a master process to restart any GAN training that halted. We store only information about the central models in the cell and when restarting training, each cell contacts its neighborhood and pulls information from them.
Another requirement for resiliency was to allow the client nodes to continue the training if there is a failure. Thus, the cells in the dead client’s neighborhood simply do not try to communicate with it or update their sub-populations up to a maximum number of nodes that can fail during a run. Further, it is difficult to guarantee the exact number of clients that will successfully connect on some HPC so a maximum and minimum number of clients are specified.
13.4 Experiments and Discussion
In this section, we summarize and discuss relevant experimental studies of Lipizzanerfor adversarial evolutionary learning in the form of GAN training. In particular, we present investigations regarding:
Spatial population: The use of spatial coevolutionary algorithms and diverse parameters to overcome pathologiesto improve GAN training is shown in Sect.  13.4.1
Topology: The effect on the population diversity for GANs trained with different topologies and communication is shown in Sect.  13.4.1.1
Scale: The effect of large spatial grids’ impact on the accuracy of the generative model trained is shown in Sect.  13.4.1.2
Operator: The diversity of loss functioncombinations at the cells in the spatial topology is shown in Sect.  13.4.2
Data: The impact of diverse data for spatial coevolutionary GAN training is shown in Sect.  13.4.3
Task objective transfer: The ability to use the population diversity to create high-quality and diverse ensembles of generative models is shown in Sect.  13.4.4
13.4.1 Spatial Coevolution for Generative Adversarial Network Training
An initial study showed that AEL with CCA can enhance the performance of gradient-based GAN training methods  [2]. Two empirical experiments were considered: (a) a theoretical model was used to analyze the capability of coevolutionary algorithms to solve theoretical GAN problems and (b) empirical experimentation was carried out to evaluate it on commonly used image-generation datasets. The results showed that the use a competitive coevolutionary GAN training could avoid more frequent degenerate training behaviors with the diversitythat comes from a population (phenotypes). For example, Fig.  13.13illustrates how the competitive coevolutionary approach escaped the mode collapse in the next generation. These results motivated the design and further research on spatially distributed coevolutionary GAN training proposed by Lipizzaner
Fig. 13.13
Sequence of CelebAimages generated during the competitive coevolutionary GAN training, when a mode collapse  (MC) occurs
As presented in Sect.  13.3.2, the Lipizzanersystem combines spatial coevolution with gradient-based learning to improve the robustness and scalability of GAN training. One of the main features of Lipizzaneris that using the cellular (topology) approach based on a grid topology promotes population diversity, i.e., it provides spatial population diversity. The improvement of the GAN training with the population diversitywas confirmed with an experimental analysis of Lipizzanerby evaluating the quality and diversityof the samples generated by the computed generative models  [54]. The experiments considered Lipizzanerusing different grid sizes (from 22 to 55) and two image datasets: MNIST[62] and CelebA[69].
The empirical results demonstrated that the samples’ quality and diversityimprove as the grid size grows. For example, in MNISTexperiments, the average FIDscore decreased (improved) from 40.78 on the 33 to 26.78 on the 55 (see Fig.  13.14), and the average TVD score decreased (improved) from 0.14 on the 33 to 0.12 on the 55. In addition, Lipizzanerprovided the most competitive results when comparing it against other GAN training approaches (i.e., vanilla GAN, WGAN, and EGAN) with the same computational budget.
Fig. 13.14
Distribution of FID results on MNIST dataset
Moreover, the diversityin the genome space (i.e., the parameters of the ANNs) increased with the grid size. Thus, the spatial separation of the sub-populations fosters diversity in the whole population. The gain of population diversity leadsthe generative models to produce better and more diverse samples because the ensembles of generatorsconsist of more diverse generators[54].
Lipizzaneris also scalable, experiments showed that the spatial grid distribution architecture allowed the computational effort to increase linearly instead of quadratically  [95]. This claim was supported by the observation that the wall clock time per training epoch grew nearly linear with the grid size. The communication took 0.5 seconds on average. Finally, the asynchronous communication pattern led to the usage of different time slots and therefore reduced high network peak loads.
Previous studies also showed with an ablation study that other evolutionary learning features of Lipizzanerhad a significant impact, such as learning rate hyper-parameter evolution and ensemble weight optimization  [54, 105]. These studies investigated different Lipizzanervariations: without learning rate evolution, without local selection pressure, without ensemble weight optimization, without selection and communication, and only keeping ensemble weight optimization. Moreover, the empirical results showed that the impact of ablating evolution of the initial value of the Adam learning rate is the most severe. This indicated that the performance and convergence of both GAN and network model training are susceptible to initial conditions. However, when selection and communication were eliminated while maintaining learning rate mutation and ensemble weight optimization, Lipizzanerachieved an average mean FIDscore closer to the baseline than any single ablation. Furthermore, retaining only ensemble weight optimization showed the positive impact of learning rate mutation on the results. So, these findings confirmed that communication, selection, learning rate evolution optimization, and ensemble optimization each, as well as in combination, provide a significant contribution.
13.4.1.1 Signal Propagation Through Spatially Distributed Population
Different topologies canbe defined on the spatially distributed GAN training when using Lipizzanerfor AEL. The spatial topology plus the cells neighborhood impacts on the signal propagationthrough the cells. As presented in Sect.  13.4.1, Lipizzanerwas designed to consider different topologies, e.g., the toroidal two-dimensional grid and the one-dimensional ring. In Lipizzaner, the signal propagationtakes the form of copying the best individuals (i.e., ANNs) to neighborhood cells to update the sub-populations, after each training epoch. Lipizzaneruses a two-dimensional grid by default (i.e., all the experimental analyses performed used this type of topology).
An empirical investigation of the ring topology in Lipizzanerwas performed  [108]. The primary goal was to investigate the performance quality and efficiency of different directionality and reach of the signal propagation(effectively migration to one or more neighbors on each side of a cell) compared to Lipizzanerwith grid topology. In addition, the diversityin the populations was evaluated. The analysis considered two different ringtopology approaches according to the communication radius (i.e., the number of cells that are affected by the signal propagationin each direction): the ringof radius , named Ring(2), which provides a signal propagationsimilar to the Lipizzanertoroidal grid; and the ring of radius , named Ring(1), which reduces the propagation speed, while accelerating the population’s convergence.
The use of Ring(1) hasthe following main benefits: (a) it mitigates the communication overhead because each cell exchanges information with only two cells (instead of four); (b) the reduction of sub-population size leads to the reduction of the number of operations required for fitness evaluation, selection, and replacement; and (c) it does not require to have a rectangular grid of cells, but the population size (number of cells in the grid) may be any natural number. The experimental analysis considered three different datasets: MNIST, COVID-19-positive 190 chest X-Rayimages provided by Cohen et al.  [29], and CelebA. Four methods were evaluated: Grid, Ring(2), and Ring(1), witha fixed computational budget, for three different population sizes: 9, 16, and 25.
The empirical analysis of the evaluated spatially distributed coevolutionary GAN training methods showed that using a ring topology instead of a two-dimensional grid maintained the performance of the trained generative models, and it could even improve them, depending on the setup. In general, Ring(1) trainedthe most competitive generative models. Ring(2) and Grid topology provided comparable generative models. Figure  13.15shows images synthesized by the generative models trained by using Ring(1) andGrid topologies.
Fig. 13.15
COVID-19and CelebAimages generated by Ring(1) and Grid
Figure  13.16summarizes the FIDresults for the MNISTdataset experiments. Ring(1) exhibited the most diverse populations, which diminishes with longer training. When comparing different ring topologies, increasing the migration radius, i.e., Ring(1) versus Ring(2), led to a reduction in diversity. In addition, there was no marked difference in the diversity of populations when comparing migration directionality betweenRing(2) and Lipizzaner. Finally, by altering the signal propagationfrom four to two directions and using a migration radius of one, Ring(1) reduced Lipizzaner’s computational time cost by 14.2–41.2% while maintaining comparable quality results.
Fig. 13.16
FID results on MNIST dataset for Lipizzaner, Ring(2), and Ring(1)
13.4.1.2 Large-Scale Spatially Distributed Adversarial Learning
Large parallel computation infrastructures enable new distributed methods that take advantage of the computational scale and parallelism. There are studies of the combination of large-scale parallel computation infrastructure, such as high-performance computing (HPC) platforms, with Lipizzanerand the ability to scale GAN training to large spatial grids and with datasets with large images  [43, 86, 103]. It has been shown that Lipizzanerimproves the quality of the trained generative models as the grid size increases. However, the results were limited due to the availability of the hardware platforms used for the experiments. Thus, further investigations were performed to evaluate the scalability of Lipizzanerby running it on the Summit Supercomputer housed at Oak Ridge National Labs (ORNL)  [61]. This HPC environment provided nodes with two IBM POWER9processors with 1600GB of non-volatile memory and six GPUsNVIDIA Volta V100s with 32GB memory.
The empirical analysis was reported by Flores et al.  [45]. The experiments were performed on two datasets: MNISTand COVID-19positive 190 chest X-Rayimages provided by Cohen et al.  [29], considering two image resolutions: 2828 and 6464 pixels. Figure  13.18a illustrates some X-Ray images from the COVID-19dataset. Different ANN architectures were considered: a basic four-layer perceptron (4LP) and two CNNsnamed CNN28 and CNN64. CNN28 had fewer trainable parameters. Thus, less competitive results were expected for CNN28. The goal of comparing these architectures was to show that scaling Lipizzanermay overcome the limitation of using non-optimal ANN architectures, i.e., with fewer parameters, when the nodes of the HPC platform have hardware  (memory) limitations.
The MNISTexperiments were performed on grid sizes up to 1818. The results showed significant improvements in the quality of the generated images when grid sizes increase (see Fig.  13.17a). Thus, the averageFID  (lower is better) score ranged from 41.23 (when Lipizzanerwas run on a 33 grid) to 17.20 (when a 1818 grid was used). In the COVID-19positive chest X-Rayimages generation, the results confirmed that increasing the grid size up to 1212 led to better results, with the same ANN architecture (see Fig.  13.17b). Thus, when increasing the grid size from 33 to 1212, the CNN28 IS  (higher is better) results increased (improved) from 1.31 to 1.49, and the CNN64 results improved from 1.37 to 161. When comparing the results achieved by both CNNs, it was observed that scale may make up for lower ANN complexity. Thus, CNN28 was able to get an IS of 1.49 (when using a 1212), which is higher than the IS of 1.37 got by CNN64 when using a 33. The less complex CNNprovided more accurate generative models than the more complex CNN when increasing the scale. Figure  13.18b and c illustrates samples produced by the generative models trained by using the 1212 grid to train CNN28 and CNN64.
Fig. 13.17
Mean results on MNIST and COVID-19experiments
Fig. 13.18
Real and generated COVID-19chest x-Ray images
The improvements in the results, when Lipizzanertraining was performed on a large-scale HPC platform, had similar run time. This indicates that it is possible to scale the distributed GAN training without significant additional computational time increase. In summary, it was shown that spatially distributed AEL improved performance even when using sub-optimal ANN architectures due to hardware constraints. Next, we study how operator diversitycan impact AEL with Lipizzaner
13.4.2 Spatial Adversarial Evolutionary Learning with Operator Diversity
GAN training pathologies, such as instability and mode collapse, generally arise from lack of diversityin the generatorand discriminatorinteractions. AEL with Lipizzanerinjects diversityin the network parameters of the ANNs in the population  (genome space) by training two populations whose ANNs  (individuals) are separated in cells of a toroidal grid. Furthermore, as presented in Sect.  13.3.2(Algorithm4(Fig.  13.9)::CoevolveAndTrainModelslines 8 and 13), Lipizzanerincludes a property devised to further increase diversityin the population through the application of a randomized variation  (mutation) operator affecting the ANN training loss function. Mutation in Lipizzaneris performed by applying SGD to the networks according to a given loss value. Thus, inspired by EGAN  [110], Lipizzanerwas extended by randomly picking a loss functionfrom a set of three loss (BCE, MSE, and HEU) at each epoch and each cell of the grid  [104].
The impact of operator-driven diversitywas analyzed by comparing four variants of Lipizzaner(i.e., three using only one of the tree loss functionsand one using the randomized loss function, a.k.a., Mustangs), vanilla GAN, and EGAN. The experimental analysis considered MNISTand CelebAdatasets and three grid sizes: 33, 44, and 55.
The quality of the images  (FIDscore) for both datasets was better for all Lipizzanervariations which produced better images compared to vanilla GAN and EGAN. Among the Lipizzanervariations, Mustangs and Lipizzanerwith BCE provided the most competitive results. However, the results indicated that Mustangs is robust to the variation of single loss functions(FID results showed reduced standard deviation) and can still find a high-performing mixture of generators(see Fig.  13.19). This provides further empirical evidence that diversity, both in genome  (ANN parameters) and operator  (loss function)space, provides robust GAN training.
Fig. 13.19
FID results on CelebAfor Lipizzanerand Mustangs on a 22 grid
The diversityof the generated samples also showed that the Lipizzanervariants outperformed vanilla GAN and EGAN  [104]. In addition, Mustangs trained the generative models that produced the most diverse images. Moreover, the diversityof the evolved ANN parameters was investigated to see if any ANN was replicated on grid. The result showed no significant replication of generatorANNs for all the Lipizzanervariations. Moreover, the diversity varied between setups and Lipizzanervariants. In general, the ANN diversity increased with the gird size, and it was further increased for Mustangs, randomization of loss functions duringtraining. The increase is most likely due to the impact of the loss functionon the training convergence of the ANN parameters. To conclude, the experimental evaluation showed that the spatially distributed coevolutionary GAN training combined with the SGD mutation operator (i.e., Mustangs) provided further diversity. Next, the impact of data diversityon AEL performance with Lipizzaneris presented.
13.4.3 Spatial Adversarial Evolutionary Learning with Data Diversity
Generative models consisting of mixtures of generatorsperform well (see Sect.  13.4.1). However, relying on multiple generators can be resource-intensive. AEL with Lipizzanermitigates some of this issue by distributing the training of mixtures of GANs on different computational resources to train them in parallel. A complementary strategy is to decrease the amount of training data. A straightforward approach is to train various GANs on smaller subsets of the training dataset. Using less training data entails lower time and storage (disk and memory) requirements while depending on the ensemble of generatorsto limit possible loss in performance from the reduction of training data.
As observed in Sect.  13.4.1.1, Lipizzanerspatial topology with overlapping neighborhood provides signal propagation(i.e., implicit communication) among the cells in the grid. This communication occurs during the update of the sub-population, when the neighbors are copied, since the best pair generator–discriminator in each sup-population is replaced after each epoch for each cell. The research presented in this section concerns the impact of the use of different subsets of training datasets at each cell of the grid topology. This approach was named Redux-Lipizzanerand it empirically studies data sample diversityin the spatially distributed GAN training.
The fundamental aspect of the Redux-Lipizzanerinvestigation was: the impact on the quality of generative models when training with less data. Instead of training each sub-population with the entire training dataset, per Lipizzaner, Redux-Lipizzanerapplies random sampling with replacement of the training data to define different subsets (partitions) of data that are used as a training dataset at each cell (see Fig.  13.11). Thus, each cell has its own training subsetof data.
The experimental analysis was performed on well-known image datasets: MNISTand CelebAMNISTexperiments were performed on grid sizes of 33 and 44, and CelebA on a grid size of 33. A vanilla GAN training method was also considered as baseline. Four different experiments for each dataset and grid size were defined by training the grid cells with 25, 50, 75, and 100% of the data batches. The dataset sampling process is independent for each cell of the grid and consists of randomly picking different batches of the training dataset. For each dataset, all experiments had the same budget of batches while keeping batch size, i.e., the number of images per batch, constant. Thus, when the training dataset size per cell varied, the number of generations was adjusted to maintain the same batch budget.
Fig. 13.20
FID results on MNIST experiments. The Y-axis was cut at FID=100 to facilitate comparison of the methods, x-axis shows the fraction of dataset used
Figure  13.20illustrates the mean FIDscore  (lower is better) for MINST experiments. Focusing on vanilla GAN (baseline), FID increased (performance worsened) as the GAN was trained on less data. The mean FIDwhen the GAN used 25% of data was 574.6 (i.e., the generative models produced basically noise). When the data subset was doubled to , the mean FIDscore improved to 71.2 (i.e., the generative models created poor-quality images). Mean FID significantly improved when 75% and 100% of data were used with values of 39.8 and 38.3, respectively. Additionally, for the 44 grid, the mean FID scores were 44.1, 40.5, 33.0, and 30.7, when the cells were training with 25%, 50%, 75%, and 100% of the dataset, respectively. In the case of the 55 grid, the mean FIDscores were 36.2, 31.8, 30.1, and 26.3. Finally, for the CelebAexperiments, the same outcomes were observed with mean FID scores which were 48.3, 42.4, 38.9, and 42.7 when the cells were training with 25%, 50%, 75%, and 100% of the dataset, respectively.
Three main results were drawn from these results: 
1.
The application of spatially distributed GAN training of Redux-Lipizzanerdramatically improved the results (in comparison with vanilla GAN).
2.
As the grid size increased, the trained generative models produced better quality samples (i.e., 44 FID is higher/worse than 55 FID). Moreover, 55 trained with 25% of the training dataset achieved comparable results to method 44 trained with 75 and 100% of the dataset.
3.
For all the experiments, generative models trained with 75% of the data produced fair quality samples as the models trained with the whole dataset.
In summary, it was observed that the spatially distributed evolutionary GAN training of Redux-Lipizzanerleverages neighborhood information exchange to produce high-performing generatorensembles. The spatially distributed grids allow training with fewer exemplars from the dataset, since signal propagationfacilitates information exchange among the grid. Next, we investigate how the ensembles generated by AEL with Lipizzanercan be re-purposed.
13.4.4 Re-purposing Heterogeneous Generative Ensembles with Optimization Objective Diversity
One approach to enhance the machine learning models’ task performance (e.g., classification or prediction) is to combine  (fuse) them. This is sometimes known as ensemble learning  [119]. There exists a bodyof work on applying evolutionary computation to learn ensembles of machine learning models known as Evolutionary Ensemble Learning (EEL)  [101]. The main idea is to compensate for the limitations of the models trained with the abilities of other methods by combining evolutionary computation with learning algorithms.
Lipizzanerconstructs generative models in a mixture of generatorsthat provide competitive results (i.e., high-quality generative models) while mitigating GAN training pathologies. Evolutionary Strategies (ES)  [73] is used in Lipizzanerto evolve the mixture weights used to define the ensemble of generatorsconsisting of the sub-populations of generators. The main objective of the ES in Lipizzaneris to maximize the quality of the generated samples (e.g., to optimize IS or FID metrics).
Toutouh et al.  [107] combined the spatially distributed GAN training of Lipizzanerwith EEL. The principal goal of this research was to improve the existing GAN training methods by applying a methodology inspired by the successful EEL. Applying this approach makes it possible to achieve better results in the task of constructing generative model ensembles without re-training the generative models from scratch. The methodology combines heterogeneously trained generatorsto create more competitive and efficient generative models while avoiding the high computational costassociated with re-training them. The focus was on applying EEL to re-purpose generative models, i.e., given a set of heterogeneous generatorsthat were optimized to provide the most accurate samples, create ensembles of them for optimizing a different objective (in this case, maximize the diversityof the generated samples). The EEL approach evolved the mixture weights that define the ensemble of generators to optimize the diversity measure  (i.e., total variation distance) of generated samples from an ensemble. At the same time, Lipizzaneroptimizes the network weights to optimize the minimaxoptimization GAN objective, which maximizes the quality of the generated samples.
For this new task of constructing ensembles of generative models with a different objective, two different optimization problems were defined to optimize (minimize) TVD: Restricted Ensemble Optimization of GENenrators (REO-GEN) and Non-Restricted Ensemble Optimization of GENenrators (NREO-GEN). REO-GEN restricts the ensemble search to a given specific size, while NREO-GEN relaxes the restriction by defining an upper bound for the size. These two optimization problems were approach with a genetic algorithm, which applies specifically designed crossover and mutation operators, and two greedy algorithms as baselines.
The proposed approaches were empirically evaluated over the MNISTdataset generation problem. The generative models were trained using Lipizzaneron a 33 grid. The experimental analysis showed that both evolutionary approaches built generative models that significantly improved the diversityof the pre-trained generatorswhile keeping a bounded computational cost. The genetic algorithm addressing NREO-GEN obtained the most competitive TVD results. The degree of freedom in searching for ensembles without specific sizes of NREO-GEN allowed the genetic algorithm to explore the search space more efficiently, leading to better results than the other approaches. Moreover, even though the greedy methods were less competitive than the evolutionary method, they were still able to compute ensembles of generators that improved diversity. However, the effectiveness of the greedy methods decreased when the ensemble sizes increase, due to the associated computational costof the greedy heuristic. Finally, the constructed ensembles significantly enhanced the TVD and also improved the FIDscore. Ensembles with four generatorsproduced the best results for generating MNISTdigits. Interestingly, 46.7% of the ensembles returned by the genetic algorithm for NREO-GEN comprised four generators. In summary, the adversarial evolutionary learning with a spatial coevolutionary algorithm can help enable high performance on tasks with different objectives. Next, we summarize the AEL experiments performed with Lipizzaner
13.4.5 Summary
The Lipizzanersystem for AEL demonstrates the ability to create high-performing generative models with reduced resources. This is done with a distributed spatial CCA for GAN training. In addition, Lipizzanercan scale to improve the performance even with the use of sub-optimal ANN architectures due to hardware limitation. Furthermore, Lipizzanerprovides diversitythat enables optimization of different generative task objectives. We also demonstrated how an increase in diversityimproves performance. In summary, the main advantages of AEL with Lipizzanerare 
(a)
fast convergence due to gradient-based steps;
(b)
improved convergence due to hyper-parameter evolution;
(c)
robustness due to spatial topology with coevolution and communication;
(d)
computational resource reduction from use of diverse data subset;
(e)
improved samples quality and diversity due to mixture evolution; and
(f)
scalability due to spatial distribution topology and asynchronous parallelism.
13.5 Conclusions and Future Work
We presented an overview of AEL, i.e., evolutionary approaches with competing adversaries which adapt over time. This is often defined as a minimization–maximization problem. We introduced a categorization of AEL based on the number of adversaries, population dynamics, and learning methods. CCA, MARL, AML, and EGT are categories that address AEL. We provided an in-depth demonstration of AEL applied to address the challenges of GAN training by applying a spatially distributed competitive coevolutionmethod named Lipizzaner. By using populations of multiple individual solutions (i.e., ANNs), the Lipizzanerfosters diversityin the gradient-based GAN learning. Extensive experimental analysis has been summarized to show that diversityincreases the robustness of the GAN training. The spatial topology helps to improve the scaling possibilities by reducing the computational complexity and to keep diversity through the grid. In addition, it has been shown how the application of hyper-parameter evolution and the diversityin the learning objectives, search operators, and data can be applied to improve the training further. All these results lead us to consider that the application of AEL methods in which diversity among populations is maintained may facilitate convergence in dealing with problems involving adversarial agents.
Future work includes further investigation on AEL, including the Lipizzanerframework, on other ML tasks, such as RL and multi-agent learning. Moreover, other input representations and application areas for AEL should be explored. Finally, the theory regarding AEL needs further elaboration because the current theoretical methods from EA theory do not completely explain or validate AEL methods.
References
1.
Adamopoulos, K., Harman, M., Hierons, R.M.: How to overcome the equivalent mutant problem and achieve tailored selective mutation using co-evolution. In: Genetic and Evolutionary Computation–GECCO 2004, pp. 1338–1349. Springer (2004)
2.
