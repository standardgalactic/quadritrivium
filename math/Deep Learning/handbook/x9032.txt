Kim, S., Kim, S.: Index tracking through deep latent representation learning. Quant. Financ. 20(4), 639–652 (2020)MathSciNetzbMATH
211.
Satpathy, T., Shah, R.: Sparse index tracking using sequential Monte Carlo. Quant. Financ. 22(9), 1579–1592 (2022)zbMATH
212.
Li, W., Paraschiv, F., Sermpinis, G.: A data-driven explainable case-based reasoning approach for financial risk detection. Quant. Financ. (2022)
213.
Lu, X., Abergel, F.: High-dimensional Hawkes processes for limit order books: modelling, empirical analysis and numerical calibration. Quant. Financ. 18(2), 249–264 (2018)
214.
Zhao, Z., Xu, F., Du, D., Meihua, W.: Robust portfolio rebalancing with cardinality and diversification constraints. Quant. Financ. 21(10), 1707–1721 (2021)
Footnotes
1
https://​www.​ft.​com/​content/​3405a512-5cbb-11e1-8f1f-00144feabdc0
2
https://​charteredabs.​org/​academic-journal-guide-2021/​
3
https://​scholar.​google.​com
©  The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.  2024
W. Banzhaf et al.(eds.)Handbook of Evolutionary Machine LearningGenetic and Evolutionary Computationhttps://doi.org/10.1007/978-981-99-3814-8_25
25.  Evolutionary Machine Learning and Games
Julian  Togelius1, 2, Ahmed  Khalifa3, Sam  Earle1, Michael  Cerny  Green1and Lisa  Soros4
(1)
Computer Science and Engineering, New York University, 370 Jay Street, Brooklyn, NY  07960, USA
(2)
Odl.ai, Nørrebrogade 184, 2200  Copenhagen, Denmark
(3)
Institute of Digital Games, University of Malta, 20 TriqL-Esperanto, Msida, MSD2080, Malta
(4)
Computer Science, Barnard College, 3009 Broadway, New York, NY 10027, USA
Julian  Togelius
Email: julian.togelius@gmail.com
Abstract
Evolutionary machine learning (EML) has been applied to games in multiple ways, and for multiple different purposes. Importantly, AI research in games is not only about playing games; it is also about generating game content, modeling players, and many other applications. Many of these applications pose interesting problems for EML. We will structure this chapter on EML for games based on whether evolution is used to augment machine learning (ML) or ML is used to augment evolution. For completeness, we also briefly discuss the usage of ML and evolution separately in games.
25.1 Introduction
Games of all sorts (including card games, board games, and video games) provide a rich domain for exploring computational intelligence. In many ways, games reflect the parts of the real world that we as humans find interesting, isolating key facets of our experience and encapsulating them within a tractable and interactive medium.
Beyond mere entertainment, games provide a unique domain for exploring and evaluating AI. Historically, the intersection of AI and games has focused on agents that play specific games well. In this way, games complement the litany of task environments for AI such as embodied agent control. However, games offer additional challenges beyond reward maximization such as effecting spirited play or emulating the style of particular humans.
In addition to playing games, there are challenges including generating content (such as levels, quests, textures, and characters), modeling players, matching players, and adapting interfaces. Content generation in particular requires deeply creative algorithms capable of understanding the essence of a domain and conjuring up new artifacts. In this way, games also provide an opportunity for exploring concepts such as algorithmic innovation and open-endedness
In this chapter, we survey the application of EML to games. We take an inclusive view of EML, focusing on cases where a ML model is evolved, but including examples of all kinds of interaction between evolution and ML. We also give brief overviews of the use of non-evolutionary ML and non-ML evolution in games, but given the breadth of the topic, those sections are mere sketches.
EML can, in one form or another, be applied to almost any AI challenge in games. However, this family of methods has seen much more application in some areas rather than others. Reflecting on this, a relatively large number of examples in this chapter will be from game content generation. But there will also be plenty of examples of game-playing evolutionary ML.
25.2 Machine Learning in Games
Some of the earliest advancements in ML are due to research on game-playing. In particular, Samuel’s Checkers player from 1959  [81] was the first example of what we now call reinforcement learning. While programs for Chess  [12], Checkers  [82], and Go initially built (and usually still build) on some form of tree search, machine-learned board value functions were introduced at an early stage and became crucial to any advanced efforts to play classical board games. These value functions could be learned through supervised learning, reinforcement learning, or some combination. AlphaGo  [89] and AlphaZero  [90] represent very successful combinations of tree search with ML that originated in research on Go playing but have found applications in a wide variety of fields.
At the outset of the deep learning era, certain video games came to play important roles as testbeds and benchmarks of deep reinforcement learningalgorithms. Deep Q-networks were introduced in a landmark Nature paper in 2015  [67], where they were trained on games from the Atari 2600 console via the Arcade Learning Environment  [6]. Since then, Atari games have been a crucial benchmark for deep reinforcement learning, and new algorithms are often tested on Atari games first  [49]. Several other video games have also become important as benchmarks for RL algorithms, notably Doom  [52] and MineCraft  [40]. To a lesser extent, supervised learning has been applied to these games to learn game-playing from large sets of human playtraces  [3].
Although a much smaller research area, ML algorithms of various kinds have also been applied to generating game content  [100]. Most of these applications fall into self-supervised learning, such as using generative adversarial networksor recurrent neural networks for generating levels forSuper Mario Bros  [2, 99, 112] or cards for Magic: The Gathering  [98]. However, there have also been attempts to use reinforcement learningfor level generation more recently  [53].
Other AI challenges in games for which ML has been used prominently include player modeling  [91], cheat detection  [47], and matchmaking  [66]. In player modeling, preference learning has been successfully applied to predicting player affect in response to game levels, game situations, or behavior. For cheat detection, explainable ML algorithms (such as decision trees) are most commonly used as the AI cannot act by itself and block players. It usually needs the moderator’s input to confirm if the flagged player is a cheater before banning the player from the game. Microsoft’s TrueMatch system, widely used in XBox games, uses ML on top of a ranking system to learn to match players well.
25.3 Evolution in Games
The robustness and wide applicability of evolutionary search means that evolutionary algorithms have been applied rather extensively to games. Many, perhaps most, examples of applications of evolution to games are in the form of neuroevolution, which counts as EML. However, there are also “pure” applications of evolutionary computation in games. Here we will survey two types: evolutionary planning and search-based procedural content generation.
25.3.1 Evolutionary Planning
For playing a game that has a fast-forward model, one preferably uses some kind of search algorithm for planning. Traditionally, this would mean a tree search algorithm, such as a Minimaxvariation for two-player games or some form of A* for single-player games  [79]. For games with a nontrivial search depth, one would need to cut off the search at some specified depth and use a board or state evaluation function to evaluate the deepest search node.
The plan that is generated by the tree search algorithm is simply a list of actions, with a numeric value (obtained from the state evaluation function) that signifies the projected value of that sequence of actions. Recognizing this, one might just as well use an evolutionary algorithm to evolve the plan, using the state evaluation function as a fitness function. In an approach called rolling horizon evolution  [71], several groups have done this for single-player games  [31], with results that are generally competitive with state-of-the-art tree search algorithms such asMonte Carlo Tree Search  [11].
The advantages of evolution over classic tree search algorithms become more clear in games with a very large branching factor  [84]. Games that feature control over multiple units, including many real-time or turn-based strategy games, can have branching factors in millions or even billions. This chokes most tree search algorithms as the search never gets past the first few moves. Evolutionary algorithms, operating on the level of the whole plan as a sequence, are not affected by this limitation. Results for several strategy games  [50] show that evolutionary planning can out-compete tree search by a large margin.
25.3.2 Search-Based PCG
After game playing, procedural content generation (PCG) is probably the topic in games that has received the most interest from the AI research community  [86]. Within academic research on PCG, evolutionary computation is currently the dominant approach because of the natural fit between the method and the problem. In search-based PCG, as the application of evolution to PCG problem is called, individual pieces of game content are evolved with a fitness function that reflects some measure of content quality  [109]. This approach has been applied to a large number of types of game content, including quests  [20], music  [26], textures  [114], rules  [17], and levels for platform games  [85], first-person shooters  [13], and real-time strategy games  [62].
Finding a fitness function that accurately reflects the quality of a candidate content artifact is typically nontrivial. Judging the quality of, for example, a game level is inherently hard and often requires playing the game. Writing code that performs this quality judgment is often harder still. Therefore, many search-based PCG implementations use simulation-based evaluation, where an agent of some kind plays part of the game that incorporates the content under evaluation. For example, the agent could play a level to see if it is playable under certain conditions  [54]. Such agents may be based on ML  [108]. In other cases, the fitness function is model-based and relies on some kind of learned model to do the evaluation  [51]. In yet other works, fitness is assigned interactively by a human player  [32].
25.4 Evolutionary Machine Learning in Games
In this section, we overview the use of EML in games research. While one way of categorizing this bodyof work is to divide game application (e.g. planning for non-player character actions, and PCG), another is to tear apart EML, considering the ways in which evolution and ML interact. We take the latter approach in the context of this book, as the former is better suited to works on AI and Games  [116]. We survey research in games using EML (as shown in Fig.  25.1) by
Augmenting Machine Learning with Evolution.
Augmenting Evolution with Machine Learning.
Augmenting other methods with Evolutionary Machine Learning.
The way we acquired this division is by looking into what is the end goal of the work that uses EML and analyze what is the core algorithm that is being used. For example, if the end goal is to have a neural network agent that controls a player in a video game, then evolution in that case is helping the ML (neural network) to achieve its goal. On the other hand, if the goal is to use evolution to find a certain content, then the search process is what is important and the ML is helping it to achieve its goal. We also found some other cases where the goal algorithm is neither an evolutionary algorithm nor ML, but where EML nonetheless plays an important supporting role.
Fig. 25.1
Taxonomy of EML in Games based on the dynamics between evolution and ML with examples of the usage of these techniques in games
25.5 Augmenting Machine Learning with Evolution
The easiest way to incorporate the evolutionary algorithm with ML is to utilize evolutionary optimization power to help the ML algorithm. Thinking about it from this direction allows us to see two ways where evolution can help. We can either use an evolutionary algorithm as the training process for the ML or the evolutionary algorithm for adjusting and helping the training process of ML, or both.
25.5.1 Evolutionary Training
Evolution can be used to directly train neural networks, either the weights of the network, its topology, or both. This is called neuroevolutionand has a long history in artificial life, robotics  [68], and video game research  [76]. It should be noted that some recent neuroevolution work, specifically focused on finding structures for deep networks, is alternatively billed as “neural architecture search”  [27].
25.5.1.1 Neuroevolution for Gameplay Tasks
The probably most common use for neuroevolutionin games is as a form of reinforcement learning. Here, the fitness evaluation of a chromosome is calculated by converting the genotype to a neural network (which might be as simple as setting the weights of a fixed topology) and using it to try to play the game, and basing the fitness on how well the network played the game. Compared to other forms of reinforcement learning, such as those based on variations of temporal difference learning or policygradients, neuroevolutionary reinforcement learningtends to be more reliable but has issues with scaling to very large networks  [76, 107].
In board games, the most common (and probably most sensible) version of neuroevolutionis to evolve a board value function used together with some version of the Minimaxalgorithm. Successful applications of this method go back a few decades  [28]; see also Sect.  25.7
In video games, where fast and deterministic forward modelsare rare, it is typically more appropriate to use neuroevolutionto generate a network that directly selects what action to take based on an observation of the environment. Early examples of this type of neuroevolutioninclude work on 2D car racing games  [105, 106]. In those examples, the neural networks were fed simulated rangefinders representing the track geometry in front of the car, and outputted accelerator/brake and steering commands. Another relatively early example of this type of neuroevolution is the evolution of neural nets to play Super Mario Brosbased on discrete grid sensor inputs  [104].
As the ALE benchmark suite, featuring games from the old Atari 2600 video game console, became popular with the rise of deep reinforcement learning, evolutionary algorithms were also applied to learn to play Atari games from pixel inputs. An influential paper by a teamat OpenAI showed that a simple evolution strategy can be competitive with standard DQN, and in particular that evolution scales effortlessly compared to other types of reinforcement learning[80]. Follow-up work by other authors showed that the slightly more sophisticated Canonical Evolution Strategy can perform much better  [16]. However, the more recent and sophisticated gradient-based RL methods outperform existing evolutionary methods on Atari games. In general, while standard neuroevolutionremains competitive with gradient-based RL when the network size is small, it tends to not scale very well to very large networks. This may be because the single fitness measure imparts less information than the dense rewards that can be used with some RL methods, or because of the lack of directional information in the random mutations in a high-dimensional space.
Various attempts have been made to overcome this limitation of neuroevolutionary reinforcement learningfor games with high-dimensional (such as visual) input space. One idea is to separate visual processing from the policy. This way, a smaller network that receives a lower dimensional encoding of the observation can be trained effectively by evolution. One idea is to use an autoencoderto compress the visual input, feeding the smaller policynetwork of the bottleneck layer in the autoencoder. This was tested successfully on the classic FPS game Doom, in a setup where the autoencoderwas continually re-trained as new sections of the level were discovered  [1]. Another idea is to represent the state using a dictionary of centroids instate space, feeding a sparse encoding to the policynetwork. This setup can be used to evolve surprisingly tiny networks, with as few as six neurons, to play Atari games well  [18].
One of the areas where neuroevolutioncan do things that regular reinforcement arguably cannot is in topology and weight evolution, where both the topology and the weights of the network are evolved. The pre-eminent algorithm here is NEATby Stanley and Miikkulainen, which has been applied to learn to play various games and performs very well on small to moderate-size networks  [96]. Later, it has been applied to the General Video Game framework with decent results  [72]. HyperNEATis an indirect encodingthat can in principle handle much larger input spaces  [95], and which has been applied to playing Atari games with some success  [14].
25.5.1.2 Neuroevolution for Non-gameplay Tasks
Neuroevolutioncan also be applied to other tasks in games besides game playing. In particular, there are several prominent examples of neuroevolutionfor procedural content generation. This includes the Galactic Arms Race game  [43], where players collaboratively evolve weapons, and the Petalz social network game where players evolve flowers  [75]. One can even use neuroevolution as a form of meta-content generator, evolving a population of networks that can generate a large range of different levels  [24]. An interesting use of neuroevolutionfor non-player characters is in the NERO game, where players train populations of agents to fight for them  [94]. Even earlier, the cult video game Creatures uses a form of neuroevolution as a game mechanic.
Neuroevolutionhas also been used to model a certain playstyle  [45, 46]. Holmgaard et al. used a evolutionary strategy to evolve a single-layer neural network that decides on the next target in MiniDungeons. The agent then uses the A* algorithm to navigate toward the selected target. The agents were evolved using action agreement ratio  [45], tactical agreement ratio, and strategic agreement ratio  [46] to mimic different playstyles from a corpus of collected human data. The final evolved agents were not only able to finish unseen levels but also navigate them in the same vein as the target playstyle.
25.5.2 Evolutionary Assisted Training
In the work described in Sect.  25.5.1above, the evolutionary process serves to learn more sophisticated policies—e.g. for game playingor design, whether by the modification of action sequences, game assets, or of neural network weights or architectures. In this section, on the other hand, the learning or training process is implemented by some non-evolutionary algorithm (typically gradient descent), while evolution exists as an auxiliary process that renders learning more capable or efficient. This section focuses primarily on a particularly prominent role taken on by such an assistive evolutionary process, which is creating or curating a “curriculum” for a learning agent.
25.5.2.1 Curriculum Design in Machine Learning
Curriculum learning[7] takes inspiration from human developmental psychology, where, by way of example, children are taught complex subjects (e.g. mathematics) incrementally over time, by first familiarizing them with more elementary and fundamental concepts (such as basic arithmetic) before moving on to more sophisticated techniques (such as calculus or linear algebra). In the context of ML, a curriculum may expose a learning agent to increasingly complex training examples, switching from simpler to more complex ones once the agent’s abilities reach a certain level.
For example, a neural network-based agent tasked with navigating through a maze to a goal tile will in all likelihood be incapable of navigating a complex maze by sheer luck at the very beginning of training (i.e. with randomly initialized weights resulting in it taking effectively random actions at each step). It would thus be wasteful to expose it to such complex mazes at the beginning of training. It has a much better chance, on the other hand, of stumbling upon the goal tile in simpler mazes, where the goal is close to the starting position. A curriculum learningapproach could thus order mazes by their complexity (or the distance between start and goal positions) only introducing more complex mazes into the pool of training data once the agent has learned to reliably solve a set of simpler mazes.
In ML, curriculum learningcan not only make training more efficient (avoiding training on infeasible examples early on) but also lead to improved generalization, by allowing the model to incrementally learn more meaningful representationsof the problem at hand. The importance of curriculum learningfor training generally capable strategies over a large space of tasks has been demonstrated in XLand [102, 103], where a large set of tasks are generated in a 3D environment, where embodied agents take visual input and must navigate a procedurally generated terrain to achieve certain goal states, which involve manipulating and re-combining various primitive objects. In XLand, a massive space of tasks (involving different rules and initial map conditions) are generated before learning, then curated during agent training so as to produce a curriculum of increasing complexity. This curation is aided by metrics using agent play-through to measure whether a given task is both nontrivial and learnable (i.e. whether it is on the “frontier” of agent abilities). The resulting player agents are able to generalize to new tasks—involving different object-recombination mechanics and goal states—that were not seen during training.
25.5.2.2 Evolutionary Curriculum Design
Otherwork generates training environments on the fly, then curates or filters them to ensure learnability, resulting in a series of environments of increasing complexity over the course of agent training player agents. PAIRED[21] introduces this notion of “Unsupervised Environment Design”, using an RL agent to generate level layouts of training environments so as to maximize learnability (formulated as approximate regret). More recently, ACCEL[70] effectively evolves these environments, applying small mutations to level layouts and filtering them for regret. The domains considered include tile-based maze-like environments in which the player must navigate to a goal (while sometimes avoiding lava) and the 2D physics-based “bipedal walker” environment. The mutations involve changing the state of given tiles in the former case and changing the height of the terrain in the latter. The fitness criterion of a mutated level is the regret it induces in the player agent. The authors show that the evolution of increasingly complex environments (lengthy and obstacle-heavy mazes, and rough or steep terrain) coincide with the development of a robust player agent capable of generalizing to unseen environments (e.g. capable of solving a maze in a grid much larger than those evolved during training).
POET  [113] similarlyevolves environments online while training player policies in the bipedal walker environment, though it trains multiple player agents concurrently, pairing these with particular environments (and occasionally transferring players to different environments mid-training in a paradigm reminiscent of transfer learning[110]). Here, terrain is represented by a Compositional Pattern Producing Network (CPPN  [93]), and evolved using Neuroevolutionof Augmenting Topologies (NEAT  [96]). Agent policies are optimized via an Evolutionary Strategy  [80]. Mutated environments are added to the pool of environments if they are neither too hard nor too easy for existing agents, and if they are sufficiently novel with respect to environments already in the pool, thus encouraging diversityamong the problems generated (and solved) by the algorithm. PINSKY  [22] applies the framework introduced by POETto 2D tile-based games.
In Go-Explore  [25], a variant of novelty search[60] is used to incentivize explorationby an RL agent, by effectively maintaining an archiveof behaviors that result in novel states in a sparse reward problem. The domain tackled in this work is Montezuma’s Revenge, a side-scrolling adventure game from the Atari 2600 suite, which was formerly unsolved by methods not relying on expert demonstrations. Go-Explore facilitates explorationby looking at the (x,  y) coordinate location of the player avatar on the screen, and, whenever a new position is reached by the player, storing in memory the sequence of behaviors that led to this novel state. On subsequent play-throughs, the agent can then reliably navigate back to an (x,  y) position on the frontier of that which it has already explored, so that it will tend to explore further and further out from its starting location, and so that once a tricky segment has been solved—such as jumping carefully across a dangerous chasm—it can be reliably solved on every subsequent play-through without further trial and error. When the goal state is reached, they train a game-playing agent on the successful trajectories from the archive
25.6 Augmenting Evolution with Machine Learning
The goal in augmenting the evolutionary process is to mimic the output of the evolution process by using ML to create an easier landscape to navigate, better operators, faster fitness function, etc.
Fig. 25.2
An example of a Sokoban level with and without the human player
25.6.1 Machine Learning as a Representation
Evolutionary algorithms are sensitive to the input representation  [35]. There is a bodyof research on understanding and exploring the different representations and their effects  [78]. In most of these works, it is agreed that a good representation provides us with a smooth fitness landscape. This means that individuals that are close to each other in representation space have similar fitness. In many game domains such as level generation, a direct representation (2D matrix of tiles) is usually not the best as it is too sensitive to small changes. For example, removing the player tile from a level will make the level unplayable immediately (Fig.  25.2). Instead of utilizing domain knowledge to figure out a good representation, ML models can be used to learn this representation. In that case, ML acts as a genotype-to-phenotype mapper for the evolutionary algorithm. The evolutionary algorithm can just work on the provided representation (called latent variables) and modify it using a mutation function, then the ML transforms it to the phenotype where it gets evaluated using a fitness function.
Fig. 25.3
Generated Super Mario scenes using CMA-ME that are similar to either sky levels or ground levels
25.6.1.1 Super Mario Bros
Although there are many ML methodsthat can be used to learn a good representation, most of the work focuses on either AutoEncoders[57] or Generative Adversarial Networks(GANs)  [36]. For example, Volz et al.  [112] trained a GAN on the levels of Super Mario Bros(Nintendo, 1985). The levels are divided into single scenes of size (28 14) using a sliding window. After training the network on generating new scenes, they used CMA-ES[42] algorithm to search the latent spaceof the GAN for playable Mario levels with different game features such as maximizing/minimizing the number of jumps. The fitness was calculated using an A* agent that measures the playability percentage of the generated levels and the number of jumps that have been performed. The generated levels mostly follow the structure of original Mario levels without the need for specifying that in the fitness function or the representation  [19, 85]. Following the previous work, Fontaine et al.  [29] useda quality diversityalgorithm called Covariance Matrix Adaptation MAP-Elites(CMA-ME)  [30] to discover new and diverse playable levels. Utilizing CMA-ME helped the authors overcome the repetition of the generated levels using normal CMA-ESand find new levels that are hard to find using normal CMA-ES such as levels that combine two different level styles (Fig.  25.3). Another advantage is at the end of the run, the algorithm manages to produce a corpus of playable levels with different features instead of a single playable level as in the case of CMA-ES
So far, the previously discussed work only focused on generating a small scene of Super Mario levels of size (28 14); creating a full level by concatenating the generated scenes sometimes might end in an unplayable level (see Fig.  25.4). This is because evolution does not have any context about the previously generated scenes. This can be easily solved by modifying the fitness to incorporate all the previous scenes. The problem is that evolution cannot change anything from the previously generated scenes. This might cause evolution to repeat certain patterns to solve the problem. Another simple solution is to evolve the whole level all at once and use the GAN to transform each sequence independently. The problem with that is for very long levels; this could be hard as the representation might be big and not easy to evolve due to conflict in fitness between different parts of the chromosome. To solve this issue, Schrum et al.  [83] proposed encoding the big level using a compressed representation which is encoded in the form of a CPPN  [93]. The CPPN takes the location of the scene, and it spits out the latent variable for that area. The NEAT  [96] algorithm was used to create the full level. The output of this experiment shows that using CPPNs to evolve the structure of the level followed by normal evolution on the concatenated latent variables produced better results than any part alone.
Fig. 25.4
These Mario scenes are playable on their own, but combining them in that order will not be playable
25.6.1.2 Other Applications
Tanabe et al.  [101] used latent variable evolution withVariational AutoEncoder(VAE)  [74] to generate levels for Angry Birds(Rovio, 2009). They treated the generated levels as a sequence instead of being a tile map to overcome the problem of having variable size objects and having objects overlayed each other. They searched the latent spaceof VAE using CMA-ESalgorithm to find levels that minimize the usage of birds, maximize the number of pigs, etc.  search. Another avenue is to use interactive evolutionto create a mixed initiative tool  [87] combined with the power of ML. This is usually a common avenue in subjective domains that are hard to measure its success. There is a substantial breadth of work in this area, and a full review is outside the scope of this book chapter. However, some recent works generate art  [48], human faces  [117], backgrounds, and shoes  [10]. However, as far as we know, these tools have not been used in texture generation or game art. This could be an interesting avenue to explore with the rise of large generative models such as Stable Diffusion  [77].
25.6.2 Machine Learning as a Target Function
