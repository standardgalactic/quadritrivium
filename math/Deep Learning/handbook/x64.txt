Energy (pJ)
Add
Integer
8
0.003
16
0.005
32
0.1
Float
16
0.4
32
0.9
Multiply
Integer
8
0.2
32
3.1
Float
16
1.1
32
3.7
Read
SRAM
32
DRAM
32
Hardware parametersof a given CNNimplementation are usually obtained using the following methods:
Measuring a real hardware implementation provides exact values; however, it is time-consuming to build and measure real hardware.
Simulation using precise hardware simulatorscan provide accurate results, but it is still time-consuming when conducted at the gate level.
Analytical estimation consists of analyzing the CNN’s computational graph and applying precomputed knowledge about the cost of particular operations on given hardware  [7].
Building a surrogate model capable of predicting a given hardware parameter. It requires selecting suitable features for the predictor and collecting annotated data. Various machine learning models have been utilized for this purpose, e.g., linear regression  [71], neural network   [80], Gaussian process  [37], and Bayesian Ridge Regression  [78].
Predicting the resulting latency using easy-to-obtain properties of DNNs, such as the number of weights or MACs, is highly unreliable  [69]. However, a recent paper shows that one carefully constructed proxy model(predictor) is enough for hardware-aware NAS[37]. This is also documented by detailed benchmarking  [31], disclosing that the inference latency and energy of CNNarchitecture on hardware are strongly correlated. It concludes that an energy constraint can be implicitly mapped to a corresponding latency constraint in NAS methods.
To illustrate the complexity of the mappingoptimization, we consider ResNet-50 CNN[23], which should be implemented on Eyeriss and Simba accelerators. Tools such as Accelergy and Timeloop help in determining the most suitable mapping. Accelergy  [75] is an early-stage energy and execution time estimation tool. It estimates hardware parametersof a CNN implementationon a given accelerator whose organization is described at the architecture level in the YAML language, i.e., using characteristics such as the number of PEs, memory size, on-chip network, and data floworganization. Plug-ins for different fabrication technologies can be integrated. Timeloop  [53] is a tool searching for the most suitable mappingof  CNNto hardware accelerator(several search methods are available in the tool). It performs CNNlayer-wise data tiling reflecting the memory hierarchyof the accelerator.
As an example, Fig.  12.5shows a distribution of the energy efficiencyof half a million randomly generated mappingsfor the first convolutional layerof the ResNet-50 network. Timeloop conducted this estimation for two accelerator models: Eyeriss, featuring 168 PEs, and Simba, equipped with 16 PEs. Both models were simulated using 45 nm fabrication technology and utilized external weight memory (LPDDR4). Note that the considered layer performs convolution over the 2242243 input feature map, with 77 filters, stride 2, and 64 output channels. The energy efficiencyof mappingsexhibits substantial variations. These variations stem from the diverse options for tiling and scheduling, which are represented by different mappings. The more efficient mappingsexcel in utilizing buffer capacity, network subsystems, and loop ordering to maximize data reuse. Nevertheless, the optimal mappingis subject to change based on the workload. What may be an optimal mappingfor one architecture could prove to be suboptimal or even invalid for another architecture.
Fig. 12.5
The distribution of mappingsshowcasing energy efficiency (pJ/MAC) for the first layer of ResNet-50. The mappings were generated by Timeloop for the Eyeriss accelerator (left) and the Simba accelerator (right)
12.3 Evolutionary Optimization in DNN Hardware Accelerators
Surveys  [44, 61] document the gradually increasing interest in optimizing hardware implementations of fully trained CNNs(i.e., the inference accelerators), especially in the context of edge computing. In this chapter, we present approaches utilizing EAs for this purpose. Our primary focus will be on elucidating the rationale behind employing EAs in each design or optimization problem and discussing the methodologies employed.
12.3.1 Evolutionary Design of Components of Hardware Accelerators
Elementary components ofCNN acceleratorssuch as various arithmetic operations can be evolved and optimized to improve hardware parametersof these accelerators, and in some cases, the accuracy, too. Genetic programming (GP) is almost exclusively used as the evolutionary method. The fitness function reflecting the functionality of candidate designs is based on either (1) applying a candidate solution in a CNN, training the CNN, and interpreting the CNN’s error as the fitness value, or (2) comparing the functionality of a candidate solution with a reference implementation of a given component on some data and determining the fitness as, e.g., a mean squared error. While approach (1), in principle, leads to more reliable solutions (the entire CNNis evaluated), approach (2) is computationally less expensive. In this context, typical targets for the evolutionary approach are multipliers, MACs, and activation functions
12.3.1.1 Approximate Multipliers
The inexact (approximate) multipliers provide inexact products; however, this inexactness can be tolerated because CNNsare often highly error-resilient  [60]. In addition to reducing the bit width of multipliers used in MAC units, the approximation can be achieved by simplifying the logic equations specifying the product. The task is to design approximate multipliers showing good trade-offs between the accuracy and hardware parameters(such as energy and latency). In addition to many manual approximation methods, a fully automated circuit approximation methodology based on Cartesian genetic programming(CGP) has been developed  [49, 70]. A common strategy is to optimize the multiplier with respect to an exact multiplier. In the fitness function, the error is expressed using error metrics such as the worst-case error or the mean absolute error. If a multiplier showing a good trade-off between the error and hardware parametersis discovered by CGP, it is used instead of the exact multipliers in one or several layers of a CNN. The CNN’s accuracy is then determined, typically after a short fine-tuning. Based on the final accuracy, the evolved multiplier is accepted or rejected. This two-step design process is adopted because many candidate multipliers have to be generated, and evaluating each of them directly in the final CNNis very time-consuming.
Every candidate approximate multiplier , which is generated by a gate-level CGP, has two inputs (nand mbits) and produces a bit output  [70]. The objective is to minimize the cost of the circuit (which highly correlates with power consumption) assuming that shows the worst-case error (WCE) at most (Eq.  12.1):
(12.1)
The cost is estimated as the sum of the weighted areas of the gates used in the circuit. As the approximate multipliers are supposed to be used in neural networks, the requirement for accurate multiplying by zero (is integrated together with the WCE constraint in the fitness function. The validityof both conditions is checked using a single pass of exhaustive simulation of each candidate multiplier. At the end of evolution, the best-scored circuit is synthesized to get all its hardware parameters. The resulting approximate multiplier is also used in a given CNNto obtain classification accuracy, which is typically worsened in comparison with the CNN utilizing exact multipliers. However, the accuracy has usually recovered after retraining for a few epochs.
The case study reported in [49] deals with a situation in which all 8-bit multiplications of all convolutional layersof ResNet CNNsare replaced with one particular approximate implementation of the multiplier. Various evolved approximate 8xN-bit multipliers that are available in the EvoApproxLib  [48] are tested. One operand (the activation) is always at 8 bits and the second operand (the weight) is on Nbits, where . Optimizing the bit width leads not only to smaller circuits but also to the reduced size of weight memory. Figure  12.6shows trade-offs between accuracy and energy of multiplication when ResNet-26 uses various approximate multipliers in its convolutional layers. For a small drop in accuracy, a 50% energy reduction of multipliers is obtained if a suitable approximate multiplier is used. Results are given for the 45 nm process and power supply voltage 1 V. The same approach was taken to design approximate MAC circuits in authors’ work   [9].
Fig. 12.6
The energy-accuracy trade-offs when all exact 8-bit multiplications of all convolutional layersof ResNet-26 CNNare replaced with an approximate multiplier taken from the EvoApproxLib library of 8xN-bit approximate multipliers (35 different multiplier implementations tested)
12.3.1.2 Activation Functions
There are some frequently used activation functionssuch as ReLU or Sigmoid. However, better activation functions can be obtained for particular data sets and CNNarchitectures. EAs are thus employed to either deliver new functions (without considering their hardware implementation)  [4, 52, 65] or optimize existing functions (with respect to resources)  [58]. A common approach to evolve a new activation functionis to employ a tree-based genetic programming (GP) which typically utilizes the function set consisting of hardware unfriendly functions (atan, tanh, etc.). The fitness is based on evaluating the entire CNNin which a candidate activation functionis embedded  [4, 52].
For example, Lapid and Sipper  [30] employs coevolution to evolve activation functionsfor image-classificationtasks using CGP. The function set comprises ten commonly used activation functions(ReLU, tanh, ELU, etc.) and 5 arithmetic operations (, minimum, and maximum). A cooperative coevolutionalgorithm evolves input-layer, hidden-layer, and output-layer activation functions (each having a separate population). An individual’s fitness is determined by the activation function’sability to cooperate with members of the other populations (the fitness procedure is elaborated in  [30]). On four classification datasets (MNIST, FashionMNIST, KMNIST, USPS) and two neural networks (a 7-layer MLPand an 8-layer CNN), the method was capable of improving the classification accuracy compared to the reference solution.
A hardware-aware evolutionary designof activation functionsis conducted by Prashanth and Madhav  [58]. Ordinary activation functions(sigmoid, tanh, Gaussian, ReLU, GeLU, Softplus) are considered as golden solutions, and their gate-level implementations are evolved for a given bit width using CGP. In the fitness function, a fully functional solution specified by a truth table is sought in the first step. Once it is obtained, its size is minimized in the second step.
12.3.1.3 Component Selection and Precision Scaling
Suppose that an 8-bit CNNthat hasto be accelerated consists of more layers (neurons) than processing units available in the accelerator. Furthermore, approximate multipliers can be utilized in configurable processing units. Two tasks have to be solved together: (1) the assignment of the approximate multipliers to MACs of the processing units and (2) the assignment of the convolutional layersto the processing units. ALWANN is an optimization tool capable of selecting a suitable approximate multiplier for each processing unit in such a way that one approximate multiplier serves several layers, and the overall classification error and energy consumption are minimized  [50]. The optimizations, including the multiplier selection problem, are solved by means of the NSGA-IIalgorithm in which the overall CNNaccuracy and the energy consumed by the approximate layers are considered. Each candidate solution is uniquely defined by a pair of mappings(map1, map2), where map1 is a list of kintegers in which each integer represents an approximate multiplier (taken from EvoApproxLib) assigned to processing units , and map2 is another list of lintegers in which each integer determines the index of a processing unit that will be used to compute the output of the layer. Additional restrictions may be applied depending on the chosen HW accelerator’s structure. In order to altogether avoid the computationally expensive retraining of CNN, which is usually employed to improve the classification accuracy, a simple weight updating scheme is proposed that compensates for the inaccuracy introduced by employing approximate multipliers. ALWANN is evaluated for two architectures of CNNaccelerators with approximate multipliers from the open-source EvoApproxLib library while executing three versions of ResNet on CIFAR-10. ALWANN saves 30% of energy needed for multiplication in convolutional layersof ResNet-50 while the accuracy is degraded by only 0.6% (0.9% for the ResNet-14).
Barone et al.  [1] propose E-IDEA, an automatic application-driven approximation tool targeting different implementations (hardware and software). E-IDEA uses Clang-Chimera tool to analyze the Abstract Syntax Tree (AST) of the application’s source code. Through the so-called mutators, approximations can be introduced at the source code level. The set of mutatorsincludes loop-perforation mutators, precision-scaling mutatorsfor floating-point arithmetic, a precision-scaling mutatorfor integer arithmetic, and a mutatorsupporting approximate arithmetic operator models of circuits being part of the EvoApproxLib library. An evolutionary approximation method based on NSGA-IItries to find the best approximation version of a given C/C++ code according to user-defined optimization objectives. A candidate solution is represented as a vector of integers; each of them corresponds to one parameter that can be modified. A set of matching rules specifies the positions in the source code at which a mutation can be applied. E-IDEA was used to approximate weighted sums computed within neurons to reduce hardware requirements and power consumption. Clang-Chimera was configured to truncate input operands and results of multiplications in the three convolutional and the two fully connected layers of the considered network (LeNet). Thus, the tool generates an approximate version of the considered CNNin which it is possible to configure the number of approximate bits to tune the introduced approximation degree for each multiplication involved in the weighted sum. Moreover, Clang-Chimera could select a suitable approximate multiplier from a library of approximate multipliers in these layers. NSGA-IIoptimized the CNNerror on MNIST, aiming at reducing the circuit area. This allowed finding solutions to achieve more than 30% savings, with a negligible accuracy loss (0.48%) compared to the reference solution.
12.3.1.4 The CNN-to-Hardware Mapping Optimization
In the GAMMA (Genetic Algorithm-based Mapper for ML Accelerators) framework, configurable hardware acceleratorsare considered, i.e., computation order, parallelizing dimensions, and tile sizes can be configured at compile-time  [29]. For given constraints (the maximum number of parallelism levels and maximum tile sizes), GAMMA is searching for the most suitable mappingof a CNNlayer (see the algorithm in Fig.  12.2) on the hardware resources modeled in MAESTRO  [42]. For a given CNNlayer, hardware configuration (the number of PEs, local buffer size, global buffer size, latency, and bandwidth), and a mappingstrategy, MAESTRO estimates the statisticssuch as latency, energy, runtime, power, and area.
Fig. 12.7
GAMMA’s encoding of a two-level mapper (top) and its decoded description for cost model (MAESTRO) of an NVDLA-like two-level mapper (bottom).
Adopted from  [29]
The mappingis composed of several levels. Each level represents parallelism across a spatial dimension of the accelerator. Figure  12.7shows how a two-level mappingis encoded in the chromosome. The convolutional layeris specified using Cinput channels, Koutput channels, input activations of size , output activations of size , and filers of size . Each dimension is encoded using seven pairs of values. A pair of genes contains a CNNlayer tensor notation (e.g., K, C) and its tile size. The ordering of pairs specifies the computation order. The first pair defines the parallelizing dimension. The L1-mapper describes the inner loop. The L2-mapper describes the outer loop, while containing PL1 number of instances of L1-mapper. The chromosome is used to create a candidate mappingwhich is then evaluated in MAESTRO.
GAMMA supports several optimization algorithms, including a genetic algorithm with application-specific operators for mutation and crossover  [29]. The fitness is defined as a reward value (e.g., latency, energy, or power) if a constraint on hardware resources is met. Otherwise (i.e., when evolved mappingrequires more resources than the accelerator provides), a large penalty is assigned. GAMMA is evaluated on five CNNmodels with different complexity (VGG16, MobileNet-V2, ResNet-50, ResNet-18, MnasNet) and two platforms (TPU and Eyeriss) with different number of hardware resources. Across CNNmodels and various hardware platforms considered, GAMMA finds solutions costing 5to (1.2E+5)less latency and 2to (1.6E+4)less energy.
In another method, AnaCoNGA, the quantization problem and hardware optimization problem are solved concurrently for a given (trained) CNN[19]. The hardware architecture search (HAS) is embedded into quantization strategy search (QSS), in a nested genetic algorithm formulation. For each potential quantization strategy proposed by QSS, the HAS loop efficiently optimizes the accelerator’s parameters. In QSS, a multi-objective GA is used to tackle the multi-criteria optimization problem of maximizing accuracy and minimizing hardware-related costs. No hardware design takes place in this search. The quantization search space for a CNNhas a size of , where Qis the set of possible quantization levels for weights and activations, and Lis the number of layers in the neural network. In the second GA (HAS), each individual’s genome captures hardware parameterssuch as the PE size, the number of binary dot-products each PE can perform in parallel, and buffer sizes. The fitness criteria of this GA are the hardware design’s execution performance (compute cycles and DRAM accesses) of a predetermined quantizedCNN, as well as the number of FPGA resources (BRAMs and LUTs) it requires for its allocation. Note that an alternative approach to the nested formulation could be to combine HAS genomes with QSS genomes into one GA. However, this would result in a prohibitively complex and large search space which is difficult for GA.
In order to quickly evaluate candidate hardware designs, an analytical hardware model for the execution of CNNon a state-of-the-art accelerator (such as the Xilinx Z7020 SoC on the PYNQ-Z1 board) is created. AnaCoNGA is evaluated on ResNet-20 (using CIFAR-10data set), ResNet56 (CIFAR-100), and ResNet-18 (ImageNet). With AnaCoNGA, the accuracy of ResNet-20 (on CIFAR-10)is improved by 2.88% compared to a uniform 2-bit CNN, and achieved a 35% and 37% improvement in latency and DRAM accesses, while reducing LUT and BRAM resources by 9% and 59% respectively, when compared to an edge variant of the accelerator  [19].
12.3.1.5 Weight Sharing and Compression
Weight sharingenables toreplace a group of similar weights with a single value. Instead of storing all the CNNweights, only a limited number of shared values and a codebook, where original weights are replaced by their corresponding indexes, are stored in the CNN memory. This approach is also known as weight compression. For example, if the original weights are encoded on 32 bits and there are 256 different shared values, only the shared values and 8-bit indexes must be stored, reducing thus the memory footprint almost four times compared to the original weight set. Shared weight values are typically obtained with a clustering algorithm like K-means. The weight-sharing ideacan be applied at the level of the entire CNNor for each layer separately. The objective is to automatically select the optimal number of shared values per layer for the input CNN assuming that a range of possible numbers of shared values is provided. Introducing the shared values typically leads to a loss in accuracy. Hence, a suitable trade-off between the accuracy drop and compression rate is sought. Dupuis et al.  [17] adopted a two-step approach to compute the shared weights. In the first step, the number of shared values is determined locally and separately for each layer by an exhaustive search The second step, based on the values obtained in the first step, tries to determine the most suitable combinations of shared weights across the entire CNN. As the number of combinations grows exponentially with the number of layers, the problem is solved by NSGA-II. The bottleneck is the accuracy evaluation because it requires evaluating CNN for each set of candidate (shared) weights. A proxy regression model was created using data obtained in the first step to accelerate the evaluation. The results carried out on recent CNNmodels, trained with the ImageNetdataset, show over 5memory compression at an acceptable accuracy loss without any retraining step.
12.4 NAS Considering the Target Hardware
NASmethods utilizing evolutionary algorithms to deliver a CNNarchitecture with a minimum error on test data were introduced in previous chapters of this book. Hardware-aware NASmethods extend this approach by considering other objectives such as latency, energy efficiency, and memory footprint with respect to a hardware platform implementing the neural network  [3, 63]. Hardware-aware NASmethods can be seen as multi-objective optimization methods. Hence, in certain steps of the NAS algorithm, all relevant objectives must be evaluated, either by direct measurement on real hardware or estimated using software models (Sect.  12.2.4). A common approach to solve the multi-objective NASproblem adopted by the NAScommunity is either (i) to transform it into a single-objective one (using suitable constraints, prioritization, or aggregation techniques) and solve it with a common single-objective method or (ii) to employ a truly multi-objective approach(such as NSGA-II)  [15].
A common practice is to model a candidate CNNusing a directed acyclic graph encoded as a variable-length string. If only some hyperparameters are optimized then the chromosome is a fixed-length list of integers. All possible strings describing valid CNNs constitute the search space. To reduce the NAStime, the so-called supernetisoften constructed first  [55]. A supernet is an over-parameterized neural network built over a certain backbone CNN model, in which many options are supported for selected hyperparameters. The supernetis trained to solve a given problem. Its training is usually very costly as the supernetis more complex than any individual CNN. However, this cost can be amortized as many suitable subnetworks, including their weights, can be extracted from the resulting supernet for a given specification (e.g., latency, accuracy, or energy constraint on given hardware) and used without repeating the expensive training process. The search for a suitable subnetwork, which can be conducted using an EA, is less expensive because it does not involve any training. A general limitation of this approach is that the supernetrestricts the search space to its subnetworks.
Two major directions can be identified in the area of NAS methods explicitly targeting hardware implementations:
Hardware-aware NAS, whose goal is to find the most suitable CNNmodel concerning a target hardware platform and the objectives to be optimized. Note that there is no additional search space to the neural architecture search space.
NAS with hardware co-optimization, whose goal is to co-optimize CNNmodel and hardware configuration (such as amount and type of resources, dataflow strategies, buffer sizes, and compiler options). These methods work in three search spaces (weights, neural architectures, and hardware configurations) and must innovatively orchestrate several search algorithms to produce the best trade-offs between the accuracy and various hardware-relevant metrics.
These two directions will be discussed in the rest of this section.
12.4.1 HW-Aware Evolutionary NAS
An evident approach to optimizing the CNNarchitecture for given hardware is employing only hardware-friendly hyperparameters and operations, i.e., suitable convolution types, arithmetic operator implementations, quantization schemes, or memory access mechanisms with respect to the optimization objectives. For example, based on benchmarking 32 different operators, Hurricane  [78] uses different subsets of operator choices for three hardware platforms. This way, the search space is narrowed toward CNNarchitectures suitable for a given hardware platform.
Table 12.4
Hardware-aware evolutionary NASmethods. Titles of some data sets are abbreviated, e.g., C-10 for Cifar-10, C-100 for Cifar-100, ImgNet for ImageNet; the symbol denotes that some additional data sets were omitted because of space limitations
Method
Ref.
Year
Search space
Super net
Objectives
Estimation method
Target device
Data set
Large-Scale
[59]
2017
macro
None
None
GPU
C-10, C-100
JASQNet
[11]
