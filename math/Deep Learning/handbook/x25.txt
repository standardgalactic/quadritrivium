The goal of feature constructionis to build new high-level features that are functions of the original features, but leads to simpler decision boundaries. The main difference between feature selectionand feature construction is that feature constructionbuilds new features while feature selectiononly selects a subset of original features (i.e., no new features). Feature construction is particularly useful when the original features are not informative enough. Among EC algorithms, GP has been widely applied to achieve feature construction since its flexible representation can represent functions naturally [102]. GP does not require any pre-defined model structure for a new feature and any operators can be included in a GP program. Figure  7.8shows a new feature () represented by a GP tree.
Fig. 7.8
Tree-based representation for feature construction
The overall system of feature constructionis very similar to the system of feature selection in Fig.  7.7. The only difference is that the discovery component generates new high-level features instead of feature subsets as in feature selection. When the number of classes is small (i.e., binary classification), building a single high-level feature could be sufficient [73, 112]. However, when the number of classes is large, we usually need to build multiple high-level features, which can be achieved by multi-tree GP. Particularly, each individual in multi-tree GP consists of a set of trees. A common approach is to associate each tree to a class. Each tree differentiates the instances from the tree’s corresponding class to the instances from the other classes. The results show that such association results in much better performance than single-tree GP or multi-tree GP without any association [102]. Nguyen et al. [81] use a hybrid vector-tree representation to perform feature selectionand feature constructionsimultaneously, which achieves better performance than performing each of the two tasks separately.
7.2.4.3 Evolutionary Instance Selection
The goal of instance selection is to select a subset of instances that are not noisy or redundant, which is similar to feature selection. We can use the same representations of feature selectionto achieve instance selection. The only difference is that each vector entry (in the vector-based representation) or node (in the tree/graph-based representation)correspondsto an original instance rather than a feature. EC algorithms have been successfully used for instance selection since they do not require any assumption regarding data structure or data distribution [19, 55].
7.3 GP for  Multi-class Classification
Among the EC algorithms, GP is one of the few algorithms that can perform classification directly (without relying on any external classification algorithm). For binary classification problems, GP can use a single threshold (usually 0) to distinguish between the two classes. However, real-world applications usually have more than two classes, also known as multi-class classification problems, which are more challenging for GP to address. This section discusses the basic ideas used to extend GP for multi-class classification.
7.3.1 Program Class Translation Based on  Single Evolved Tree-Based Programs
Since each individual in standard GP is typically represented by a single tree, such representation with a single threshold is not readily applied to classify more than two classes. This subsection introduces two basic but powerful approaches that allow a single tree-based program to cope with multi-class classification. The first approach is to have multiple thresholds, also known as boundary translations. The second approach is to let each tree program represent a set of class distributions, also known as probability/statistical GP
7.3.1.1 Static Versus Dynamic Boundary Translations
One way to extend GP for n-class classification is to use threshold values, i.e., {}, to form nintervals corresponding to nclasses. An example is given in Fig.  7.9, where the interval (-] corresponds to Class 1, the interval (] corresponds to Class 2and so on. In other words, the threshold values are the boundaries between different classes. These threshold values can be either staticallypre-defined or dynamicallyupdated during the evolutionary process [65].
Fig. 7.9
Static boundaries of nclasses
In the static approach, the set of thresholds are manually pre-defined. However, it is hard to pre-define the optimal threshold values. Furthermore, the static approach enforces a class order, for example the distance between Class 1and Class 2is much farther than the distance between Class1and Class 3, which might not be true in the dataset. An early version of this static method was to use an “equal” interval with a single parameter/threshold value to define different classes, which was used for multi-class object detection [128].
An alternative approach is to allow the thresholds to be updated during the evolutionary process, so the class boundaries are adjusted to better suit a certain classification problem. Zhang and Smart [130] proposed two methods for dynamic threshold values based on the GP population. The first method is regarded as centred dynamic class boundary determination with an example given in Fig.  7.10. The first step is to calculate the output values of all the GP programs on all the training instances. Then, for each class, its class centre is obtained by averaging the output values of all the instances belonging to the target class. The threshold value or class boundary between every two classes is the middle point of the two adjacent class centres.
Fig. 7.10
Centred dynamic class boundaries
The second method is regarded as slotted dynamic class boundary determination, with an example given in Fig.  7.11. The first step is to split a closed range into a certain number of slots and the task is to determine which class a slot belongs to. It is suggested that the range [−25, 25] with 200 slots would be sufficient if the input features are scaled into [−1, 1]. After splitting, the output values are obtained by applying all the GP programs to the training instances, where each output value falls in a certain slot. For each slot, the number of training instances in each class is recorded, and the majority class (i.e., the class with the largest number of instances in the slot) is assigned to the slot. If there is no training instance in a slot, the slot will be assigned to the class of the nearest neighbouring slot.
Fig. 7.11
Slotted dynamic class boundaries
The results indicate that the static approach can perform well on relatively easy classification problems where the class labels are ordinal. The dynamic approach achieves much better performance when the class labels are nominal (unordered). Among the two dynamic methods, the slotted dynamic class boundary determination is recommended for a complicated classification problem [130].
7.3.1.2 Probability/Statistical GP
Although the class boundary approaches are intuitive, they usually require certain prior domain knowledge. To address the issue, Zhang and Smart [131] integrate the probability theory and GP to build classification rules. Instead of determining class boundaries, the idea is to assume that the distribution of each class follows a Gaussian distribution, which is defined by a mean and a standard deviation. The two values can be obtained by calculating the mean and standard deviations of the output values when applying GP programs to the training instances from a target class.
In probability GP, each GP program can form a set of class distributions for all the classes and the class distribution can be presented as a normal curve. Figure  7.12shows two bell curves for two classes. In the ideal case, the GP program should classify all the training instances correctly, i.e., the two curves should be clearly separated as in Fig.  7.12c. Thus, the area of the overlapping region can measure the fitness of a GP program. The smaller the area, the better the program. For multi-class classification, the fitness measure is the sum of the overlapping region between every two classes.
Fig. 7.12
Examples of the overlap between class distributions. aWorse fitness with large overlap; bBetter fitness with small overlap; cBest fitness with no overlap
7.3.2 Strongly Typed GP for  Multi-class Classification
Standard GP usually works on one type of data while complex classification problems may involve multiple types of data. To address this issue, Montana [70] proposed an enhanced GP version called Strongly Typed GP (STGP). For terminal nodes, STGP specifies their data types such as Float, Integeror Array. For internal (function) nodes, STGP also species their inputand outputdata types. Accordingly, the input data type of an internal node has to match the output data types of its children nodes. STGP has been applied to complex classification problems that require different function types, such as cancerdiagnosis[56], credit control [14], software quality classification [49, 50] and image classification[10, 12, 58].
Fig. 7.13
Strongly Typed GP (STGP) for image classificationwith multiple layers [9, 11]. Different layers have different functionalities and are shown in different colours
To provide intuitive explanation, we take the image classificationtask as an illustrative example. An image content is represented by its pixel values, i.e., low-level features. However, it has been known that using more informative high-level features, which are functions of low-level features, usually results in a better classification performance. The process of extracting high-level features from an image is called feature extraction. Traditional image classificationapproaches perform feature extraction and classification as two separate steps, which “ignores” the complement between the extracted features and the classification algorithm. In contrast, STGP can perform feature extraction and classification simultaneously within in a single evolved GP program. A typical work was proposed by Bi et al. [9, 11] where the idea is to have various layers, i.e., multi-layer GP (MLGP), with different functionalities. In this work, a GP program has five main layers as showed in Fig.  7.13. The first (bottom) layer is an input layer with four possible types of each terminal: Imagerepresents an input image, (X, Y) are the top left of the region, Sizeis the size of a square region, and (Width, Height) are the width and height of a rectangle region. The second layer is to perform region detection with two possible operators: Region_Sfor a square region and Region_Rfor a rectangle region. While Region_Srequires a Sizeinput, Region_Rrequires (Width, Height). The third layer is to perform feature extraction which uses different traditional feature extraction methods such as Sobel, LBP, G_Stdand Gau1to extract features from the detected regions. The fourth layer is to perform feature construction, which builds high-level features based on the features extracted from the feature extraction layer. The fifth (top) layer performs classification based on the output of the fourth layer.
The results show that STGP not only achieves better classification than non-GP approaches but also generates interpretable features. Figure  7.14shows an evolved GP program achieving 100% accuracy on a face classification problem. The left side of the program extracts features from a rectangle area between the two eyes of a face. The right side of the program extracts features from a rectangle region covering the lower left side of the face including the left mouth part of a face. Particularly, the surprised faces have wider mouth and bigger (whiter) eyes than the happy faces.
Fig. 7.14
An explainable tree evolved by MLGP for the JAFFE dataset [9]
7.3.3 Multi-tree GP for  Multi-class Classification
Besides extending single-tree GP for multi-class classification, one can also apply multi-tree GP in which each GP individual is a set of trees [72, 118]. Particularly, for a n-class classification problem, each individual consists of ntrees: {}. The question is how to determine the instance’s class based on noutputs of the ntrees. The most straightforward approach is to directly compare the noutputs, then assign the class with the largest output to the instance, also known as the “winner-takes-all” approach [29].
Another approach is to use the output of to predict whether an instance belongs to the ith class () or not, which is a binary classification problem. Formally, for an instance x:
Thus, multi-tree GP decomposes a n-class classification problem into nbinary classification problems which can be solved simultaneously by ntrees with a single threshold value of 0. Figure  7.15represents an example of multi-tree GP for multi-class classification.
Fig. 7.15
Multi-tree GP for a classification problem containing 3 classes
Although the multi-tree representation fits multi-class classification naturally, the standard classification accuracy cannot be applied as in single-tree representation. For example, in a 3-class classification problem, the output of an example in the firstclass should be [1, 0, 0]. However, a 3-tree individual may output [1, 0.9, 0] which can then use the “winner-takes-all” approach to classification. Thus, we need to count how many instances are correctly classified by each treein a GP individual. In general, multi-tree representation usually achieves better classification performance than single-tree representation. However, multi-tree representation requires a careful design of evolutionary operators, especially when there are a large number of classes [102].
7.3.4 Linear GP for  Multi-class Classification
Linear Genetic Programming (LGP)is a variant of GP, where each individual is a linear sequence of instructions [15]. The instructions are operated on a number of variables, called registers. Particularly, LGP uses original features as input registersto calculate one or more output registers. When applies to an n-class classification problem, LGP can build noutput registers {}, where the register corresponds to the ith class. Given an instance, an LGP program outputs nvalues from the noutput registers. The class with the highest value is assigned to the instance [25, 30]. An example of LGP for multi-class classification is given in Fig.  7.16. The example shows that LGP can co-evolve nsub-programs (or registers) for nclasses. Furthermore, a register can be re-used to define other registers, which can reduce the number of original features in comparison with using the tree-based representation [30]. Besides defining the set of registers, users also define an instruction setfor LGP, which consists of two main instruction types: operations(such as arithmetic/Boolean operations) and conditional branches(i.e., ifstatements).
Fig. 7.16
An example of applying LGP to a classification problem having 4 classes and 7 original features (). The four registers and correspond to the 4 class labels and . The left sub-figure is the program evolved by LGP. The right sub-figure is the directed acyclic graph representation of the evolved program [30]
It can be seen that LGP (phenotype, graphs) and Artificial Neural Networks (ANNs) have quite similar structure. In fact, they can be seen as alternatives and perhaps competitive techniques to address classification problems. Brameier and Banzhaf [15] compared the two approaches by conducting experiments on medical datasets. The results showed that LGP could achieve comparative classification performance even though it was not tuned for each dataset unlike ANNs. In terms of computation time, LGP was much more efficient than ANNs. In contrast to ANNs, LGP was shown to build more interpretable modelsfrom which the extracted knowledge can provide more insightinto the medical diagnosis
7.4 GP for  Classification with  Missing Data
In classification, missing datais a common issue where many feature values are unknown due to issues like mechanical failures or respondents’ refusal during the data collection process [61]. Most existing classification algorithms require complete data, which makes them unusable for incomplete or missing data. Note, LCSstreat missing values as a “don’t care” so can simply match the remaining input to continue classification. Generally, there are two main approaches dealing with missing values. The first approach is to impute missing values according to available values. The imputed values are often considered functions of the available values. After the imputationstep, standard classification algorithms can be applied to the obtained (complete) data. The second approach is to build a classification algorithm that can directly deal with missing values. The previous sections show that GP has a great potential in building functions and performing classification, which makes GP an excellent choice to address missing data[104, 106]. The two following subsections show how GP can be used as a data imputationapproach and a direct classification algorithm for incomplete dataclassification, respectively.
7.4.1 GP-Based Imputation
In a classification problem, there is typically a certain kind of relationship between features, and thus it is expected that a feature can be represented as a function of other features. The idea of GP-based imputationis to build functions that can map from complete features to incomplete features. The built functions can be used to fill all the missing values. Given a training set X, the overall steps of imputing the dth feature are as follows.
Split the training set Xinto two subsets: contains all the instances that the dth feature has complete values and contains all the instances that the dth feature value is missing.
Use GP on to build a mapping function from other features to the dth feature. This process is very similar to the process of applying GP for feature construction (or symbolic regression).
Apply the mapping function to impute the missing values of the dth feature in 
Form a complete training set by combine the subset and the subset with imputed values.
Once all the missing values are imputed, standard classification algorithms can be trained on the completed data. The results show that GP-based imputationcan achieve better classification performance than traditional imputationmethods [103]. Since the mappingfunction of each incomplete feature is built based on the complete data , it is possible that some features in the mappingfunction may have incomplete values in . In addition, different instances in may have different incomplete features. To address the above situation, multiple mappingfunctions are built for each incomplete feature. The goal is to provide a wide range of functions among which the most suitable mapping function can be selected for each instance in . The multiple mappingfunctions can be obtained by running GP multiple times for each feature [103, 105].
7.4.2 GP-Based Classifier
Standard GP cannot be applied to classify the incomplete datasince its evolved function requires all the feature values to calculate the function’s output. Tran et al. [104] propose Interval GP (IGP)thatcan handle the missing datadirectly. The idea is instead of using a specific feature value of each feature, IGP uses the value interval of each feature as its input. The main consideration is that if a feature value is missing, GP can use the feature’s interval directlyestimated from the training set. The use of the feature intervals eliminates the need of imputing missing values that is usually more time-consuming.
Given two features xand y, their corresponding intervals and , IGP can combine the two features by using the following operators:
Since the input of IGP is feature intervals, its output is also an interval . The middle point of the interval is used as the final output of IGP. The results show that IGP can achieve better classification performance than classifiers that can directly handle missing datasuch as CART and C4.5[133].
7.5 GP for  Classification with  Unbalanced Data
Mostclassification algorithms assume an even data distribution between different classes. However, such assumption does not hold in many real-world applications which often have unbalanced data. For example, in cancerdiagnosis, most patients are non-cancerous (called the majorityclass), while there are very few cancerous patients (called the minorityclass). If all the instances are treated equally, the trained classifier will be biasto the majority class, i.e., instances from the majority class are more likely to be classified correctly than instances from the minority class. However, in many classification problems, correctly classifying the minority class is essential. For instance, wrongly classifying non-cancerous patients (false positives) may lead to a few additional clinical tests, while wrongly classifying cancerous patients (false negatives) may cost their lives. Therefore, building classifiers with high accuracy on all classes is an important task in unbalanced data. This section shows how GP can be used to address the unbalanced dataproblem.
7.5.1 Fitness Measures
A common fitness measure for GP-based classification algorithms is the overall classification accuracy which can be calculated by Eq. (7.3).
(7.3)
where the minority class is the positiveclass, are the number of true positives, true negatives, false positives and false negatives, respectively.
Clearly, the overall classification accuracy ignores the fact that the positive class has a much smaller number of instances. The classification algorithm will focus more on classifying the negative instances (from the majority class) correctly, which easily yields a higher overall classification accuracy. Thus, Eq. (7.3) considers the classification performance on the minority class and the classification performance on the majority class equally important. To address the issue, we can use weighted average classification accuracywhich uses a weighting factor wto control the trade-off between the performance of the two classes [86]. The weighted average classification accuracy is shown in Eq. (7.4).
(7.4)
where . If w= 0.5, the two classes are equally important. If w> 0.5, the weighted average accuracy focuses more on the minority class. The key question is how to select w, which is typically problem-dependent.
Note that the weighted average accuracy cannot differentiate two different GP classifiers that have the same TP and TN rates. The weighted average accuracy can be further improved by considering the GP output in its fitness measure. Firstly, two target outputs for the majority class and the minority class are defined. The smaller distance between the classifier’s outputs and the desired outputs, the better the classifier [7]. The fitness function follows the idea of mean square error and its formula is shown in Eq. (7.5).
(7.5)
where
and are the numbers of instances in the positive (minority) class and the negative (majority) class. and are the outputs of a GP classifier being evaluated on the ith instance of the positive class and negative class. and are the two desired outputs of the two classes. It has been shown that AveMseachieves significantly better performance than AveAcc
AUC(Area Under the Curve) is another measure that can be used with GP to handle unbalanced data. However, AUCis computationally intensive since it requires to calculate TP and FP many times to form an accurate rendition of the curve. GP can use a direct estimator of the AUC metric [124], which can be seen in Eq. (7.6).
(7.6)
where
and are two output values of the classifier Pwhen the positive instance iand the negative instance jare used as the classifier’s inputs. In general, AUC-based fitness measures usually result in a good performance on unbalanced databut also cause the long training times.
Geometric mean (G_Mean) is another measure for unbalanced datasets which can be calculated by Eq. (7.7).
(7.7)
The advantage of G_Meanis that it does not require to specify weights to combine the minority accuracy as the majority accuracy as in Eq. (7.4).
7.5.2 Cost-Sensitive Methods
Table 7.1
A cost matrix for unbalanced data
Minority class
Majority class
(Class 0)
(Class 1)
Predicted minority class (Class 0)
= 0
= 1
Predicted majority class (Class 1)
> 1
= 0
Cost-sensitive learning [135] is another approach to handling unbalanced data. The idea is to treat different misclassification output differently. For example, the cost of predicting a cancerous patient as a non-cancerous patient is much more than the cost of predicting a non-cancerous patient as a cancerous patient. Thus, cost-sensitive learning can enhance the classification performance on the minority class. In cost-sensitive learning, the cost information is presented by a cost matrix as in Table  7.1and are the cost of correct predictions, so they are set to 0. is the cost of wrongly predicting an instance from the majority class, and it is usually set to 1. is the cost of wrongly predicting an instance from the minority class, and it is usually greater than 1, so GP can focus more on avoiding wrong prediction in the minority class. If prior knowledge is not known, is usually set to the ratio between the number of instances in the majority class and the number of instances in the minority class. Such a ratio is guaranteed to be greater than 1. GP-based classification algorithms are then trained to minimise the total cost of all the training instances. It has been shown that GP with cost-sensitive learning achieves better performance than GP with the overall classification accuracy [88]. Recently, GP has also been used to learn the cost matrix, which achieves better performance than the manually defined cost matrix [87].
7.5.3 Ensemble Learning
The goal of classification is to achieve good classification performance on both the minority and majority classes. However, Bhowan et al. [8] showed that the minority accuracy and the majority accuracy are usually in conflict, i.e., a classifier with a high minority accuracy usually has a low majority accuracy and vice versa. Thus, building a single classifier might not be sufficient, which requires to build a set or ensemble of classifiers. However, the set of classifiers needs to be diverse. Bhowan et al. [8] proposed to use Multi-objective GP (MOGP) to build the set of classifiers. In MOGP, a classifier dominates the other classifier if is better than on both classes. The goal of MOGP is to find all the classifiers that are not dominated by any other classifiers, also known as Pareto-optimal classifiers. During its evolutionary process, MOGP maintains an archiveset containing all the non-dominated classifiers discovered so far. Once the stopping criterion is met, MOGP returns its archiveset as the ensemble of classifiers, which is shown to achieve good performance on the unbalanced dataset. An advantage of MOGP is its ability to build a diverse ensemble of classifiers in a single run.
7.6 Genetic Programming for  Explainable/Interpretable Classifiers
Explainabilityis an essential criterion in building classification models, especially in critical applications such as self-driving cars, laws and healthcare. The focus of explainable classifiers is to improve the transparencyand trustworthinessof classification models. Given its symbolic representation, GP has a great potential to build understandable classifiers. In general, GP-based methods canbe divided into two main categories: model-based interpretabilitymethods which aim to build interpretable modelsduring the training process and post-hoc interpretabilitymethods which aim to build a more interpretable modelto approximate a trained model. A recent comprehensive survey on GP for learning interpretable modelscan be seen from [67].
7.6.1 Model-Based GP for  Classification
Although GP offers potential explainability, most existing GP-based classifiers are still large and complex due to the depth and breadth of the learnt GP-tree, which make them less interpretable. One way to improve the interpretabilityof GP is to reduce its tree size. An example is to dynamically control the tree depth limit [95] based on the best tree in the population. During the evolutionary process, any offspring, which has a larger depth than the limit, is removed. The only exception is when the offspring is better than the current best tree. The tree size can also be controlled by adding penalty terms to the GP’s fitness function that penalises individuals with large trees [26, 43, 127]. Another alternative is to develop genetic operators that generate small offspring. For example, crossover is allowed to swap sub-trees with similar sizes only, which prevents crossover from generating offspring that is much larger than its parents [53, 90]. Similarly, mutation is allowed to replace a sub-tree with another sub-tree that is not much larger than the replaced one [54]. The model size can also be embedded into the selection process where the small trees are given higher priority. For instance, Wang et al. [117] maintain an archiveset of small and high-performing trees. Selecting parents from the archive set will generate small offspring. Zhang and Wong [132] proposed to simplify the GP programs during the evolutionary process, which is known as online program simplification. The idea is to reduce a program’s size by removing the redundancy in the program, for example, a sub-tree (a-0)is replaced by a single node a
