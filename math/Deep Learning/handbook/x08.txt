Besides the architecture, there are several other aspects of machine learning system design that need to be configured properly for the system to perform well. Those include learning hyperparameters (such as the learning rate), activation functions, loss functions, data sampling and augmentation, and the methods themselves. Approaches similar to those used in NAScan be applied to them; however, the evolutionary approach has an advantage in that it is the most versatile: It can be applied to graphs, vectors of continuous and discrete parameters, and configuration choices. This ability is particularly useful as new architectures are developed. For instance, at this writing, work has barely begun on optimizing designs of transformer [90] or diffusion [75] architectures. They have elements such as attention modules, spatial embeddings, and noise transformations that are different from prior architectures, yet may be parameterized and evolution applied to optimize their implementation. Most importantly, evolution can be used to optimize many different aspects of the design at the same time, discovering and taking advantage of synergies between them. Several such approaches are reviewed in this section.
2.4.2.1 Loss Functions
Perhaps the most fundamental isthe design of a good loss function. The mean-squared-error (MSE) loss has been used for a long time, and more recently, the cross-entropy(CE) loss has become popular, especially in classification tasks. Both of those assign minimal loss to outputs that are close to correct, and superlinearly larger losses to outputs further away from correct values. They make sense intuitively and work reliably, so much so that alternatives are not usually even considered.
However, it turns out that it is possible to improve upon them, in a surprising way that would have been difficult to discover if evolution had not done it for us [25, 27]. If outputs that are extremely close to correct are penalized with a larger loss, the system learns to avoid such extreme outputs—which minimizes overfitting (Fig. 2.8a). Such loss functions, called Baikal lossfor their shape, lead to automatic regularization. Regularization in turn leads to more accurate performance on unseen examples, especially in domains where the amount of available data is limited, as is the case in many real-world applications.
Fig. 2.8
Regularizationand Robustness with Evolved Loss Functions.aThestandard loss function, such as log loss (or cross-entropy)has a high loss for outputs that are far from correct (1.0 in this case) and a low loss otherwise. In contrast, evolutionary optimization of loss functions through GLO/TaylorGLO [25, 27] discovered a new principle: When the output is very close to the correct one, a high loss is incurred. This principle, termed Baikal lossfor its shape, discourages overfitting, thus regularizing the network automatically, leading to better generalization. Such a loss is effective but counterintuitive, and thus unlikely to be discovered by human designers. bThe Baikal lossalso makes the network performance more robust. This effect can be quantified by perturbing the network weights. With Baikal loss, the network’s performance is less affected than with cross-entropyloss. This effect can be further magnified by making robustness against adversarial inputs an explicit second objective in evolution. Thus, loss-function optimization can be used to improve not only regularizationbut also robustness.
Figures from [25, 26]
Baikal losswas originally discovered with a classic genetic programming approach where the function was represented as a tree of mathematical operations [25]. The structure of the tree was evolved with genetic algorithms and the coefficients in the nodes with CMA-ES[29]. This approach is general and creative in that it can be used to explore a large search space of diverse functions. However, many of those functions do not work well and often are not even stable. In the follow-up TaylorGLO method[27], the functions were represented instead as third-order Taylor polynomials. Such functions are continuous and can be directly optimized with CMA-ES, making the search more effective.
Regularizationin general is an important aspect of neural network design, there are many techniques available, such as dropout, weight decay, and label smoothing [30, 77, 87], but how they work is not well understood. Loss-function optimization, however, can be understood theoretically, and thus provides a starting point to understanding regularizationin general [26]. It can be described as a balance of two processes, one a pull toward the training targets, and another a push away from overfitting. The theory leads to a practical condition for guiding the search toward trainable functions.
Note that Baikal lossis a general principle; evolutionary optimization was crucial in discovering it but it can now be used on its own in deep learning. It is still possible to customize it for each task and architecture, and even small modifications to the standard Baikal shape may make a difference. Optimization may also have a significant effect on various learning challenges, for instance, when there is not much training data [24], or when the labels are particularly noisy [20]. It may also be possible to modify the loss functionduring the course of learning, for instance by emphasizing regularizationin the beginning and precision toward the end (similarly to activation functions; Sect. 2.4.2.2).
It turns out that loss functionsthat regularize also make networks more robust, and this effect can be further enhanced by including an explicit robustness goal in evolution (Fig. 2.8b). One way to create such a goal is to evaluate performance separately wrt adversarial examples. This result in turn suggests that loss-function optimization could be an effective approach to creating machine learning systems that are robust against adversarial attacks.
Loss-function optimization can also play a major role in systems where multiple loss functionsinteract, such as Generative Adversarial Networks(GANs; [23]). GANs include three different losses: discriminative loss for real examples and for fake examples, and the generative loss (for fake examples). It is difficult to get them right, and many proposals exist, including those in minimax, nonsaturating, Wasserstein, and least-squares GANs [3, 28, 50]. Training often fails, resulting, e.g., in mode collapse. However, the three losses can be evolved simultaneously, using performance and reliabilityas fitness. In one such experiment on generating building facade images given the overall design as a condition, the TaylorGLO approach was found to result in better structural similarityand perceptual distance than the Wasserstein loss [23]. Although this result is preliminary, it suggests that evolutionary loss-function optimization may make more complex learning systems possible in the future.
2.4.2.2 Activation Functions
Early on in the 1980s and 1990s, sigmoids (and tanh) were used almost exclusively as activation functionsfor neural networks. They had the intuitively the right behavior as neural models, limiting activation between the minimum and maximum values, a simple derivativethat made backpropagation convenient, and a theorem suggesting that universal computing could be based on such networks [12, 36]. There were indications, however, that other activation functionsmight work better in many cases. Gaussians achieved universal computing with one less layer and were found powerful in radial basis function networks [59]. Ridge activations were also found to provide similar capabilities [46].
However, with the advent of deep learning, an important discovery was made: Activation functionactually made a big difference wrt vanishing gradients. In particular, rectified linear units (ReLUs), turned out important in scaling up deep learning networks [56]. The linearly increasing region does not saturate activation or gradients, resulting in less signal loss. Moreover, it turned out that in many cases ReLU could be improved by adding a small differentiable dip at the boundary between the two regions, in a function called Swish [62]. This result suggested that there may be an opportunity to optimize activation functions, in general, and for specific architectures and tasks.
Like with loss functions, there is a straightforward opportunity in evolving functions through genetic programming [6]. Similarly to loss functions, such an approach can be creative, but also results in many functions that make the network unstable. A more practical approach is to limit the search space to, e.g., computation graphs of two levels, with a focused set of operators, that are more likely to result in useful functions. This approach was taken, e.g., in thePangaea system [7]. Given a list of 27 unary and 7 binary operators, 2 basic two-level computation graph structures, and 4 mutation operators, evolution can search a space of over 10 trillion activation functions
However, finding an effective function is only part of the challenge. The function also needs to be parameterized so that it performs as well as possible. While coefficients multiplying each operator can be evolved together with the structure, it turns out that such fine tuning can be done more efficiently through gradient descent. In other words, in Pangaea evolution and gradient descent work synergetically: evolution discovers the general structure of the function, and gradient descent finds its optimal instantiation.
The method is powerful in two ways: it finds general functions that perform better than previous functions (such as ReLU, SeLU, Swish, etc.) across architectures (such as All-CNN, Wide ResNet, Resnet, and Preactivation Resnet) and tasks (such as CIFAR-10, CIFAR-100). However, it is most powerful in discovering activation functionsthat are specialized in architecture and task, apparently taking advantage of the special requirements in each such context.
Furthermore, performance can be further improved by allowing different functions at different parts of the network, and at different times throughout training (Fig.  2.9). The optimal designs change continuously over time and space. Different activation functionsare useful early in training when the network learns rapidly and late in training when fine-tuning is needed; similarly, more nonlinear functions are discovered for later layers, possibly reflecting the need to form a regularized embedding early, and make classification decisions later.
Fig. 2.9
Activation Functions Discoveredover Space and Time.Pangaea [7] combines evolution of function structure synergetically with gradient descent of its parameters. It is possible to discover general functions, but the approach is most powerful in customizing them to a particular architecture and task. Moreover, the functions change systematically over learning time as well as through different depths of layers, presumably starting with coarse learning and regularizationand transforming into fine-tuning and classification. These results suggest a possible duality with weight learning and a possible synergy for the future.
Figure from [7]
The Pangaea results suggest an intriguing duality: While neural network learning is mostly based on adapting a large number of parameters (i.e., weights), perhaps a similar effect might be achieved by adapting the activation functionsover space and time? Perhaps the two mechanisms could be used synergetically? Evolution of the activation functionstructure provides the foundation for this approach, which still needs to be developed fully.
2.4.2.3 Data Use and Augmentation
Another important opportunity for evolutionary optimization of supervised learning systems is to optimize the training data. For instance, it may be possible to form embeddings of the training samples through an autoencoder, and then form a strategy for utilizing different kinds of samples optimally through time [24]. In this manner, evolution could discover ways for balancing an imbalanced dataset or designing curricular learning from simple to more complex examples. Especially in domains where not a lot of labeled samples are available, such techniques could result in significant improvements. It may also be possible to extend the methods to utilize multiple datasets optimally over time in a multitask setting.
Another possibility is to evolve methods for augmenting the available data automatically through various transformations. Different datasets may benefit from different transformations, and it is not always obvious ahead of time how they should be designed. For instance, in an application to develop models for estimating the age of a person from an image of their face, evolution was used to decide vertical and horizontal shift and cutout, as well as a direction of flip operations, angle of rotation, degree of zoom, and extent of shear [53]. Unexpectedly, it chose to do vertical flips only—which made little sense for faces, until it was found that the input images had been rotated 90 degrees. It also discovered a combination of shift operations that allowed it to obfuscate the forehead and chin, which would otherwise be easy areas for the model to overfit.
A particularly interesting use for evolved data augmentationis to optimize not only the accuracy of the resulting models but also to mitigate biasand fairnessissues with the data. As long as these dimensions can be measured [72], they can be made part of the fitness, or separate objectives in a multiobjective setting. Operations then need to be designed to increase variance across variables that might otherwise lead to biasthrough overfitting—for instance gender, ethnicity, and socioeconomic status, depending on the application. While evolutionary data augmentationis still new, this area seems like a differentiated and compelling opportunity for it.
2.4.2.4 Learning Methods
An interesting extension of NASis to evolve the learning system not from high-level elements, but from the basic algorithmic building blocks (mathematical operations, data management, and ways to combine them)—in other words, by evolving code for supervised machine learning. In this manner, evolution can be more creative in discovering good methods, with fewer biases from the human experimenters.
The AutoML-Zerosystem [65] is a step toward this goal. Given an address space for scalars, vectors, and matrices of floats, it evolves setup, predicts, and learns methods composed of over 50 basic mathematical operations. Evolution is implemented as a linear GP and consists of inserting and removing instructions and randomizing instructions and addresses. Evaluation consists of computing predictions over unseen examples.
Starting from empty programs, AutoML-Zerofirst discovered linear models, followed by gradient descent, and eventually several extensions known in the literature, such as noisy inputs, gradient normalization, and multiplicative interactions (Fig.  2.10). When given small datasets, it discovers regularizationmethods similar to dropout; when given a few training steps, it discovers learning-rate decay.
Fig. 2.10
Evolutionary Discovery of Learning Methods.In AutoML-Zero[65], sequences of instructions for setup, prediction, and learning are evolved through mutation-based regularized search. AutoML-Zerofirst discovered simple methods such as linear models, then several known extensions such as ReLU and gradient normalization, and eventually more sophisticated techniques such as multiplicative interactions. The approach could potentially be useful in particular in customizing learning methods to different domains and constraints.
Figure from [65]
Thus, the preliminary experiments with AutoML-Zerosuggest that evolutionary search can be a powerful tool in discovering entire learning algorithms. As in many metalearning approaches, the main power may be in customizing these methods to particular domains and constraints. A crucial aspect will be to guide the evolution within the enormous search space toward meaningful solutions, without hampering its ability to create, again a challenge shared with most of metalearning.
2.4.2.5 Synergies
Perhaps the most important future direction in evolutionary metalearning is to discover and utilize synergies between the different aspects of the learning system design. For instance, the best performance was reached by optimization activation functionsfor the specific architecture; it might be possible to optimize the architecture simultaneously to emphasize this effect.
Simply running evolution on all these design aspects simultaneously is unlikely to work; the search space would be prohibitively large. Similarly, adding more outer loops to the existing process (where supervised learning is the inner loop and metalearning is the outer loop) is likely prohibitive as well. However, it might be possible to alternate evolution of different aspects. Better yet, techniques from bilevel (or multilevel) optimization could be useful—the idea is to avoid full inner–outer loop structure, but instead use, e.g., surrogate modelsto evaluate outer loop innovations [45, 74].
A practical approach is to simply add constraints, and search in a smaller space. A first such step was already taken in the EPBT system [43], which combines hyperparameter tuning, loss-function optimization, and population-based training (PBT) into a single loop. That is, hyperparameters and loss functionsare evolved at the same time as the networks are being trained. Hyperparameter tuning is limited to those that do not change the structure of the networks (e.g., learning rate schedules) so that they can be continuously trained, even when the hyperparameters change. Similarly, loss-function optimization is limited to TaylorGLO coefficients [43] that can be changed while training is going on. Even so, the simultaneous evolution and learning was deceptive, and needed to be augmented with two mechanisms: quality-diversityheuristic for managing the population and knowledge distillation to prevent overfitting. The resulting methodnot only worked well on optimizing ResNet and WideResnet architectures in CIFAR-10and SVHN but also illustrated the challenges in taking advantage of synergies of metalearning methods.
Similarly, promising results were obtained in an experiment that compared human design with evolutionary metalearning [53]. Using the same datasets and initial model architectures, similar computational resources, and similar development time, a teamof data scientists and an evolutionary metalearning approach developed models for age estimation in facial images (Fig. 2.11). The evolutionary metalearning approach, LEAF-ENN, included optimization of loss functions(limited to linear combinations of MSE and CE), learning hyperparameters, architecture hyperparameters, and data augmentationmethods. Evolution discovered several useful principles that the data scientists were not aware of focusing data augmentationto regions that mattered most, and utilizing flips only horizontally across the face; utilizing different loss functionsat different times during learning; relying mostly on the output level blocks of the base models. With both datasets, the eventual accuracy of the metalearned models was significantly better than that of the models developed by the data scientists. This result demonstrates the main value of evolutionary metalearning: it can result in models that are optimized beyond human ability to do so.
Fig. 2.11
Utilizing Metalearning Synergies to Beat Human Designers.In two datasets (D0 and D1) for age estimation from facial images, LEAF-ENN evolutionary metalearning [53] was able to discover models that performed better than those simultaneously designed by human experts. The humans optimized the ResNet-50 architecture for D0 and EfficientNet-B8 for D1. The evolutionary runs progressed in stages: In D0, ResNet-50 (S0) was expanded to Densenet 169 (S1); in D1, DenseNet-169 (S0) was expanded to DenseNet-201 (S1) and trained longer (S2), then expanded to EfficientNet-B6 (S3), and ensembling (S4). At the same time, evolution-optimized learning and architecture hyperparameters, data-augmentationmethods, and combinations of loss functions. The approach discovers and utilizes synergies between design aspects that are difficult for humans to utilize. The final accuracy, MSE of 2.19 years, is better than typical human accuracy in age estimation (3–4 years).
Figure from [53]
2.5 Conclusion
Although much of evolutionary machine learning has focused on discovering optimal designs and behavior, it is a powerful approach to supervised learning as well. While gradient-based supervised learning (i.e., deep learning) has benefited from massive scale-up, three opportunities for evolutionary supervised learning have emerged as well. In particular, in domains where such a scale-up is not possible, it can be useful in two ways. First, it can expand the scope of supervised learning to a more general set of design goals other than simply accuracy. Second, it can be applied to solution structures that are explainable. Third, in domains where deep learning is applicable, it can be used to optimize the design of the most complex such architectures, thus improving upon the state of the art. Two most interesting research directions emerge: how to extend the generality and explainabilityto larger domains, and how to take advantage of synergies between multiple aspects of machine learning system design. Such work most likely requires developing a better understanding of how the search can be guided to desired directions without limiting the creativityof the approach.
References
1.
Aharonov-Barki, R., Beker, T., Ruppin, E.: Emergence of memory-driven command neurons in evolved artificial agents. Neural Comput. 13, 691–716 (2001)CrossrefzbMATH
2.
Aitkenhead, M.J.: A co-evolving decision tree classification method. Expert Syst. Appl. 34, 19–25 (2008)Crossref
3.
Arjovsky, M., Chintala, S., Bottou, L.: Wasserstein generative adversarial networks. In: Precup, D., Teh, Y.W. (eds.), Proceedings of the 34th International Conference on Machine Learning, vol.  70, pp. 214–223 (2017)
4.
Barros, R.C., Basgalupp, M.P., De Carvalho, A.C., Freitas, A.A.: A survey of evolutionary algorithms for decision-tree induction. IEEE Trans. Syst., Man, Cybern., Part C (Appl. Rev.) 42, 291–312 (2012)
5.
Bi, Y., Xue, B., Zhang, M.: Genetic programming-based evolutionary deep learning for data-efficient image classification. IEEE Trans. Evolut. Comput. (2022). https://​doi.​org/​10.​1109/​TEVC.​2022.​3214503
6.
Bingham, G., Macke, W., Miikkulainen, R.: Evolutionary optimization of deep learning activation functions. In: Proceedings of the Genetic and Evolutionary Computation Conference, pp. 289–296 (2020)
7.
Bingham, G., Miikkulainen, R.: Discovering parametric activation functions. Neural Netw. 148, 48–65 (2022)Crossref
8.
Breiman, L.: Random forests. Mach. Learn. 45, 5–32 (2001)CrossrefzbMATH
9.
Breiman, L., Friedman, J., Stone, C.J., Olshen, R.A.: Classification and Regression Trees. Chapman and Hall/CRC (1984)
10.
Butz, M.V., Lanzi, P.L., Wilson, S.W.: Function approximation with xcs: Hyperellipsoidal conditions, recursive least squares, and compaction. IEEE Trans. Evolut. Comput. 12, 355–376 (2008)Crossref
11.
Canatar, A., Bordelon, B., Pehlevan, C.: Spectral bias and task-model alignment explain generalization in kernel regression and infinitely wide neural networks. Nat. Commun. 12, 1914 (2021)Crossref
12.
Cybenko, G.: Approximation by superpositions of a sigmoidal function. Math. Control Signals Syst. 2, 303–314 (1989)MathSciNetCrossrefzbMATH
13.
Dai, E., Zhao, T., Zhu, H., Xu, J., Guo, Z., Liu, H., Tang, J., Wang, S.: A comprehensive survey on trustworthy graph neural networks: privacy, robustness, fairness, and explainability. arXiv:​2104.​05605, 2020
14.
De  Jong, K.: Learning with genetic algorithms: an overview. Mach. Learn. 3, 121–138 10 (1988)
15.
Deb, K., Myburgh, C.: A population-based fast algorithm for a billion-dimensional resource allocation problem with integer variables. Eur. J. Oper. Res. 261, 460–474 (2017)MathSciNetCrossrefzbMATH
16.
Dolotov, E., Zolotykh, N.Y.: Evolutionary algorithms for constructing an ensemble of decision trees (2020). arXiv:​2002.​00721
17.
Elsken, T., Metzen, J.H., Hutter, F.: Neural architecture search: a survey. J. Mach. Learn. Res. 20, 1–21 (2019)MathSciNetzbMATH
18.
Gaier, A., Ha, D.: Weight agnostic neural networks. In: Wallach, H.,  Larochelle, H.,  Beygelzimer, A.,  d’Alché-Buc, F.,  Fox, E.,  Garnett, R. (eds.), Advances in Neural Information Processing Systems 5364–5378 (2019)
19.
Ganon, Z., Keinan, A., Ruppin, E.: Evolutionary network minimization: adaptive implicit pruning of successful agents. In: Banzhaf, W., Ziegler, J., Christaller, T., Dittrich, P., Kim, J.T. (eds.) Advances in Artificial Life, pp. 319–327. Springer, Berlin (2003)Crossref
20.
Gao, B., Gouk, H., Hospedales, T.M.: Searching for robustness: loss learning for noisy classification tasks. IEEE/CVF International Conference on Computer Vision, pp. 6650–6659 (2021)
21.
Gomez, F., Miikkulainen, R.: Incremental evolution of complex general behavior. Adapt. Behav. 5, 317–342 (1997)Crossref
22.
Gomez, F., Schmidhuber, J., Miikkulainen, R. and Mitchell, M.: Accelerated neural evolution through cooperatively coevolved synapses. J. Mach. Learn. Res. 937–965 (2008)
23.
Gonzalez, S., Kant, M., Miikkulainen, R.: Evolving GAN formulations for higher quality image synthesis. In: Kozma, R., Alippi, C., Choe, Y., Morabito, F.C. (eds.) Artificial Intelligence in the Age of Neural Networks and Brain Computing, 2nd edn. Elsevier, New York (2023)
24.
Gonzalez, S., Landgraf, J., Miikkulainen, R.: Faster training by selecting samples using embeddings. In: Proceedings of the 2019 International Joint Conference on Neural Networks, pp. 1–7 (2019)
25.
Gonzalez, S., Miikkulainen, R.: Improved training speed, accuracy, and data utilization through loss function optimization. In: Proceedings of the 2020 IEEE Congress on Evolutionary Computation (CEC), pp. 1–8 (2020)
26.
Gonzalez, S., Miikkulainen, R.: Effective regularization through loss-function metalearning (2021). arXiv:​2010.​00788
27.
Gonzalez, S., Miikkulainen, R.: Optimizing loss functions through multivariate Taylor polynomial parameterization. In: Proceedings of the Genetic and Evolutionary Computation Conference, pp. 305–313 (2021)
