Hao, X., Chang, H., Zhang, D.: DLGA-PDE: Discovery of PDEs with incomplete candidate library via combination of deep learning and genetic algorithm. J. Comput. Phys. 418, 109584 (2020)MathSciNetzbMATH
120.
Hao, X., Zhang, D.: Robust discovery of partial differential equations in complex situations. Phys. Rev. Res. 3(3), 033270 (2021)
121.
Hao, X., Zhang, D., Zeng, J.: Deep-learning of parametric partial differential equations from sparse and noisy data. Phys. Fluids 33(3), 037132 (2021)
122.
Haoran, X., Ma, J., Tan, P., Chen, B., Zhen, W., Zhang, Y., Wang, H., Xuan, J., Ni, M.: Towards online optimisation of solid oxide fuel cell performance: combining deep learning with multi-physics simulation. Energy AI 1, 100003 (2020)
123.
Yaseen, Z.M., Afan, H.A., Tran, M.-T.: Beam-column joint shear prediction using hybridized deep learning neural network with genetic algorithm. In: IOP Conference Series: Earth and Environmental Science, vol. 143, p. 012025. IOP Publishing (2018)
124.
Yoon, E.J., Thorne, J.H., Park, C., Lee, D.K., Kim, K.S., Yoon, H., Seo, C., Lim, C.-H., Kim, H., Song, Y.-I.: Modeling spatial climate change landuse adaptation with multi-objective genetic algorithms to improve resilience for rice yield and species richness and to mitigate disaster risk. Environ. Res. Lett. 14(2), 024001 (2019)
125.
Yoshikawa, N., Terayama, K., Sumita, M., Homma, T., Oono, K., Tsuda, K.: Population-based de novo molecule generation, using grammatical evolution. Chem. Lett. 47(11), 1431–1434 (2018)
126.
Zhan, Z.-H., Li, J.-Y., Zhang, J.: Evolutionary deep learning: a survey. Neurocomputing 483, 42–58 (2022)
127.
Zhang, C.-X., Yuan, Y., Zhang, H.-W., Shuai, Y., Tan, H.-P.: Estimating stellar effective temperatures and detected angular parameters using stochastic particle swarm optimization. Res. Astron. Astrophys. 16(9), 008 (2016)
128.
Zhong, X., Velez, C., Acevedo, O.: Partial charges optimized by genetic algorithms for deep eutectic solvent simulations. J. Chem. Theory Comput. 17(5), 3078–3087 (2021)
129.
Zhou, Y., Zhou, N., Gong, L., Jiang, M.: Prediction of photovoltaic power output based on similar day analysis, genetic algorithm and extreme learning machine. Energy 204, 117894 (2020)
130.
Zuo, Y., Qin, M., Chen, C., Ye, W., Li, X., Luo, J., Ong, S.P.: Accelerating materials discovery with bayesian optimization and graph deep learning. Mater. Today 51, 126–135 (2021)©  The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.  2024
W. Banzhaf et al.(eds.)Handbook of Evolutionary Machine LearningGenetic and Evolutionary Computationhttps://doi.org/10.1007/978-981-99-3814-8_19
19.  Evolutionary Machine Learning in  Environmental Science
João  E.  Batista1and Sara  Silva1
(1)
LASIGE, Department of Informatics, Faculty of Sciences, University of Lisbon, 1749-016  Lisboa, Portugal
Sara  Silva
Email: sgsilva@fc.ul.pt
Abstract
This chapter reviews the use of Evolutionary Machine Learning  (EML) in environmental science. We cover the various steps of the machine learning pipeline, also addressing topics like model robustness, interpretability, and human-competitiveness. Environmental science is an interdisciplinary field mainly dedicated to climate change, natural resource management, conservation biology, and sustainability. We review applications such as forest monitoring, optimization of photovoltaic installations, improvement of traffic flow, and reduction of waste in animal farms, among others.
19.1 Introduction
This chapter covers Evolutionary Machine Learning  (EML) methods for data preparation, feature engineering, regression and classification, among other common tasks in environmental science. While going through the different steps of the machine learning pipeline, we also address the robustness, interpretability, and human-competitivenessof the models obtained with EML methods. We follow the definitions proposed in  [103], according to which an “interpretable” model can be understood by domain experts without requiring explanation algorithms, thanks to its size, operators, and number of used features. By opposition, some models are not inherently interpretable but can be “explained” using other algorithms.
EML adds flexibilityto machine learning algorithms that are already highly versatile. Therefore, it is not surprising that EML is being applied to an endless list of tasks in various scientific domains. In environmental science, however, EML is still underused, with many experts still preferring other methods like random forests and Artificial Neural Networks  (ANNs). Nowadays, ANNs are being used in several scientific domains but, although these models provide good results, they are not interpretable. EML-based models, on the other hand, are generally known for their potential interpretability[11].
Following the description by Miller et al.  [83], environmental science is a vast interdisciplinary field that integrates knowledge from several fields such as biology, chemistry, geology, geography, economics, political science, philosophy, and ethics. Its major goal is to understand how nature works, how we interact with the environment, and how to deal with environmental problems and live sustainably. Machine learning is increasingly recognized as a strong ally in tackling environmental issues  [101].
In this chapter, we review several examples of EML being successfully applied to different tasks in environmental science. We promote EML by surveying the application of methods based on Genetic Algorithms  (GA)  [52] for data cleaning; methods based on GA, Ant Colony Optimization  (ACO)  [57], Genetic Programming  (GP)  [96], Multi-Objective Evolutionary Algorithms  (MOEA)  [34], and Particle Swarm Optimization  (PSO)  [39] for feature selection; methods such as Evolutionary Feature Synthesis(EFS)  [7], Feature EngineeringAutomation Tool  (FEAT)  [65] andEvolutionary Polynomial Regression  (EPR)  [46], and methods based on GA and GP, for feature construction; methods based on ACO, PSO, and GA for other tasks, e.g., parameter tuning. We also promote environmental science by surveying an extensive range of applications, among which land cover-type detection, identification of probable forest fire ignition points, and prediction of ground salinity in remote sensing; prediction of solar radiation for photovoltaic installations; improvement of gas turbine performance; animal farm management to avoid waste and other applications. Although not specifically focused on EML, a recent survey  [101] also includes several applications of EML in environmental science.
19.2 Data Sources and Types
This field relies on data obtained from two sources: in situmeasurements and remote sensing. While in situmeasurements require the sensors to be in contact with the objects under study, remote sensingcan obtain measurements without any physical contact. In situmeasurements can be made using a large variety of instruments that are highly dependent on the study case. As an example, Johari et al.  [55] mention that soil characteristics can be obtained using pressure plates, Büchner funnels, tensiometers, pressure membranes, filter paper, and heat dissipation sensors. Remote sensing[30] measurements can also be obtained through different sensors, like sonars or cameras installed in boats, and different types of sensors onboard Unmanned Aerial Vehicles  (UAVs) and satellites, including optical and radar sensors. Since in situmeasurements can be performed using an endless list of instruments that vary from task to task, in Table  19.1, we provide only a list of remote sensinginstruments used in our bibliography.
Table 19.1
List of remote sensing instruments used in the surveyed works
Instrument type
Instruments
Sonars
Simdar EQ60  [110]
UAV Sensors
Compact Airborne Spectrographic Imager (CASI)  [29]
Fabry–Pérot interferometer (FPI)  [13]
Hyperspectral Digital Imagery Collection Experiment (HYDICE)  [126]
Airborne Visible InfraRed Imaging Spectrometer (AVIRIS)  [106]
Satellites (SAR)
Sentinel-1  [43]; E-SAR, RADARSAT-2 and AIRSAR  [71]
Satellites (Optical)
Landsat-8 (LS8)  [90] and Sentinel-2  (S2)  [42]
Synthetic Aperture Radar (SAR)
Described as Polarimetric SAR (PolSAR) imagery
Both UAVand satellite sensors measure radiance in specific wavelength intervals. Each of these intervals is called a band, and each sensor may perform measurements in several different bands. For consistency with the machine learning nomenclature, we will freely call “feature” to each of these bands. UAVs allow the acquisition of very high-resolution imagery  (e.g., 10cm per pixel, using the P4 multi-spectral UAV[82]). However, this data is limited in coverage. By opposition, satellite imageryis freely available and offers continuously updated worldwide coverage, for example, with Landsat-8  (LS8)  [90] and Sentinel-2  (S2)  [42]. The downside of free satellite imageryis the spatial resolution, which tends to be much lower than with UAVs, ranging from 10m to 60 m per pixel on both LS8 and S2.
The data used by the authors in our bibliography can be divided into two categories: uni-temporal and multi-temporal datasets. The first category includes datasets whose measurements were obtained in a single time frame and will be referred to as “uni-temporal datasets”, or simply “datasets”. The second category includes datasets whose values are obtained using several measurements, obtained in a succession of different time frames, and will be referred to as “time series”. Although uni-temporal datausually provides good results, time seriesnormally contains more useful information and therefore can be used to solve harder tasks. One application mentioned later is the separation of similar types of vegetation through their phenological cycles  [16].
This chapter covers the use of EML-based machine learning methods in uni-temporal datasets and time series, in several different tasks. Each section of this chapter covers a different kind of application, in the following order: data preparation, feature engineering(Sect.  19.4), split into feature selectionand feature construction; regression tasks (Sect.  19.5), i.e., the prediction of a numerical value for each sample, like the amount of biomass in each location; classification tasks (Sect.  19.6), split into binary and multiclass classification, i.e., prediction of the nominal label of each sample, like the land cover type for each pixel in satellite imagery; other tasks (Sect.  19.7) like image alignmentand parameter tuning. Other examples of applications are then given, focusing on the robustness of the EML models (Sect.  19.8) and highlighting the human-competitivenessof results obtained in selected work by the authors (Sect.  19.9).
19.3 Data Preparation
The machine learning pipelineincludes several steps that should be followed to ensure the quality of the final model. After collecting the data, the first step is data preparation. Data preparation includes tasks such as dealing with missing values, removing duplicated samples and outliers, and data normalization, among other tasks that may depend on the algorithms to be used later on, e.g., one-hot encoding to convert categorical features into numerical or boolean features.
Dealing with missing values is normally done using statistical methods. However, some authors use machine learning, and a recent survey  [49] provides a list of machine learning techniques used for this task. From the EML field, only methods based on GAs are mentioned, namely, the Multi-Objective GA for data Imputation(MOGAImp)  [73]. Two other works using EML for missing value imputationdescribe the use of GAs  [44] and ANNs assisted with GAs to minimize their error function  [2]. Specific to environmental science, we only find two EML works that perform missing value imputation: one uses GAs for the measurement of gas turbine blades  [40] and the other is the MOGAImp method  [73] applied to the classification of land cover types using satellite imagery
In some cases, the presence of a few bad samples within a dataset can lead to overfitting. Although their work is not focused on data preprocessing, in  [111] the authors propose a semi-supervised GP method that effectively detects and avoids learning mislabeled data in a dataset regarding the detection of burnt areas in satellite imagery. Such methods can also be studied from the data preparation point of view.
19.4 Feature Engineering in Environmental Science
After data preparation, feature engineeringis the next step in the machine learning pipelineFeature engineeringis necessary to ensure the machine learning algorithms use high-quality data to induce their predictive models, leading to better results and less overfitting  [35]. This step usually requires an expert to indicate which variables correlate with the problem, which variables we should discard and which variables we should combine  (e.g., to create indices that highlight characteristics of the dataset samples). This is traditionally a manual task that requires domain-specific knowledge, therefore it is time-consuming. However, featureengineeringis becoming more automated, namely, with its integration in AutoMLsystems likeTPOT  [91]. According to  [72], feature engineeringcan be partitioned into feature selectionandfeature construction.1This section addresses both types.
19.4.1 Feature Selection
Feature selectiondeals with reducing the number of features within a dataset. The objective is to facilitate the learning of the machine learning algorithm or to reduce the likelihood of overfitting by removing redundant or noisy featuresfrom the dataset  [72]. Although this selection is usually made by experts using their domain-specific knowledge, many methods are being developed to perform this task automatically.
Over the years, many EML algorithms have been used to performfeature selection. GAs  [52] are probably the oldest subtype of EML to be used forfeature selection  [68] and continue to be a very popular choice nowadays. There are other popular EML methods for feature selection, such as the ACO  [57], GP  [96], MOEA, and PSO  [39] algorithms.
As a practical example of feature selection, consider a GA in a hypothetical dataset with 10 features, where eight are noise and two are necessary to reach a good solution. Following the illustration in Fig.  19.1, the GA randomly generates a population of possible solutions to the problem in the form of arrays of “allowed features” to be used by a decision tree and uses their accuracy on a validation set as fitness. Over the generations, the noisy featuresare dropped out of the population while the good features survive. In a perfect scenario, the final model uses all the good features and none of the noisy ones.
Fig. 19.1
Example of feature selection using a GA. Using a fitness function that promotes selecting the good (green) features while avoiding the noisy (red) features, the GA learns which features are useful for the problem over the generations. A possible fitness function is the performance of a learning algorithm using only the selected features, e.g., validation accuracy of a decision tree
We now focus on methods created and studied to perform feature selectionin environmental science tasks. In remote sensingaerial imagery, feature selectionis commonly applied to select relevant bands in an image out of dozens or hundreds of bands. In  [29], the authors use the CASI imager to generate high-resolution images using 72 spectral bands from 409 to 947nm. Then, the authors compare the feature selectionperformed by GP-SVI  (their proposed GP-based method) and GA-PLS  (a GA-based method) with traditional statistical methods, concluding that GP-SVI outperforms the traditional methods and that GA-PLS, although not outperforming the traditional methods, has a very low standard deviation on the test results. Similarly, in  [51], the authors study three datasets obtained from images with a large number of bands  (115, 220, and 224 bands). They compare HC-ABC  (their proposed method, Hypergraph Clustering Artificial Bee Colony)with four non-EML algorithms and two EML algorithms and conclude that their method outperforms the other six in all three datasets. The PSO algorithm is used in  [13] to select 15 bands out of the 380 bands available using the FPI sensor, and in  [126] to select 7 bands out of 180 using the HYDICE sensor. Lastly, in  [106], the authors compare 10 different EML algorithms for feature selectionin three images obtained from the AVIRIS sensor with 176, 200, and 204 bands.
Overall, this illustrates the potential of EML-based algorithms for feature selectionin datasets with a large number of features. While these works are specific for datasets obtained using UAVimagery, datasets with many more features exist, and works from other areas show that EML-based algorithms can find optimal features in datasets with numbers of features that range from one thousand to over one million  [22, 66, 94, 99, 118, 122].
Unlike UAVimagery, satellite imageryfrom the most popular satellites has a smaller number of bands. For example, the LS8 and S2 satellites only use 11 and 13 bands, respectively. Since satellites usually use a smaller number of bands, the gap between the wavelengths measured by each band tends to be bigger. This results in satellite bands having well-known and different meanings, as explained in detail in  [90]. As described in this web page about the LS8 satellite, healthy vegetation is very reflective in Near-Infrared  (NIR) wavelengths  (band 5), so this band is very important to study vegetation. Shortwave Infrared (SWIR, bands 6 and 7) bands are very useful to study the soil, and band 7 allows an easier detection of burnt areas, as illustrated in Fig.  19.2. Although satellite imagerydatasets usually have a small number of features, it is useful to have feature selectionmethods that remove unnecessary bands from the dataset since this usually leads to the creation of simpler machine learning models.
Fig. 19.2
Landsat-8 satellite imagery over Guarda (Portugal) in September 2022. The image in  (b) uses band 7 to highlight burnt areas  (brown) and band 5 to highlight healthy vegetation  (green), facilitating the detection of burnt areas
In  [56], the authors build an improved version of the Normalized Difference Vegetation Index  (NDVI)  [102] using a GA to evolve weights in a mathematical expression that uses several bands. After obtaining the final weights, the authors removed the bands that had little impact on the expression  (i.e., the bands associated with very small weights). In  [71], the authors propose SAE-MOEA/D, a new MOEA algorithm with a Stacked AutoEncoder(SAE), to select bands to be used by convolutional neural networks  (CNN)[47] in PolSAR imagery.
Some methods solve tasks by evolving models that do not use all the available features. Since feature selection isa collateral result of their primary objective, we consider this “implicit” feature selection. Since some GP-based algorithms evolved models that only use a subset of the features, this implicit feature selectionis a frequent side effect of their evolution. The following works perform implicit feature selection, although their main objective is feature construction. As such, we will explain them in greater detail in Sect.  19.4.2
In  [16, 18, 82], the authors use GP-based methods and comment on the features selected for the final models. In  [82], the authors report that the red and blue wavelengths seem to be highly preferred by the models when detecting mistletoe, while the red-edge infrared and NIR wavelengths seem to be avoided. In  [16], the authors use time seriesto separate two similar land cover types using data from several months and comment that the features of August are frequently selected by the models, indicating that this month is important in the separation of the two land cover types. In  [18], the authors evolve features that detect water in cloudy imagery and comment that, besides the models selecting the same features as remote sensingexperts do to detect water, they also use the features that are associated with detecting clouds. Other works also use GP-based algorithms for classification [15, 17, 76] and regression  [55, 93, 97, 115] tasks. However, the authors do not give emphasis on the feature selectionside of these algorithms.
These algorithms can also be applied to feature selectionin datasets that are not based on remote sensing. In  [127], the authors apply the Standard GP  (Std-GP)  [96] algorithm to a regression dataset to predict fish weights based on the measurements of the fish. Then, the authors comment on the use frequency of each feature for each fish species, indicating that the weight of each fish species is correlated with the measurements of different bodyparts. In  [38], the authors propose the evolutionary feature selectionalgorithm and compare this method’s accuracy and number of selected features with other EML-based algorithms in a benchmark that, among other non-EML datasets, include a dataset for the classification of animal types based on their physical characteristics. Although this work is not focused on conservation biology, this dataset may help to study habitats in future.
19.4.2 Feature Construction
Feature constructioncombines or modifies features to create more informative ones  [72]. Ideally, these new features are easier to handle by machine learning algorithms and, assuming their interpretability, also by human experts. However, due to the large number of possible combinations of features and the computational costof testing all combinations, this task is usually performed manually by experts. In this section, we talk about EML for automatic feature construction
Domain experts can use their knowledge to perform manual feature construction, selecting features that are considered relevant to the problem and combining them. For example, in the remote sensingfield, experts know that some ratios between certain wavelengths are informative for specific problems. Those ratios can be added to the datasets, thus facilitating learning. The NDVI  [102], displayed in Eq.  19.1, is an example of a popular ratio. This popular index highlights healthy vegetation in remote sensingdata, as exemplified in Fig.  19.3
(19.1)
Fig. 19.3
Landsat-8 satellite imagery, obtained over Lisbon  (Portugal) in September 2022. The NDVI in  (b) is used to highlight the vegetation within the image
Although feature constructionis traditionally a manual task, over the years, many evolutionary computation methods have been developed to perform automatic feature construction(e.g., EFS  [7] and FEAT  [65]). Depending on the feature constructionalgorithm and the complexity of the problem, the evolved set of features varies in the number of features, their complexity, and the number of original features used in each new feature’s expression. This leads to some variance in the interpretabilityof the final models. Some machine learning algorithms, namely those based on deep learning  [47], are known not only for their state-of-the-art results but also for their lack ofmodel interpretability, despite some visualization methods  [128] and other recent efforts  [120, 129] that allow them to be explained. By opposition, EML algorithms are known for their potential interpretability[11, 103]. It should be noted that some EML algorithms, like the ones of neuroevolution, evolve models that are not considered interpretable. Some works may also use algorithms known for providing interpretable modelsand fail to obtain them due to, e.g., the complexity of the problems or the use of sub-optimal parameters.
As a practical example of feature construction, consider the Std-GP algorithm in a hypothetical dataset where the optimal solution is the sum of two specific features and the other features are noise. Following the illustration in Fig.  19.4, the Std-GP randomly generates a population of possible solutions in the form of mathematical expressions that convert a dataset sample into a single numerical value. Over the generations, the features selected by the model (implicit feature selection)and the operators and shape of the model (feature construction)will change while optimizing the fitness function. In this case, the fitness function can be the correlation between the model’s output and the prediction, making a model applicable to regression tasks and binary classification (e.g., by converting class labels to numerical values 0 and 1). More complex methods, like M3GP  [85], evolve a set of trees, resulting in multidimensional model output. Similarly to the GA in the previous section’s example, these models may use a second learning algorithm to calculate their fitness. Due to the multidimensionality of the output, these models can also be used for feature constructionin multiclass datasets.
Fig. 19.4
Example of feature construction using Std-GP. Here, the optimal solution is A+B. Using a fitness function that promotes the search for this expression (e.g., accuracy), the population evolves models that select the correct features (implicit feature selection)while using the correct operators
The typical way to perform feature constructioninvolves the creation of features and adding them to the dataset, as seen in several works that add manually created features to the dataset  [15, 95]. Although some works replace the original features with those evolved by the machine learning algorithms  [16, 17, 75], other works suggest that replacing the original features might not always be the best option. Instead, these works add the evolved features to the dataset  [15]. However, depending on the classifier, this approach should be avoided since it increases the dimensionality of the dataset, possibly leading to a higher computational costwhen training other models in the extended dataset.
The following works dealt with the evolution of features whose expression is small enough to be considered easily interpretable. In  [31], the authors use a GA to evolve weights for an arithmetic expression that combines three features and three weights, with the objective of mimicking the NDVI using the RGB values. This allows the use of cameras without infrared sensors in the study of healthy vegetation. Similarly, in  [56], the authors create an improved version of the NDVI by evolving a set of 12 weights and, after training the model, remove the features associated with small weights, simplifying the expression evolved by the algorithm.
The previous works use GA-based algorithms to evolve features with a fixed structure predefined by authors. In opposition, GP-based algorithms do not require a predefined structure for the models since they are mutable and adapted over the evolutionary cycle. Even among GP-based algorithms, the structure of the models can be decomposed into two types: models composed of a single tree and models composed of multiple trees. While the first approach is usually far more limited, it can be applied to regression and binary classification tasks in a straightforward way, while multiclass classificationtasks usually require multiple features, i.e., a model with multiple trees/outputs.
Typically, the authors use GP-based algorithms, namely the Std-GP algorithm, to evolve models composed of a single tree, both for regression tasks  [55, 93, 97, 113, 115, 127] and for binary classification  [18, 21, 76, 123]. Other applications of GP-based methods in remote sensingcan also be seen in  [67]. Unlike Std-GP, other EML-based feature constructionmethods, such as the already mentioned EFS  [7], EPR  [46] and M3GP  [85] algorithms, evolve a set of trees, rather than a single tree, resulting in a multidimensional output from the model. Although these algorithms show potential in creating interpretable models, this interpretabilitymay depend on the number of trees and the mathematical operators used. In  [81], the authors review several works related to feature constructionusing GP-based methods in the context of explainable artificial intelligence. Those works have a wide range of application fields, including environmental science.
For more complex tasks, such as multiclass classificationand harder binary classification and regression tasks, the authors typically use methods that evolve several features. These features can work independently from each other or in a cooperative manner. In works that treat multiclass classificationas a succession of binary classification problems  [110], we consider that the features are independent, since each feature separates a different set of classes. In more advanced multiclass classification methods, such as the M3GP  [85] and M4GP  [66], and the Std-GP algorithm with COMB functions  [75], the algorithms evolve a set of cooperative features (a transformation) that converts the feature space, and then perform multiclass classificationin this new space, separating all classes using the same transformation. Another method derived from M3GP, called eM3GP  [109], assumes that an ensemble of transformations, each proving to be good for discriminating a single class, may do a better job than a single transformation used for discriminating all the classes. In  [87], a set of features is evolved to be jointly used in a single linear regression model.
19.5 Regression Tasks for Environmental Science
In regression tasks, the machine learning methods attempt to predict a numerical value to be associated with each sample in a dataset. Taking the NDVI  [102] as an example, this index associates each pixel in an image with a value from to 1, where a higher value indicates healthier green vegetation. In a regression task to detect vegetation, a machine learning model should have a similar kind of behavior. In this section, we will give an overview of works related to the detection of vegetation and salinity levels, the prediction of necessary soil nutrients for crop development, the prediction of soil properties, and the prediction of solar radiation.
The most common application of regression models in environmental science is precisely the creation of models to detect some property on the terrain, such as vegetation, biomass, or salinity. In  [97, 117], the authors apply the Std-GP algorithm to satellite imageryto create vegetation indices that correlate with the Revised Universal Soil Loss Equation  (RUSLE) C factor, an index that assesses how land use affects soil loss and sediment generation  [8]. The authors improve previous work that could only be applied to green vegetation by creating models that also correlate with the RUSLE C factor in dry vegetation. In  [31], the authors use GAs to evolve weights to be used in a function that tries to predict the NDVI values using only the visible wavelengths sensor  (i.e., without using the NIR wavelength). Similarly, in  [56], the authors use GAs to evolve weights to create a new vegetation index. In  [5], the authors use the Std-GP algorithm to evolve indices for phenology analysis using time seriesdata obtained from a hemispherical lens camera set in an 18-meter tall tower. In  [28], Std-GP is used to predict levels of chlorophyll concentration, obtaining better results than traditional methods. In  [108], the authors apply 14 machine learning methods, including Std-GP and GeometricSemantic GP(GSGP)  [84] in the prediction of forest biomass. With this work, the authors conclude that even the best methods overfit the training data, indicating that this is a complex task, especially when using less than 100 samples for training. In  [3], the authors use a GA-based algorithm to recommend nutrients for optimal crop development, improving soil fertility using time series, resulting in increased production. Similarly, in  [29], the authors use a GP-based algorithm for precision farming, obtaining better results than those obtained using traditional approaches. In  [130], the authors use GA to optimize the number of hidden layers and nodes in a CNN. In this work, the model is applied to the prediction of evapotranspiration values of soybean plantations in different growth stages. In  [1], the authors use Std-GP to predict the fraction of calves that survive per cow each year in several farms. This work on precision farming leads to a better estimate of the number of calves that the farms should produce to minimize waste and maximize profits.
Some works are related to the application of the Std-GP algorithm to the prediction of soil properties. In  [115], the authors use satellite imageryto predict the soil electrical conductivity and soil surface salinity. In  [55], the authors study the Soil–Water Characteristic Curve  (SWCC) of different kinds of soils from several soil properties. In  [93], the authors study the saturated hydraulic conductivity of different soil types.
As an important theme in environmental science, several works focus on predicting several aspects related to energy. In  [45], the authors use several EML algorithms to estimate the amount of solar radiation that would hit Queensland  (Australia) using time seriesobtained from monthly predictors over 5 years. Similarly, in  [9], the authors study the prediction of solar power for the integration of a photovoltaic site to improve the reliabilityof photovoltaic systems using time series, with measures of solar power every 5 minutes, over several days, in three different cities in Florida  (USA). In  [89], the authors use GAs to search for the optimal locations for photovoltaic installations in La Palma Del Condado  (Spain) under a series of constraints to minimize environmental, safety, and economic risks. Those constraints make the algorithm avoid suggestions near environmentally protected areas, roads, urban areas, and vineyards  (due to their importance to the local economy). Similarly, in  [98], the authors also deal with the prediction of solar power production from solar panels, in Potsdam University  (Germany). Gas turbines are also of great importance for power generation. However, due to their complexity and their execution environment, they can have a high failure rate that leads to severe consequences. To minimize this problem, in  [41], Std-GP and several variants of GSGP are used to predict the fuel flow and the exhaust gas temperature in two separate datasets, to improve the performance of the gas turbines.
