0.8695
0.9739
0.9688
0.9852
0.9826
Propublica recidivisim
Race
0.8324
0.7714
0.6849
0.6788
0.9022
0.8078
0.9105
0.8469
Propublica recidivisim
Sex
0.9408
0.9659
0.6756
0.6623
0.9254
0.9326
0.9451
0.9411
Average rank
1.2857
1.7143
1.4762
1.5238
1.2381
1.7143
1.2381
1.7619
Wilcoxon signed-rank test
0.0357
0.5222
0.0036
0.0300
17.4.4 A Pareto Dominance-Based Genetic Algorithm for Fair Feature Selection
Rehman et al. have proposed another GA forfair feature selection [34], again in the context of the classification task of machine learning. This GA has both similarities and differences with respect to LGAFFS [3] (described in the previous Subsection), as follows.
First of all, in both this GA and LGAFFS, each individual represents a candidate feature subset and both GAs use the same individual representation: each individual consists of mbits, where mis the number of features (attributes) in the dataset, and each bit takes the 1 or 0 value to indicate whether or not its corresponding feature is selected, respectively.
Another similaritybetween Rehman et al.’s GA and LGAFFS is that, at a high level of abstraction, both these GAs follow the wrapper approach for feature selection in a pre-processing phase, so that the selected features will be used later as input by a given classification algorithm (called here the ‘target algorithm’). Hence, following the principle of their wrapper approach, both GAs compute an individual’s fitness by measuring the quality of a model learned by the target algorithm when it is trained with the candidate feature subset encoded into that individual. As a result, the selected features are customised to the target algorithm. To be precise, the target classification algorithm was random forests in the experiments with LGAFFS in [3], whilst the experiments with Rehman et al.’s GA in [34] used three target algorithms: logistic regression, XGBoost and a Support Vector Machine (SVM).
However, Rehman et al.’s GA and LGAFFS also have important differences in their fitness computation, particularly with respect to which objective functions they optimise and the approach used to perform multi-objective optimisation, as follows.
First, Rehman et al.’s GA optimises two objectives: the F1-score, as a measure of predictive accuracy, and statistical parity (or discrimination score) as a measure of fairness. To be precise, we are referring here to the more sophisticated version of that GA, which is referred to as ‘Algorithm 2’ in [34], rather than the simpler version of that GA, referred to as ‘Algorithm 1’, which optimises just the fairness objective, ignoring predictive accuracy. By contrast, as mentioned earlier, LGAFFS optimises the geometric mean of sensitivity and specificity as the predictive accuracymeasure, and optimises four fairness measures(including the discrimination score)
Second, the (Algorithm 2) GA in [34] is basically the well-known NSGA-IIfor multi-objective optimisation based on the concept of Pareto dominance. By contrast, as mentioned earlier, LGAFFS follows a lexicographic approach [3], rather than the Pareto dominanceapproach.
Two experiments were reported in [34], each using only two datasets. Each run of the GA selected features for one of the three aforementioned classification algorithms. In the first experiment, the GA was used to optimise fairness only(i.e. not accuracy). As expected, in general, the features selected by the GA led to improved fairness but reduced predictiveaccuracy, by comparison with not performing feature selectionin a pre-processing step (i.e. giving all features to the classification algorithm). The second experiment used NSGA-IIto optimise both accuracy and fairness, but there was no comparison with other feature selectionapproaches.
In the future, it would be interesting to compare the results of the GAs for fair feature selectionproposed in [3, 34] on controlled experiments with the same target classification algorithm (for the wrapper approach) and the same objective functions on the same datasets, since they use basically the same individual representation but quite different multi-objective optimisationapproaches.
17.4.5 An Evolutionary Algorithm for Fair Ensembles of Artificial Neural Network Models
Zhang et al. [45] proposed a fairness-aware multi-objective GA for evolving an ensemble of Artificial Neural Network (ANN) models. The individual representation consists of a vector of real-valued weights and biases for an ANN, using the same fixed ANN architecture (a fully connected network with one hidden layer) for all individuals.
In addition, the models produced by the individuals in the last generation are combined into an ensemble of non-dominated models. This work considered four different approaches to produce the ensemble, including the selection of all non-dominated solutionsand different approaches to select a subset of non-dominated solutions, based on their quality or their diversity. The use of an ensemble of ANN models leads to good predictive accuracyin general, but it sacrifices the interpretability, due to both the black-box nature of ANNs and the aggregation of multiple models in the ensemble. Hence, in applications where the interpretabilityof the learned models is also important, it would be interesting to use somepost hoc interpretabilityapproach [5] to try to explain the predictions of the ANN ensemble.
Regarding selection, the EA proposed in [45] does not use Pareto dominance-based selection (like in the well-known NSGA-IIalgorithm) because this approach tends to be inefficient when the problem has many (more than 3) objectives, since the proportion of non-domination solutions tends to be quite large in this case. In some of the experiments in this work, there are nine objectives being optimised (see below), so Pareto dominance-based selection would not be practical. Hence, the authors used instead a selection method more suitable for many-objective optimisationproblems, based on the stochastic ranking algorithm described in [28].
The empirical results reported in [45] involve two sets of experiments. In the first set, the EA optimised three objectives, namely, one measure of predictive accuracy(the cross-entropy)and two measures of fairness—where one is a group-fairness measureand the other is an individual-fairness measure. The results of the first set of experiments were evaluated in terms of the well-known hyper-volume measureand the coverage over Pareto front(which emphasises more the diversityof solutions in the Pareto front). The results of the multi-objective EA were compared with the results of four variants of Multi-FR [42], a non-evolutionary method for multi-objective optimisationbased on gradient descent, on 8 datasets. Broadly speaking, the solutions generated by the multi-objective EA dominated (in the Pareto sense) the solutions generated by the multi-FR method more often than vice versa, showing the greater effectiveness of the multi-objective EA.
In the second set of experiments, the EA optimised nine objectives, the cross-entropyand eight fairness measures. The evaluation of the results actually considered 16 fairness measures, but they were divided into 2 groups: the fairness measure set 1, containing 8 measures optimised by the EA (chosen as representative measures of the full set of 16 measures), and the fairness measure set 2, containing the other 8 measures, which were not optimised by the EA and were used only to evaluate if the EA results also generalise well to improve the values of fairnessmeasuresnot optimised during training. This second set of experiments used 15 datasets. The four variants of the proposed EA (with four types of ensembles) were compared against five variants of non-evolutionary ensembles proposed in [22], which used various types of base classifiers as members of the ensembles. Hence, due to the use of a relatively large number of datasets and fairness measures, as well as several variants of evolutionary and non-evolutionary multi-objective ensemble methods being compared, this seems the most extensive evaluation of a fairness-aware EA in the current literature. The results of these experiments have shown that, for most datasets, the values of the fairness measuresin set 2 (not optimised during training) are also improved in general by the EA, as evaluated by the hyper-volume measure, showing good generalisation ability regarding the fairness measures
17.4.6 A Genetic Algorithm for Finding Fair Counterfactuals
Dandl et al. [10] proposed a GA for generating fair counterfactuals, which they called ‘counterfactual fairness’. To explain this concept, first, let us briefly introduce the concept of counterfactual, which is a statement of how the world would have to be different in order for a desirable outcome to occur [9].
To make this clearer, let us use as an example the typical application domain of credit approval. Suppose a customer’s credit card application is rejected by a bank. In this scenario, a counterfactualshould identify how the values of some features in the customer’s application would have to change in order for the customer to receive the desirable outcome of credit approval. An example counterfactualwould be: ‘Your current annual salary is $40,000; if your annual salary were $60,000 your credit card would have been approved’.
Turning to the concept of counterfactual fairnessin [10], in the context of supervised learning (with a target variable), the key idea is that the value of the target variable predicted by a model learned from the data for a given individual should be the same as the value that would be predicted if that individual had a different value of a sensitive (or protected) feature, which typically would mean the individual would belong to a different demographic group. For example, consider again the credit approval domain, and suppose the sensitive feature is gender. Consider a male customer whose credit-card application was approved based on a feature vector (set of variables) describing that customer. Then, counterfactual fairnessrequires that if a customer had the same feature vector with the exception of having gender female, the credit card application would also be approved. That is, the approval outcome should not be affected by the change only in the value of a sensitive feature.
The GA for counterfactual fairnessin [10] is based on the GA for counterfactual explanations in [9], and its main characteristics are as follows.
The individual representation essentially consists of a feature vector representing a candidate counterfactualfor a given input data instance. Note that a candidate counterfactual refers to the same features as the data instances; the difference is that some features in the counterfactualwill take values that differ from their values in the feature vector in the input data instance. Hence, an individual (candidate counterfactual)will contain both numerical and categorical features, if the dataset consists of both these data types of features.
In the search for fair counterfactualsfor a given input data instance x, the GA optimises three objectives: (a) validity, in the sense that the counterfactual should have a high probability with respect to the distribution of the training instances in the protected (or sensitive) group; (b) similarityto the current instance x, i.e. the counterfactual should have feature values that are similar to the feature values of x; and (c) plausibility, i.e. the counterfactualshould lie in a high-density region of the data space for the full dataset.
To perform this multi-objective optimisation, a modified version of the well-known NSGA-IIalgorithm [12] is used to return a set of non-dominated solutions(counterfactuals), based on the concept of Pareto dominance. The NSGA-II’smodifications were designed to make it more tailored for the task of finding fair counterfactuals. In particular, two types of NSGA-II modifications suggested in [10] are as follows.
First, instead of randomly generating the initial population as usual, the population can be initialised with training data instances that have the protected value of the sensitive feature.
Second, specialised mutation operators can be used that constraint the allowable changes to feature values, possibly based on background knowledge. For example, since an individual can hardly change her/his gender, this feature can have its value frozen, i.e. not being allowed to undergo mutation.
The experiments reported in [10] used two datasets to evaluate the counterfactualsgenerated by the GA: a synthetic dataset, created using the data-generating process of a law school dataset [24], and the Adult (census) dataset from the well-known UCI Machine Learning repository—often used as a benchmark in fair classification. The counterfactualsgenerated by the GA were compared against two baselines for generating counterfactuals: at random and using a nearest neighbour approach. The results showed that overall the GA generated good counterfactuals, but the use of only two datasets (only one being a real-world one) makes it difficult to draw a robust conclusion about the GA’s performance.
17.5 Discussion and Conclusions
We have reviewed several EAs developed for ‘fair classification’, i.e. EAs that aim to evolve candidate solutions for a classification problem which exhibit both high predictive accuracyand a high degree of fairness. This is naturally cast as a multi-objective optimisationproblem, involving a trade-off between the objectives of optimising accuracy and fairness. Hence, it is not surprising that all the EAs reviewed in this chapter use some multi-objective optimisationapproach for addressing this problem.
The six EAs reviewed in this chapter address in general different types of tasks within the broader process of classification, reflecting the fact that EAs are very flexible and generic search and optimisation methods. Three of the six EAs addressed a data pre-processing method for classification: one GP method for feature construction, and two (independently developed) GAs for feature selection. Another reviewed method was a GA for optimising the hyper-parameters of a base classification algorithm; and another method was an EA to evolve an ensemble of ANN models. Finally, another reviewed method was a GA for finding fair counterfactuals
In general, these EAs work together with a given classification algorithm to optimise the classification model to be learned from the data by that algorithm, i.e. the EAs find candidate solutions customised not only to the input dataset but also to the chosen algorithm—which could be in principle any generic classification algorithm. The exception is the reviewed GA for finding fair counterfactuals, which does not require any classification algorithm, since it simply evolves counterfactuals, without learning classification models.
Regarding multi-objective optimisation approaches, most of the reviewed EAs use the Pareto dominanceapproach, in general, using some variant of the very popular NSGA-IIalgorithm based on that approach. Exceptions are an EA using stochastic ranking selection for many-objective optimisation; a GA for feature selection basedon the lexicographic approach, where objectives are optimised in decreasing order of priority; and a version of a GP using lexicase selection(broadly related to the lexicographic approach).
As discussed earlier, the Pareto and lexicographic approaches have different pros and cons. In the context of the problem of evolving accurate and fair classifiers, in the case of real-world applications, arguably the choice between these approaches should depend mainly on the needs and preferences of users. In particular, if the user does not want to specify a priority order for the objectives of accuracy and fairness, and if the user would like to examine many non-dominated solutionsin order to subjectively choose the one with their favourite trade-off between those objectives, then the Pareto approach seems a natural choice. On the other hand, if the user clearly assigns different priorities to the different objectives (e.g. considering accuracy more important than fairness), and if the user would find it difficult to choose among many non-dominated solutions, it seems natural to incorporate the user’s priority order of objectives into the search algorithm, to make the search more efficient, and in this case, the lexicographic approach seems a natural choice.
In any case, the current literature lacks an empirical comparison between the Pareto and lexicographic approaches for fair classification, so this is a natural direction for future research.
In addition, there is a clear need for further work on evaluating EAs for fair classification on more datasets, since the number of datasets in nearly all the experiments evaluating the reviewed EAs varied from only 2–7. The exception is that the experiments with 8 or 15 datasets were reported in [45]. Experiments with many datasets are needed to draw more robust conclusions about the EAs’ performances. Moreover, the reviewed experiments focused on measuring fairnessusing mathematical formulas only, but recallthat (as mentioned in Sect.  17.2) there is no guarantee that maximising such fairness measureswould lead to really fair predictions from a user’s perspective, given the subjective nature of fairness. For real-world applications, it is important that real-world users evaluate the fairnessof the learned classification models.
Another direction for future research would be to consider more objectives, including different types of objectives, like some measure of interpretabilityor simplicity of the classifiers—an issue that has also become more popular in machine learning in recent years [5]. Actually, in many applications involving data about people, both fairnessand interpretabilityare needed in order to produce classifiers that can be really trusted by users and deployed in their real-world business activities.
References
1.
Angwin, J., Larson, J., Mattu, S., Kirchner, L.: Machine bias: there’s software used across the country to predict future criminals, and it’s biased against blacks (2016)
2.
Binns, R.: Fairness in machine learning: lessons from political philosophy. J. Mach. Learn. Res. 81, 1–11 (2018)
3.
Brookhouse, J., Freitas, A.A.: Fair feature selection with a lexicographic multi-objective genetic algorithm. In: Proceedings of the 2022 Parallel Problem Solving from Nature Conference (PPSN 2022), LNCS 13399, pp. 151–163. Springer (2022)
4.
Buolamwini, J., Gebru, T.: Gender shades: intersectional accuracy disparities in commercial gender classification. In: Proceedings of Machine Learning Research: Conference on Fairness, Accountability and Transparency, vol. 81, pp. 1–15 (2018)
5.
Burkart, N., Huber, M.F.: A survey on the explainability of supervised machine learning. J. Mach. Learn. Res. 70, 245–317 (2021)MathSciNetzbMATH
6.
Calders, T., Verwer, S.: Three naive bayes approaches for discrimination-free classification. Data Min. Knowl. Disc. 21(2), 277–292 (2010)MathSciNetCrossref
7.
Chouldechova, A.: Fair prediction with disparate impact: a study of bias in recidivism prediction instruments. Big Data 5(2), 153–163 (2017)Crossref
8.
Corbett-Davies, S., Goel, S.: The measure and mismeasure of fairness: a critical review of fair machine learning (2018). arXiv:​1808.​00023
9.
Dandl, S., Molnar, C., Binder, M., Bischl, B.: Multi-objective counterfactual explanations. In: Proceedings of PPSN 2020 (Parallel Problem Solving from Nature Conference), LNCS 12269, pp. 448–469. Springer (2020)
10.
Dandl, S., Pfisterer, F., Bischl, B.: Multi-objective counterfactual fairness. In: Proceedings of the GECCO’22 Companion (Genetic and Evolutionary Computation Conference), pp. 328–331. ACM Press (2022)
11.
Deb, K.: Multi-Objective Optimization Using Evolutionary Algorithms. Wiley (2002)
12.
Deb, K., Pratap, A., Agarwal, S., Meyarivan, T.: A fast and elitist multiobjective genetic algorithm: Nsga-ii. IEEE Trans. Evol. Comput. 6(2), 182–197 (2002)
13.
