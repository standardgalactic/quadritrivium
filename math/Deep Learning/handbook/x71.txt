Generator ensemble quality score . Fitness for weight evolution, see Alg.  13.8
Variables optimized by Lipizzaner, Ensemble of generators, 
Best ensemble of generators, , see Eq.  (13.5)
Dataset sampling fraction 
13.2.1.1 Learning Definitions and Notation
We define different types of learning as
Supervised Learning  (SL)The input has labels, . Learn a parameterized function . The parameters are learned by . The objective is to minimize the difference between the model output and the actual value, argmin
Unsupervised Learning  (UL)The input does not have labels, . Learn a distance function . The parameters are learned by . The objective is to minimize the distance between similar input, argmin
Reinforcement Learning(RL)An agent takes a sequence of actions that transition between states and collect a reward for their action sequence. Define environment states , agent actions , transition probabilities , and rewards . Objective is to learn a policyto maximize the reward . Competitive Multi-agent reinforcement learning(MARL) can be expressed with an increase in state, action space for the multiple agents, and competing rewards for each agent
Evolutionary LearningAn evolutionary learning process selects and varies solutions over time based on the quality  (fitness) when compared to other solutions in a population. Fitness is . The population of solutions at time tis . The variation operation is . The evaluation of the population is . The objective is to maximize fitness argmax. Figure  13.3shows a high-level EC process for learning. This is the basis for the Adversarial Evolutionary Learning  (AEL).
Fig. 13.3
The iterative process used for evolutionary computation (EC). At iteration t, a set of individual  (sample) is selected from a population and varied. These individuals are then evaluated. Individual performance is determined. The population is then updated toward the better performing individuals. The process then begins again with the new population 
Adversarial Learning  (AL)Learning with two explicit competing adversaries (player/opponent, test/solution, red/blue, attacker/defender, predator/prey, generator/discriminator) . The engagement of the adversaries is . The parameters are learned by . The objective is for one adversary to minimize and the other to maximize . Note, AL often use 
One minimax-instance of AL is GAN. We focus this chapter on how AEL can help improve a GAN. Next, we provide a more extensive description of GAN training.
13.2.1.2 GAN Notation
We adopt notation similar to  [8, 54, 65], see Table  13.2. We first introduce GAN notation, then describe some GAN learning tasks, how to use a topology, and GAN training details.
Let and denote sets of generatorsand discriminatorsis a function parameterized by gis a function parameterized by d. The generatordefines a distribution , this is done by generating zfrom an l-dimensional Gaussian distribution and then applying on zto generate a sample of the distribution . Finally, let be the unknown target distribution to which we would like to fit our generative model 
Formally, the goal of GAN training is to find parameters gand din order to optimize the objective function
(13.1)
and is a concave measuring function. In practice, we have access to a finite number of training samples to approximate or . Therefore, an empirical version is used to estimate 
13.2.1.3 GAN Learning Tasks
This section introduces the main adversarial learning tasks that are approached with GANs, i.e., generative modeling and ensemble repurposing.
For the task of generative modeling one way of expressing Lipizzaneris as a function Fthat returns an ensemble of generatorparameters and their ensemble weights (see Eq.  13.2). The variables that are optimized are , the ensemble size is , the parameters of each generatorin the ensemble , the learning rate for neural network training and the ensemble mixture weights , and is a dataset (a sample from the target distribution ). We focus on finding the best ensemble of generatorsaccording to a measuring function . This can be stated as it is described in Eq.  13.3by using the GAN training Eq.  (13.1) and is the set of parameters , shown in Table  13.2
(13.2)
(13.3)
by using the GAN training Eq.  (13.1) and is the set of parameters , shown in Table  13.2
We present the task of ensemble repurposing as follows. Given a set of generatorsfind the weight vector . For the set of generators , the optimal ensemble mixture weight vector is defined as follows:
(13.4)
where represents the probability that a data point comes from the ith generator, with 
13.2.1.4 Topology
In this chapter, we focus on a spatially distributed CCA applied to train GANs, i.e., LipizzanerLipizzaneruses a square toroidal grid with Ncells, . A cell has a neighborhoodwhich allows it to form two sub-populations of models: and . We denote the size of this neighborhood by sand the number of neighborhoods is N. We use a default five-cell von Neumann neighborhood (), with radius  (overlap) , see Fig.  13.4a.
Fig. 13.4
Overlapping neighborhoods and sub-population definitions according to topologies and neighborhood radius
For the kth neighborhood in the grid , we refer to the generatorin its center cell by and the generators in the rest of the neighborhood by , respectively. Furthermore, we denote the union of these generatorsfor the kth generator neighborhood by In Lipizzaner, the generatorsof the Nsub-populations each of size sto form Nensembles, the best ensemble is returned by Lipizzaner. For the ensembles of generatorsthe s-dimensional ensemble mixture weight vector is defined as follows:
(13.5)
13.2.1.5 GAN Training
Multiple loss functionshave been defined for addressing the underlying optimization problem behind GAN training for . The ones we are concerned with are (written for generators): Binary cross entropy (BCE) losswhere the model’s objective is to minimize the Jensen–Shannon divergence (JSD) between thereal and fake data distributions Mean squared error (MSE) losswhere the model’s objective is to minimize the average of the squares of the errors, e.g.,  [74]. Heuristic loss (HEU)objective maximizes the probability of the discriminatorbeing mistaken by minimizing the objective function  [110] Wasserstein (WASS) lossWasserstein GANs  [7] minimize the Wasserstein Distancebetween the two probability distributions  [7].
The challenge of training a GAN is increased by some pathologies:
Non-Convergence: The model parameters oscillate, destabilize, and never converge. Proposed remedies are regularizationof the discriminator, e.g., add noise to discriminator inputs and penalize discriminator weights.
Mode Collapse: The generatorcollapses which produces limited varieties of samples. Proposed remedies are unrolled GANs and Wasserstein loss.
Vanishing gradient: The discriminatorgets too successful that the generatorgradient vanishes and learns nothing. Proposed remedies are Wasserstein loss and modified minimaxloss.
Next, we present an overview of adversarial evolutionary learning.
13.2.2 Adversarial Evolutionary Learning
In this section, we discuss different AEL aspects, with focus on CCA. We briefly introduce adversarial learning with some evolutionary learning examples, and then we present CCAs.
13.2.2.1 Adversarial Learning
There are multiple subfields that have studied learning adversaries. We briefly present them here:
Evolutionary Game Theory(EGT) started as an application of the mathematical theory of games to biological contexts based on the notion that frequency-dependent fitness introduces a strategic aspect to evolution. Game theory is a formal theory of decisions of intelligent rational agents in conflicting situations where the decisions depend on each other. In evolutionary game theory, there is a very large population that are randomly pairedto play a given strategy  [102]. The interaction of multiple autonomous agents gives rise to highly dynamic and non-deterministic environments in the evolutionary dynamics of multi-agent learning  [17]. As an example a combination of elements from deep learning and artificial life demonstrated that coevolutionary neural population models can simulate population dynamics, and that evolutionary game theory can describe the behavior  [80].
Multi-agent Reinforcement Learning(MARL) is a subfield of reinforcement learningthat focuses on studying the behavior of multiple learning agents that coexist in a shared environment  [23]. Each agent has its own rewards and takes actions accordingly. Competing agent interests can result in non-trivial group dynamics. MARL is related to game theory. A comprehensive survey of state-of-the-art methods for integrating EC into RL  [12].
Adversarial Machine Learning(AML) study attacks and defenses on machine learning algorithms  [16, 49, 94, 109]. For example, artificial neural networks can produce high confidence predictions for unrecognizable images  [82]. In another example is generating evasive malware  [4, 40, 76, 113].
13.2.2.2 Competitive Coevolutionary Algorithm
An Evolutionary Algorithm  (EA) can evolve individuals, e.g., fixed length genotypes such as the bit strings used by Genetic Algorithms (GAs)  [50]. Biological coevolution refers to the influence of multiple species on each other’s evolution  [42, 90]. We are concerned with competitive coevolution, e.g., due to constrained resources under contention or a predator–prey relationship. EAs usually abstract the evolutionary process and evaluate the quality of an individual with an a priori defined fitness function. Coevolutionary algorithms extend EAs and base the fitness of an individual on its interactions with other individuals or the environment. This mimics the coupled interaction of natural coevolution  [77, 88, 90, 97].
A basic competitive coevolutionary algorithm evolves two coupled populations, each with selection and variation (e.g., crossover and mutation). In this section, we refer to individuals from the two adversarialpopulations as adversaries. Two types of adversaries perform the competitive coevolution: testsand solutions. In each generation, competitions  (engagements) are formed by pairing adversaries from their respective populations. This shared fitness evaluation couples the population. The objective of a test individual is to demonstrate the solution as incorrect. Conversely, the objective of the solution individual is to solve the test correctly. The dynamic of the competitive coevolutionary algorithm, driven by conflicting objectives and guided by performance-based selection and random variation, can gradually produce better and more robust solutions  [90, 97]. Generally, competitive coevolutionary algorithm s suit interactive domains where performance is relative to others, e.g., games  [88]. In addition, there is also some initial runtime analysisof a competitive coevolutionary algorithm named Pairwise Dominance CoEvolutionary Algorithm (PDCoEA)  [63] with level-based analysis  [30].
An efficient competitive coevolutionary algorithm minimizes the number of competitions per generation while maximizing the accuracy of its fitness estimate of each adversary. Two competition structures are: one-vs-one, each adversary competes only once against a member of the opposing population, see Fig.  13.5a, and all-vs-all, each adversary competes against all members of the opposing population, see Fig.  13.5b. One-vs-onehas minimal fitness evaluations1(O(N)) and strong competition bias. In contrast, all-vs-allhas a quadratic number of fitness evaluations, thus a higher computational cost, O(N) but reduced competition bias[97]. In [79], adversaries are placed on a mmgrid with a fixed neighborhood (size s). The structure is competition among all competitors in the neighborhood. Fitness evaluations are reduced to by this. An individual’s fitness is calculated over all engagements. At each generation, an individual is assigned a fitness score derived from some function of its performance in the competitions. The fitness assignment is a solution concept[88]. Solution concepts include best worst case, maximization of expected utility, Nash equilibrium, and Pareto optimality.
Fig. 13.5
Competition structures between adversaries/competitors
An example of an alternating competitive coevolutionary algorithm   [6] is shown in Algorithm 1 (Fig.  13.6). The adversaries evolve in an alternating manner. First, the adversaries are initialized. Then, at each generation, a new test population, , is selected, mutated, recombined, and evaluated against the current solution population, . Based on the evaluation the tests are replaced. Then, control reverts to the solution population so it can evolve.
Fig. 13.6
Algorithm 1
The interactive aspect of fitness implies that there is no exactfitness measure for a competitive coevolutionary algorithm, but the fitness value depends on the adversary. In coevolutionary algorithm s two members of the same population, during selection, can be imprecisely compared when they did not compete against the same opponents. In addition, regardless of the function that computes a fitness score, an adversary’s score may change when it competes against different opponents. These properties imply that the ranking individuals is a noisy estimate. This estimation can lead to competitive coevolutionary algorithms pathologiesthat limit the effectiveness when modeling an arms race  [88]. Furthermore, pathologiescan arise from competitive imbalance, particularly when the behavior space of one population is not the same as that of the other  [84]. competitive coevolutionary algorithm pathologiesinclude
disengagement—occurring when opponents are not challenging enough or too challenging  [25],
cyclic dynamics—generally appearing in transitive domains (A beats B, B beats C, and C beats A)  [57],
focusingor overspecialization—arising when an adversary evolves to beat only some of its opponents  [22], and
coevolutionary forgetting—occurs when the ability to beat an opponent evolves away  [20].
In order to remediate these pathologiesproposed methods center on archives/memory, maintaining sub-optimal individuals in the population, using a neighborhood spatial structure, and/or monitoring the progress of the algorithm  [18, 20, 22, 25, 44, 57, 60, 115].
13.2.3 Related Work
This section first provides an overview of some of the applications of AEL. Then, it focuses on different variations of EC-based GANs and their applications.
AEL in the form of CCA has been used for search, optimization, design, and modeling problems  [77, 88, 90, 97]. Some example domains are
Games, encompassing board games  [87], video game playing  [58, 98], and social science games  [10].
Search-based software engineering[1, 3, 39]. For example, there has been use of competitive coevolutionary algorithms to coevolve programs and unit tests from their specification  [5, 6, 114]. Another study used coevolution to distinguish correct behavior from incorrect  [15].
Security, in particular, cybersecurity  [56, 59, 75, 85, 93, 96]. Other examples are vulnerability testing for malware in mobile applications using coevolution appears in [21]. More examples are  [47, 55, 99].
Regarding AEL applied to GAN training, Evolutionary GAN (EGAN) uses multiple loss functions, each associated with a generatorand evolves a population of generators against one discriminatorby selecting the best generator  [110]. EGANs introduce diversitythrough variation with mutation. Another example is the COEGAN  [31–33] approach which evolves the neural network architectures for Generatorand Discriminator andthen uses coevolution to train the adversarial components of GANs. The t-SNE visualization showed that COEGAN evolves generatorsand discriminatorstoward higher quality models  [36]. Furthermore, COEGAN was extended by using skill rating, a game-based fitness function, to assess the individuals  [35].
In addition, different variants of EC have also been used. For example, the expression-based evolutionary GAN was explored. The main idea was to replace the generatorwith a population of expressions and evolve them by using GP  [11]. Other works extend EGAN to multi-objective evolutionary GAN based on NSGA-II[14]. Another multi-objective approach is using quality-diversitysearch with COEGAN based on Novelty Searchwith Local Competition (NSLC)  [34]. Furthermore, differential evolution has been used with GANs (DEGAN) to refine the input of the generatorof a Wasserstein GAN  [121]. Finally, a cooperative coevolutionary algorithm has been proposed to conduct adversarial multi-objective optimization by incorporating dual evolution with respect to the generatorsand discriminators  [27].
There are many investigations of using and improving EC for GANs. For example, the application of GA to evolve fake samples to make them more realistic to train thediscriminator  [28]. A variation of COEGAN, named COEGAN-v2, which applies new variation operators by including spectral normalization and upsampling layers and has RaSGAN loss functionfor training and fitness evaluation  [37]. An extension of EGAN, which introduces a new metric computed by the discriminatorto assess the realness of a sample used to mutate the individuals  [81]. Another variation of EGAN, which uses ES to evolve a population of generatorsthat apply an SGD-based mutation operator  [68]. EvolGAN  [92], which does not modify the training phase, instead evolutionary latent spacesearch based on a quality estimator metric is applied to improve the quality of the produced images when the generatoris trained on small, difficult datasets, or both. And AEGAN, which is an extension of EGAN that embeds a normalized self-attention mechanism in the discriminatorand generatorto adaptively assign weights according to the importance of the samples’ features  [116]. Other examples are based on partial transfer learningfor improving the speed  [71], crossover-based knowledge distillation  [66], and both knowledge distillation and transfer learning  [72].
Regarding the use of evolutionary GANs, a common domain of application is computer vision (i.e, dealing with images). There has been work on evolutionary generative adversarial networksfor hyperspectral image classification[13], abnormal electrocardiogram classification  [111], data augmentationfor cardiac magnetic resonance image  [46], and COVID-19infection segmentation  [53, 103]. More general image applications are face image inpainting with evolutionary generators[52], and evolving noise injection in super-resolution GANs  [91]. Furthermore, there have also been work with evolutionary GANs in other domains. For example, composite evolutionary GAN for natural language generation  [100]. Another example is category-aware GAN with hierarchical evolutionary learning fortext generation  [70]. Evolving GANs have also been used for sequential data imputation[26]. Finally, EAs have been applied to GAN to evolve the architectures of the generatorsand discriminators  [38, 48, 117].
None of the AEL methods previously discussed have studied spatial coevolution. Here, we focus on the impact of diversity, at population, hyper-parameters, operators and data and objectives, that is provided by this technique. In the next section, we describe the spatial coevolutionary GAN training method applied by Lipizzaner
13.3 Method
In this section, we consolidate the description of AEL with spatial coevolution applying the Lipizzanerframework and its variants. Lipizzaneris used for GAN training and promotes diversitywith a population, topology, operators, data and objectives. The key method components for Lipizzanerare
Gradient-based learning to update the neural networks’ parameters of the generatorg, discriminator d, andlearning rate 
Gaussian-based mutations to update the learning rate and weight ensemble parameters 
Evolutionary selection and replacement to maintain convergence.
A topology with overlapping cells between neighborhoods , with sub-populations of generatorsand discriminators in the neighborhoods.
The spatial topology for GAN training is described in Sect.  13.3.1. The algorithmic description of spatially distributed GAN training is in Sect.  13.3.2. Finally, the implementation of spatially distributed GAN training with Lipizzaneris described in Sect.  13.3.3
13.3.1 Spatially Distributed GAN Training
Lipizzaneruses von Neumann neighborhoods with radius  (overlap) 1, see Sect.  13.2.1.4. This defines the migration policy(i.e., the directionality of signal propagation)through the cells in four directions. Figure  13.4a shows an example of a 2D toroidal grid with (44 grid). The shaded areas illustrate the overlapping neighborhoods of the (1, 1) and (0, 2) cells, with dotted and solid outlines. The updates in the centerof (1, 1) will be propagated to the (0, 1), (2, 1), (1, 0), and (1, 2) cells.
In the ringtopology, the cells are distributed in a one-dimensional grid of size and neighbors are sideways, e.g., left and/or right, i.e., an index position or more away. The best GAN (center) after an evolutionary epoch at a cell is updated. Neighborhood cells retrieve this update when they refresh their sub-population at the end of their training epochs, carrying signals in two directions around the ring. Figure  13.4b shows populations of six individuals () organized in a ring topology with neighborhood radius one (). The shaded areas illustrate the overlapping neighborhoods of the cells (0) and (4), with dotted and solid outlines, respectively. The updates in the centerof (0) will be propagated to the (5) and (1). Next, we provide a more detailed algorithmic description of the AEL Lipizzanermethod.
13.3.2 Algorithmic Description of Lipizzaner
Lipizzanerstarts with Algorithm2(Fig.  13.7)::Lipizzanerwhich runs in parallel on each cell n. Each cell neighborhood randomly gets a dataset and initializes singlediscriminator dandgeneratorgneural network models (we refer to these as models when discriminators and generatorsare interchangeable). Thus there are two global adversarial populations of the same size as the topology N, a population of generators and a population of discriminators. These populations are a source of diversityin Lipizzaner, occurring in the representation space, genome space. Each cell nalso initializes its evolvable parameters—learning rate and mixture weights 
Lipizzanerat the highest level searches for and returns a mixture of generatorscomposed from a neighborhood Algorithm3(Fig.  13.8)::MixtureEAevolves a mixture weight vector for each neighborhood using an ES-(1+1) algorithm which optimizes for generatorensemble performance q, e.g., Frechet inception distance (FID)and the inception score(IS)  [19].
Fig. 13.7
Algorithm 2
Fig. 13.8
Algorithm 3
Algorithm2(Fig.  13.7)::Lipizzanerdirects each cell executing in parallel to iterate over a generations  (epochs). In each generation t, a cell, , starts by copying the latest versions of its neighbors to set up its sub-populations and . Next, each cell independently executes Algorithm4(Fig.  13.9)::CoevolveAndTrainModels. This algorithm terminates with the cell neighborhood updating its own models, i.e., , . Two key steps for information propagation are: the update at the start of a generation twhen a cell nforms a sub-population by copying models from its neighbors. The replacement of the best models in the cell after application of coevolution within the sub-population of the cell.
Upon each cell returning its sub-populations to Algorithm2(Fig.  13.7)::Lipizzaner, the mixture weights are evolved. After all generations or a computational deadline is reached Algorithm2(Fig.  13.7)::Lipizzanerselects the best performing ensemble across the entire topology and returns them as Lipizzaner’s solution.
Fig. 13.9
Algorithm 4
Selection promotes fitter models over less fit ones when updating a sub-population Lipizzaneruses tournament selection, Algorithm4(Fig.  13.9)::CoevolveAndTrainModelsline 3. First, the latest neighbors models are copied to form the sub-populations and then selection is applied. After all GAN training is completed and all models are reevaluated, the generatorand discriminatorwith the best training loss replace the worst in the sub-populations,   Algorithm4(Fig.  13.9)::CoevolveAndTrainModelsline  17.
The models’ performance in sub-populations depends on its adversary, see Algorithm5(Fig.  13.10)::EvaluateGANPairs. Calculated with some loss functionMthe performance is expected to vary. The fitness of a model (or ) is based on a solution concept, , i.e., the “best worst case” against all its adversaries.
Fig. 13.10
Algorithm 5
