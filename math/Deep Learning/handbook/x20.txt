5.6.2 Offline and Online Program Simplification
GPSR models areprone to contain many redundant branches. Although redundant branches can help protect the truly useful branches from being destroyed during evolution [71], they inevitably decrease the interpretabilityof GPSR models. Thus, it is desirable to remove the redundant branches to simplify GPSR models and make them more interpretable without affecting their performance. Following the principle of Occam’s Razor, simplification typically generates smaller-sized variants of complex GP solutions. It can work in an offline mode on the final GP solution only or in an online mode that may alter the GP search process and lead to different sets of final solutions.
Naoki et al. [57] propose an offline simplification method, Equivalent Decision Simplification, for GPSR. The simplification method works by replacing subtrees with some predefined simpler subtrees if they are equivalent. They determine equivalence between specific simple trees and a subtree based on whether they yield numerically equivalent values over a range of inputs. Zhang et al. [87] develop an online simplification method based on algebraic simplification rules, algebraic equivalence and prime number techniques to simplify GP programs. When applying to an appropriate proportion of individuals at a certain frequency, the method has been shown to significantly reduce the size while enhancing the performance of GPSR. Kinzett et al. [41] develop a numerical simplification algorithm, which calculates the numerical contribution of a node to its parents. The subtree rooted at the node will be deleted if its contribution is below a user-defined threshold. In addition, a node will be replaced by a constant if its variance is less than a threshold. Rockett [65] develops a statistical permutation test-based pruning method for GPSR. They use a constant to replace the redundant subtrees, the removal of which does not statistically significantly change the output of the parent tree. They have observed around a 20% deduction in the median tree size of GPSR models.
5.6.3 Feature Selection
In explainable AI, a goodexplanation is often selected, i.e. instead of a complete set of causes of an event, humans are more interested in a subset of the more relevant causes [55]. This explanation can be achieved by finding the most informative features via feature selection. GP hasa built-in feature selectionability that has been widely used in various tasks.
Feature selectioncan be achieved by feature rankingmethods that generates a ranking of all features in terms of how important they are in modelling the output variable, then choosing the top features. Most existing feature rankingmethods ignore interactions between features, leading to redundancy in the obtained feature subset. GP implicitly considers interactions between features when performing feature ranking. The importance of features can be simply measured based on the frequency of a feature appearing in good GP individuals [83]. Vladislavleva et al. [83] define two feature importancemeasures, which are the presence-weighted importance and the fitness-weighted importance. The presence-weighted method analyses the feature presence rate in a subset of selected models from the archiveof Pareto GP, the fitness-weighted method obtains the feature importanceby summing up the relative fitness of features in all the models. Chen et al. [20] develop a multi-stage GPSR process where at the first stage distinct features are collected from the top solutions at each generation and at the later stage only top solutions and the candidate features collected from the first stage are accepted for further features selection and modelling for SR. Smits et al. [67] measure the importance of features that appear in the best GP solutions in a multi-objective GP method. The presented features get their importance as the equally distributed fitness of the best solutions.
However, the average distribution of fitness values has the limitation that features presented in the inactive chunks of code can still obtain some credit as being a part of the best solutions. To avoid this limitation, thepermutation-based feature importancemeasures have been proposed for GPSR [26, 34]. Permuting the values of features refers to rearranging the values of features randomly within the dataset, e.g. given the values of a feature as {7,6,9}, the permutation of the feature can take a random form among {6,9,7}, {6,7,9}, {7,6,9}, {7,9,6}, {9,7,6}, {9,6,7}. The underlying assumption of permutation importance is that important features should contribute more on model fitness, thus, permuting a more important feature will lead to a worse regression performance. Aldeia et al. [11] employ thepartial effect/partial derivative, which measures the importance of the feature by how a discrete change of its value affects the target when its co-variables are with fixed values, for measuring the feature importancein GPSR.
5.7 LCSs for Symbolic Modelling
Symbolic representations have the notable benefit of the expressive power to represent complex relationships between the inputs and the prediction. Naqvi et al. [58] further extend XCSRCFA to develop XCS-SRwhich is the very first LCSmethod for symbolic regression. Instead of using a binary tree representing the code fragment for actions, the classifier in XCS-SRuses a GP-style tree to represent a symbolic function. XCS-SRhas been shown to outperform a semantic GPmethod on a number of benchmark SR tasks. They own the advantages of XCS-SRto the flexibilityof multiple solutions each of which is the collective nature of the multiple rules in LCSand the limited tree depth which generates the bloat-free classifiers.
5.8 Summary
This chapter presented an introduction and an overview of a number of EC methods for regression and modelling including coefficient learning in traditional regression and symbolic regression(SR). We have discussed a broad range of research works in this field from the classic methods to the very recent research efforts on EC for regression/symbolic regression. We have highlighted the advancements of these population-based methods over traditional least-squared-based methods for regression. Meanwhile, we also identified the main challenges and some potential solutions in these research areas. It is worth highlighting, among many cutting-edge data-driven modelling methods, great attraction could be with EC (mostly GP) for SR, particularly with limited or even no available domain knowledge. EC for SR provides analytic/symbolic models/equations, thereby supporting interpretable decision-making and knowledge discovery, and delivering scientific insights
However, there are also a number of open issues in EC for SR that deserve future research and further investigation including but not limited to the efficiency and the scalability of evolutionary search, generalisation ability of evolved SR model and constant/coefficient optimisation for evolved SR models. Several representations have been proposed for evolutionary algorithms to effectively search entire solution space for regression. However, future research should investigate the new representations that could lead to effective search in EC for SR. In addition, research on modularityin EC may be a promising direction to improve the generalisation and scalability of EC for SR when tackling complex regression problems. Moreover, gradient-based coefficient optimisation approaches could be efficient and discover accurate values for coefficients of a regression model learned during evolutionary search.
References
1.
Al-Helali, B.: Genetic programming for symbolic regression on incomplete data. PhD thesis, Open Access Te Herenga Waka-Victoria University of Wellington (2021)
2.
Al-Helali, B., Chen, Q., Xue, B., Zhang, M.: A hybrid GP-KNN imputation for symbolic regression with missing values. In: Australasian Joint Conference on Artificial Intelligence, pp. 345–357. Springer (2018)
3.
Al-Helali, B., Chen, Q., Xue, B., Zhang, M.: Genetic programming-based simultaneous feature selection and imputation for symbolic regression with incomplete data. In: Asian Conference on Pattern Recognition, pp. 566–579. Springer (2019)
4.
Al-Helali, B., Chen, Q., Xue, B., Zhang, M.: GP-based feature selection and weighted KNN-based instance selection for symbolic regression with incomplete data. In: 2020 IEEE Symposium Series on Computational Intelligence (SSCI), pp. 905–912 (2020)
5.
Al-Helali, B., Chen, Q., Xue, B., Zhang, M.: Hessian complexity measure for genetic programming-based imputation predictor selection in symbolic regression with incomplete data. In: European Conference on Genetic Programming (EuroGP), pp. 1–17. Springer (2020)
6.
Al-Helali, B., Chen, Q., Xue, B., Zhang, M.: Multi-tree genetic programming-based transformation for transfer learning in symbolic regression with highly incomplete data. In: 2020 IEEE Congress on Evolutionary Computation (CEC), pp. 1–8. IEEE (2020)
7.
Al-Helali, B., Chen, Q., Xue, B., Zhang, M.: Multi-tree genetic programming for feature construction-based domain adaptation in symbolic regression with incomplete data. In: Proceedings of the 2020 Genetic and Evolutionary Computation Conference, pp. 913–921 (2020)
8.
Al-Helali, B., Chen, Q., Xue, B., Zhang, M.: GP with a hybrid tree-vector representation for instance selection and symbolic regression on incomplete data. In: 2021 IEEE Congress on Evolutionary Computation (CEC), pp. 604–611. IEEE (2021)
9.
Al-Helali, B., Chen, Q., Xue, B., Zhang, M.: Multitree genetic programming with new operators for transfer learning in symbolic regression with incomplete data. IEEE Trans. Evol. Comput. 25(6), 1049–1063 (2021)Crossref
10.
Al-Helali, B., Chen, Q., Xue, B., Zhang, M.: A new imputation method based on genetic programming and weighted KNN for symbolic regression with incomplete data. Soft. Comput. 25(8), 5993–6012 (2021)Crossref
11.
Aldeia, G.S.I., de  França, F.O.: Measuring feature importance of symbolic regression models using partial effects. In: Proceedings of the Genetic and Evolutionary Computation Conference, pp. 750–758 (2021)
12.
Amari, S.-I., Wu, S.: Improving support vector machine classifiers by modifying kernel functions. Neural Netw. 12(6), 783–789 (1999)Crossref
13.
Arnaldo, I.,  Krawiec, K., O’Reilly, U.-M.: Multiple regression genetic programming. In: Proceedings of the 2014 Annual Conference on Genetic and Evolutionary Computation (GECCO), pp. 879–886 (2014)
14.
Banzhaf, W., Nordin, P., Keller, R.E., Francone, F.D.: Genetic Programming–An Introduction: On the Automatic Evolution of Computer Programs and Its Applications. dpunkt-Verlag and Morgan Kaufmann Publishers Inc, San Francisco, California (1998)
15.
Banzhaf, W., Nordin, P., Keller, R.E., Francone, F.D.: Genetic Programming: An Introduction: On the Automatic Evolution of Computer Programs and Its Applications. Morgan Kaufmann Publishers Inc., San Francisco (1998)zbMATH
16.
Brameier, M., Banzhaf, W.: A comparison of linear genetic programming and neural networks in medical data mining. IEEE Trans. Evol. Comput. 5(1), 17–26 (2001)CrossrefzbMATH
17.
Butz, M.V.: Kernel-based, ellipsoidal conditions in the real-valued xcs classifier system. In: Proceedings of the 7th Annual Conference on Genetic and Evolutionary Computation, pp. 1835–1842 (2005)
18.
Chen, Q.,  Xue, B.: Generalisation in genetic programming for symbolic regression: challenges and future directions. In: Women in Computational Intelligence: Key Advances and Perspectives on Emerging Topics, pp. 281–302. Springer (2022)
19.
Chen, Q.,  Xue, B.,  Mei, Y.,  Zhang, M.: Geometric semantic crossover with an angle-aware mating scheme in genetic programming for symbolic regression. In: European Conference on Genetic Programming (EuroGP), pp. 229–245. Springer (2017)
20.
Chen, Q.,  Xue, B.,  Niu, B.,  Zhang, M.: Improving generalisation of genetic programming for high-dimensional symbolic regression with feature selection. In: 2016 IEEE Congress on Evolutionary Computation (CEC), pp. 3793–3800 (2016)
21.
Chen, Q., Xue, B., Shang, L., Zhang, M.: Improving generalisation of genetic programming for symbolic regression with structural risk minimisation. In: Proceedings of the 18th Annual Conference on Genetic and Evolutionary Computation (GECCO), pp. 709–716. ACM (2016)
22.
Q.  Chen, B.  Xue, and M.  Zhang. Generalisation and domain adaptation in GP with gradient descent for symbolic regression. In: 2015 IEEE Congress on Evolutionary Computation (CEC), pp. 1137–1144, May 2015
23.
Chen, Q., Xue, B., Zhang, M.: Improving generalization of genetic programming for symbolic regression with angle-driven geometric semantic operators. IEEE Trans. Evol. Comput. 23(3), 488–502 (2018)Crossref
24.
Chen, Q., Xue, B., Zhang, M.: Improving symbolic regression based on correlation between residuals and variables. In: Proceedings of the 2020 Genetic and Evolutionary Computation Conference, pp. 922–930 (2020)
25.
Chen, Q., Xue, B., Zhang, M.: Rademacher complexity for enhancing the generalization of genetic programming for symbolic regression. IEEE Trans. Cybern. (2020). https://​doi.​org/​10.​1109/​TCYB.​2020.​3004361Crossref
26.
Chen, Q., Zhang, M., Xue, B.: Feature selection to improve generalization of genetic programming for high-dimensional symbolic regression. IEEE Trans. Evol. Comput. 21(5), 792–806 (2017)Crossref
27.
Chen, Q., Zhang, M., Xue, B.: Structural risk minimization-driven genetic programming for enhancing generalization in symbolic regression. IEEE Trans. Evol. Comput. 23(4), 703–717 (2019)Crossref
28.
Costelloe, D.,  Ryan, C.: On improving generalisation in genetic programming. In: Genetic Programming: 12th European Conference, EuroGP 2009 Tübingen, Germany, April 15-17, 2009 Proceedings 12, pp. 61–72. Springer (2009)
29.
dal Piccol Sotto, L.F., de Melo, V.V.: Studying bloat control and maintenance of effective code in linear genetic programming for symbolic regression. Neurocomputing 180, 79–93 (2016)Crossref
30.
de França, F.O.: A greedy search tree heuristic for symbolic regression. Inf. Sci. 442, 18–32 (2018)MathSciNetCrossrefzbMATH
31.
de França, F.O., Aldeia, G.S.I.: Interaction-transformation evolutionary algorithm for symbolic regression. Evol. Comput. 29(3), 367–390 (2021)Crossref
32.
Dick, G.: Interval arithmetic and interval-aware operators for genetic programming (2017). arXiv preprint arXiv:​1704.​04998
33.
Dick, G.: Revisiting interval arithmetic for regression problems in genetic programming. In: Proceedings of the Genetic and Evolutionary Computation Conference Companion, pp. 129–130 (2017)
34.
Dick, G.: Sensitivity-like analysis for feature selection in genetic programming. In: Proceedings of the 19th Annual Conference on Genetic and Evolutionary Computation (GECCO), pp. 401–408 (2017)
35.
Donders, A.R.T., Van Der Heijden, G.J., Stijnen, T., Moons, K.G.: A gentle introduction to imputation of missing values. J. Clin. Epidemiol. 59(10), 1087–1091 (2006)Crossref
36.
Friedman, J., Hastie, T., Tibshirani, R.: The Elements of Statistical Learning, vol. 1. Springer Series in Statistics, New York (2001)zbMATH
37.
Gulsen, M., Smith, A., Tate, D.: A genetic algorithm approach to curve fitting. Int. J. Prod. Res. 33(7), 1911–1923 (1995)CrossrefzbMATH
38.
Helmuth, T., Spector, L., Matheson, J.: Solving uncompromising problems with lexicase selection. IEEE Trans. Evol. Comput. 19(5), 630–643 (2014)Crossref
39.
Jervase, J.A., Bourdoucen, H., Al-Lawati, A.: Solar cell parameter extraction using genetic algorithms. Meas. Sci. Technol.12(11), 1922 (2001)Crossref
40.
Keijzer, M.: Improving symbolic regression with interval arithmetic and linear scaling. In: Proceedings of the European Conference on Genetic Programming (EuroGP), pp. 70–82. Springer (2003)
41.
Kinzett, D., Zhang, M.,  Johnston, M.: Using numerical simplification to control bloat in genetic programming. In: Asia-Pacific Conference on Simulated Evolution and Learning, pp. 493–502. Springer (2008)
42.
Kommenda, M., Kronberger, G., Affenzeller, M., Winkler, S.M., Burlacu, B.: Evolving simple symbolic regression models by multi-objective genetic programming. In: Genetic Programming Theory and Practice XIII, pp. 1–19. Springer (2016)
43.
Koza, J.R.: Genetic Programming II. Automatic Discovery of Reusable Subprograms. MIT Press, Cambridge (1992)zbMATH
44.
