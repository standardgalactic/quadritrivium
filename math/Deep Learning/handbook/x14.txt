Initially demonstrated under the deceptive maze navigationand bipedal walking tasks, behavioural novelty searchoutperformed objective-based search. However, it was subsequently found that novelty searchalone is most effective when the BC is correlated with the objective function. For example, in maze navigation, characterising an agent’s behaviour by its final position in a maze will naturally imply that novelty searchexplores the objective space of navigating to the maze target location (e.g., [106]). However, when the BC and objective function are decoupled, optimising novelty in combinationwith a measure of behavioural quality is required [38]. To address this, Quality diversity (QD)algorithms elaborateon the ideas of NS, multiple BCs, and goal-switching by keeping track of many different behavioural nichesthat are evolved simultaneously, and in effect trying to discover stepping stones by periodically testing the abilities of agents from one nichein other niches(i.e., goal-switching). If search gets stuck in one goal dimension, it might make progress in other dimensions, and thereby get around deceptive search spaces. A well-known Quality Diversity (QD) approach is the Multidimensional Archiveof Phenotypic Elites (MAP-Elites)algorithm [39, 124]. In [124], Map-Elitesis used to build a diverse repertoire of walking policies for a legged robot. If the robot is damaged, it may use this prior repertoire as a skill library to guide a trial-and-error algorithm that rapidly discovers a behaviour to compensate for the damage. While effective for diversitymaintenance, Map-Elites hasdifficulty scaling because there can be an exponential explosion of nichesas the number BCs (or objective functions) increases. Several EvoRL studies have emerged that mitigate this issue in robotics and control [34, 178]. Finally, building on Map-Elites, Go-Explore [50] specifically addresses incremental explorationfollowing the key ideas of remembering states, returning to them (GO), and exploring from them (Explore). Go-Explore was the first (Evo) RL method to succeed in two Atari game titles with extremely sparse, deceptive reward signals: Montezuma’s Revenge and Pitfall.
Novelty quantifies the degree to which a policy differsfrom prior ways of solving a particular problem. Surpriseis an alternative similaritymeasurement which quantifies the degree to which a policyis different from what is expected, given prior knowledge or intuition about a particular problem environment. Since a prediction of trends that evolution is following from one generation to the next is a necessary component for modelling evolutionary surprise, it can be characterised as a temporal novelty metric. Surprise searchis intended to mimic the self-surprise cognitive process of creativityover the (much longer) evolutionary timescale. In a deceptive maze navigationtask, allowing surprise andnovelty to operate together leads to QD algorithms of higher efficiency, speed, and robustness [64]. Surprise can also be encouraged by rewarding policies for displaying curiosityduring their lifetime. Curiosity-ES [174] rewards policies for discovering transitions that are different from those seen previously. The idea is that curiosity naturally leads to a diversityof policies because reward is provided for covering unexplored or under-explored transitions. Curiosity-ES discovers optimal policies faster than Novelty SearchEvolutionary Strategies [36] and, unlike novelty search, no BCs are required.
Finally, the evolvability of a system refers to the system’s potential to continuously discover new organisms with higher fitness than their parents. Novelty searchleads indirectly to evolvability by favouring individuals that are very mobile in the behaviour space, and thus highly evolvable [47]. Evolvability searchaims to optimise this directly by calculating the behavioural diversityof each individual’s immediate offspring and selecting for organisms with increased offspring variation. While this process is computationally expensive, it discovers successful policies faster than novelty search inthe deceptive maze navigationtask.
4.4.1.2 Competitive Coevolution
Competitive coevolution simultaneouslysearches for policies and training cases (e.g., environment configurations), coupling their fitness such that training cases are encouraged to challenge and discriminate between the performance of policies without resulting in disengagement [31]. The paradigm is akin to predator/prey or host/parasite relationships in nature. It has been used extensively in EvoRL to drive explorationby coevolving policies with problem environments (e.g., opponent behaviours in games) [27, 129, 135, 147, 165, 166]. Recently, the PairedOpen-Ended Trailblazer (POET)[183] combines NS, QD, and goal-switching to coevolve an open-ended progression of diverse and increasingly challenging environments together with agents capable of solving them. POETis evaluated in a 2D obstacle course domain, where it ultimately generates and solves diverse and challenging environments, many of which cannot be solved by direct RL, or a hand-designed curriculum tailored for each generated environment.
If the goal of competitive coevolutionis to build multiple policies that will eventually form a group behaviour, credit assignmentwill need to strike a balance between rewarding generalists (policies that solve the multiple training cases) versus specialists (those that solve specific, typically more challenging training cases). Depending on the representation for agents, it can be exceedingly challenging to resolve the two into a single policy[32].
4.4.2 Credit Assignment
The(temporal) credit assignment problem is addressed differently by step-based and episode-based learningalgorithms [14, 121].
Episode-based learningtakes place in EvoRL in the outer loop of Fig. 4.1. In this case, decision-level credit is applied implicitly since policies that make better decisions will receive higher cumulative reward (hence higher fitness) and produce more offspring. Thus, evolution manages the temporal credit assignment problemby directing the search in favour of agents that make decisions that contribute to a positive overall outcome.
Step-based (i.e., lifetime) learning can be characterised as explicitly rewarding agents for their ability to predict future states of the environment. Such forward-prediction is an indispensable quality in nonstationary and partially observable RL environments (Sects. 4.6and 4.5). However, the ability of brains to model the world emerged from evolution because it helped organisms survive and thrive in their particular environments, not because forward-prediction was explicitly selected. We can characterise any dynamic behaviour as lifetime learning—that is, a behaviour that undergoes change during its lifetime (i.e., within a single episode) in response to environmental interaction. For example, some GP models use temporal memory mechanisms to allow information acquisition from their environments while they solve problems, and to change their future behaviour in response to such information [90, 130], or even to reprogram themselves on-the-fly [99]. Importantly, this change need not be directed by gradient information available from a stepwise reward signal. Nonetheless, several EvoRL methods combine evolution and learning by leveraging such lifetime reward gradient information to address two primary objectives: (1) Improve unstable learning and brittle convergence properties that are extremely sensitive to hyperparameters, commonly associated with (gradient-based) Deep RL [117]; and (2) Improve the sample efficiency of learning.
4.4.2.1 Lifetime Reward Gradients in EvoRL
There are various ways to resolve the trade-off between individual and group credit assignmentby combining the episode-based and step-based approaches. Group-level selection can be used to encourage group-level performance, by rewarding the entire group by its combined success and replicating together [150]. However, group-level selection emphasises the group over the individual. Other interesting proposals exist that shape rewards in a more detailed way to balance group and individual, such as the Shapley Q-value approach that considers the magnitude of individual contributions [182], or the Minimum-Reward Attribution approach that rewards a group by the poorest performer to achieve an implicit balance [142].
OpenAI solved this another way. For their OpenAI Five multiplayer Dota bot, there was shaping in two ways: behavioural shaping and reward shaping. The five sub-players were constrained to stay within their spatial regions of the game for the first portion of the match and then allowed to venture for the remainder of the match. This had the effect of encouraging self-sufficiency and mitigating early poor choices. The teamof five sub-players was also encouraged to operate first independently, then cooperatively, through reward shaping. This was another way to reward cooperation, but to first ensure self-sufficiency had been established [17].
Other examples of EvoRL using lifetime reward gradients include the recently proposed Evolution-Guided PolicyGradient in Reinforcement Learning (ERL) algorithm [91]. In ERL, parameters learned via lifetime gradient descent are periodically inserted into a population of evolving neural networks, leading to faster learning (i.e., improved sample efficiency). CEM-RL [136] combines the cross-entropymethod (CEM) with Twin Delayed Deep Deterministic policygradient (TD3). In this case, policyparameters (weights) learned through stepwise reward gradients are directly applied to half of the CEM agents at each iteration to increase training/sample efficiency. Asynchronous Evolution Strategy-Reinforcement Learning (AES-RL) [104] integrates ES with off-policy gradient RL methods to take advantage of the stability of ES while improving sample efficiency. Evolution-based Soft Actor-Critic (ESAC) [162] combines ES with Soft Actor-Critic (SAC) to explore policies in weight space using evolution and exploit gradient-based knowledge using the SAC framework. The method achieves greater rewards in several MuJoCo and DeepMind control suite tasks compared to SAC, TD3 PPO, and ES while producing stable policies and reducing the average wall-clock time for training compared with ES on equivalent hardware.
Enrico et al. [113] and Shibei et al. [196] both demonstrate how efficiency and stability gains of combining evolutionary policysearch with gradient descent can be extended to on-policy RL. Supe-RL [113] periodically generates a set of children by adding Gaussian mutation to the gradient-learned policy weights, and then gradually updating (soft updating) the on-policy learner with weights from the best child. Soft updating integrates evolutionary weight changes slowly, greatly improving the stability of learning while maintaining efficiency gains. Supe-RL has outperformed ERL and PPO across a variety of MujoCo domains with continuous action spaces, some of which use raw pixels for observations
All of these methods manage to capture strengths from evolutionary and gradient-based methods: evolution improves the stability of learning by mitigating the temporal credit assignmentissue while gradient-based learning increases the speed of convergence by reducing the number of policy-environment interaction samples (See [145] for a comprehensive review of algorithms combining evolution with gradient-base deep RL). However, it is not clear a rewardsignalwould be available to the policy when deployed in the field, outside of the semi-supervisedRL training environment. As such, these methods do not directly address a critical benefit of lifetime learning: the ability to continuously adapt to novel environments that are not known at training or search time [100]. Learning of this nature is discussed in Sect. 4.6
4.4.2.2 Gradient-Free EvoRL and Cooperative Coevolution
EvoRLis increasingly exploited as a general framework for building agents that can learn to predict what is important for their survival withoutbeing explicitly rewarded for it by tracking reward signalgradients during their lifetime. From the perspective of credit assignment, modularity(behavioural and structural) and coevolution are recurring themes in order to incrementally construct heterogenous models from multiple coevolving subsystems. Three aspects of the system are simultaneously managed by coevolution: (1) Automatic decomposition of the problem into multiple simpler subtasks or smaller search spaces: (2) Development of partial solutions, or specialistmodels that solve one or more subtasks, each representing a partial solver relative to the ultimate objective; (3) Crediting the behavioural contributions of the different specialist components to the ultimate success of the heterogeneous (multi-model) agent. The methodology is synonymous with compositional evolutiondiscussed in Sect. 4.3, but here we discuss its implication on credit assignmentinRL.
In natural ecosystems, organisms of diverse species compete and cooperate with different species in their quest for survival. The fitness of each individual organism is coupled to that of other individuals inhabiting the environment. As species evolve they specialise and co-adapt their survival strategies to those of other species. This phenomenon of coevolution has been used to encourage complex behaviours in EAs and has particular relevance to credit assignmentin RL tasks. Species represent policycomponents, or partial world models, that decompose the problem in time and space. Each individual forms a part of a complete problem solver but need not represent meaningful behaviour individually. Cooperative coevolutionimplies that the components are evolved by measuring their contribution to complete problem solvers, and recombining those that are most beneficial to solving the task [61].
Symbiotic Adaptive Neuro-Evolution (SANE) demonstrates how a population of individual neurons can be coevolved to form a fully functional neural network [120]. SANE coevolves two different populations simultaneously: a population of neurons and a population of network blueprints that specify how the neurons are combined to form complete networks. In this system, neurons assume diverse (specialised) roles that partially overlap, resulting in a robust encoding of control behaviour in the Khepera robot simulator. SANE is more efficient, more adaptive, and maintains higher levels of diversitythan the previous network-based population approaches. A critical aspect of SANE’s neurons (and modular, many-model systems in general) is that each neuron forms a complete input to output mapping, which makes every neuron a primitive problem solver in its own right. Such systems can thus form subsumption-style architectures [29], where certain neurons provide basic functionality and other neurons perform higher level functions that fix or build on basic partial solvers. Cooperative Synapse Neuroevolution (CoSyNE) [61] uses cooperative coevolutionat the level of individual synaptic weights. For each network connection, there is a separate subpopulation consisting of real-valued weights. Networks are constructed by selecting one member from each subpopulation and plugging them into a predefined network topology. In a challenging double pole balancing task with properties including continuous state and action spaces, partial-observability, and non-linearity, CoSyNE is compared with a wide variety of step-based RL methods (including PolicyGradient RL, Q-learning with MLP, SARSA(), and others) and episode-based methods (including NEAT, SANE, ESP, CMA-ES, and others). It was found that episodic methods in general—particularly coevolutionary search with CoSyNE—can solve these problems much more reliably and with fewer episodic evaluations than step-based RL approaches. The authors suggest this is because “the networks can [much more] compactly represent arbitrary temporal, non-linear mappings” and that the CoSyNE algorithm aggressively explores weight combinations. Though, they note that aggressive explorationmay be detrimental for larger networks.
In the context of multi-agent robot control, [4, 94] present a method for providing local evaluation functions to directly evolve individual components in a coevolutionary system. They address how individual evaluation functions are selected so that the components evolve to optimise those evaluation functions anddisplay coordinated behaviour. Design considerations are discussed for component evaluation functions which are aligned with the global task objective andsensitive to the fitness changes of specialised components, while remaining relatively insensitive to fitness changes of other components in the system. More recently, population-based training (PBT) has been used to automatically learn internal rewards for teams of (step-based) reinforcement learners in a 3D multiplayer first-person video game [79], and automatically optimise the relative importance between a set of shaping rewards for the multi-agent MuJoCo soccer game [111]. Both methods aim to align the myopic shaping rewards with the sparse long-horizon teamrewards and generate cooperative behaviours. However, it is noted that variance stemming from temporal credit assignmentin RL updates remains an open issue.
The Symbiotic Bid-Based GP (SBB) [48] explicitly addresses credit assignmentin RL by coevolving teams of programs in which each individual program represents an action-value function for a particular discrete, task-specific action. SBB uses symbiotic coevolution to build teams (host) of diverse programs (symbiont), each of which models the contextin which a given action is appropriate, relative to the observable environmental state . Inter-program bidding determines which program will “win” the right to define the policy’saction at each timestep. The teamas a whole thus represents a collective decision-making organism. Teams (policies) are built through episode-based RL in which fitness is only assigned at the team level. Mutation operators may add and remove programs from a team. Programs may appear in multiple teams, but individual programs that are left with no teammemberships are permanently deleted. In this way, programs that specialise in some part of the problem (i.e., a particular region of the state space or a particular temporal region in the task dynamics within an episode) will be protected as long as they are a member of a team that achieves high fitness. In order words, selective pressure at the phylogenetic level effectively performs temporal credit assignmentfor individual components without needing to assign credit or blame to components directly. Explicit diversitymaintenance, care of fitness sharing [58], is critical to building teams with diverse/robust program complements. SBB’s team-based approach to problem decomposition and credit assignment is the basis for tangled program graphs, which support highly efficient hierarchical RLpolicies, discussed for visual RL and nonstationary, multi-task RL in Sects. 4.3and 4.6
More recently, cooperative coevolutionhas been employed in Deep RL frameworks to construct and manage the training of the components in modular, heterogeneous network architectures [67, 138]. Deep Innovation Protection (DIP) addresses the credit assignmentproblem in training complex heterogeneous neural network models end-to-end. DIP uses a multi-component network consisting of a vision component, memory component (LSTM), and controllercomponent that is trained end-to-end with a simple EA. Mutations add Gaussian noise to the parameter vectors of the networks. An age marker keeps track of mutations to the visual or memory systems, allowing the framework to automatically throttle selection pressure on specific components that have recently been subject to mutation. The main insightis that when components upstream in the network change, such as the visual or memory system in a policy, components downstream need time to adapt to changes in those learned representations. In the VizDoom Take Covertask, DIP policies predict upcoming events in their environment, such as whether an approaching fireball would hit the agent, without a specific forward-prediction loss derived from reward gradients.
4.4.3 Discussion and Open Challenges
Population-based learning over multiple timescales (evolutionary and lifetime learning) allows EvoRL to effectively address the temporal credit assignmentproblem in RL through coevolution. First, evolutionary search effectively performs multiple parallel searches, as opposed to optimising a single model, and generally provides more explorationthan gradient-decent methods [115]. Second, the distributed nature of population-based search allows the system as a whole to decompose a problem in both time and space, finding a diverse set of partial solutions. Third, cooperative coevolutionary algorithms automatically recombine partial solutions to form composite policies. In doing so, behaviours that are specialised for particular spatial and temporal regions of an RL problem can be identified (credited) and exploited within a generalised problem solver. Finally, lifetime learning—with or without the use of step-based reward gradients—is fully supported in EvoRL, often improves efficiency, and is essential in many cases (e.g., nonstationary or partially observable environments). Multiple parallel algorithms drive evolutionary learning (e.g., competitive and cooperative coevolution, quality and diversitysearch). Likewise, multiple lifetimeRL systems are active in the brain. For example, different regions perform model-free and model-based control, allowing learners to switch between rapid action selection from action-values (learned via TD with dopamine signals conveying reward predictions) vs. slower (but more accurate) forward simulation of the consequences of actions using a learned model of the environment [41, 128]. Each parallel system has unique implications for temporal credit assignment. Future EvoRL research should consider how multiple parallel action inference and lifetime learning systems, operating at manytimescales, will interact with evolutionary policydevelopment.
4.5 Partial Observability in Space and Time
RL policies observetheir environment through a sensory interface that provides a set of state variablesat each timestep t, . In many cases, these observations do not contain complete information required to determine the best action, i.e., the environment is only partially observable. For example, consider a maze navigationtask in which does not contain a global map of the maze. This is the case in many robotics tasks or in first-person Real-Time Strategy (RTS) video games. In partially observable environments, the policymust identify and store salient features of in memory over time, encoding a representation of the environment that captures spatial and temporal properties of the current state [63]. This implies that activeperception [131] and world modellingform a significant aspect of behaviour: constructing and managing a representation of the environment in memory. This is an example of automatic model-basedRL, i.e., the world modelis constructed on-the-fly. This property distinguishes policies with memory from purely reactive, model-freepolicies that define a direct association from sensation to action without any internal representation of state, and thus no temporal integration of experience over time. Interestingly, operation in partially observable environments also places more importance on lifetimeexplorationvs. exploitation because policies must now explore enough of the environment to gain a breadth of experience (and possibly build an internal model), while also selecting actions that satisfy their objective.
A policyrepresentation may incorporate temporal memory in at least two ways: (1) Recursive structure; and (2) External data structures that support dynamic read/write access. The two memory models differ in their potential for operation in dynamic environments, where it is critical for the memory mechanism to support dynamic access. As discussed in detail in Sect. 4.6, many realistic RL environments are highly dynamic, with transition functions, objectives, and observable properties of the world that change over time. In this case, the memory model must be capable of reading and writing to distinct memory locations at any moment, based on the state of the environment. In effect, the memory model must represent a read-write spatiotemporal problem decomposition, or working memory[89, 193]. Evolution and learning can then solve problems such as how to encode information for later recalland identify what to remember in each situation. In effect, the agent is free to access multiple encoded memories with distinct time delays and integrate this information with to predict the best output for the current situation. Purely recursive memory is potentially limited in this respect. While some memory tasks are enabled by integrating experience over time through feedback, enabling working memory through feedback is nontrivial [102, 195].
Milestone innovations in neuroevolutionsuch as ESP [62], NEAT[156], and CoSyNN [61] established the potential to evolve recurrent networks that solved relatively simple partially observable tasks such as double pole balancing without velocity information. However, the focus of these studies is on neuroevolutionproperties rather than temporal memory models. Recurrent Neural Network (RNN)s have difficulty modelling temporal processes with long time delays because they can leak information over time [73]. Also, it may be difficult to separate the action-prediction functionality of the network from the world-modelling functions [5]. Long Short-Term Memory (LSTM) neural networks introduce specific “gated” connections for explicitly learning what information to keep or forget, and are potentially less susceptible to such issues.
To overcome the difficulty RNNs have in discovering long-term and dynamic temporal dependencies, [137] extend NEATto build LSTM networks in two phases. First, networks are pre-trained (using NEAT)by optimising a novel unsupervised information maximisation (Info-max) objective. LSTM units incrementally capture and store unique, temporally extended features from the environment during pre-training. Second, the networks are evolved to a sequencerecalltask, where they significantly outperform neuroevolutionof RNNs, especially as the temporal depth of the sequence increases. Interestingly, optimising a policy’sinformationcan indirectly lead to exploratory behaviours and resemble the concept of curiosity [141], since the policyis explicitly rewarded for exploring areas in the environment that provide new information (Curiosity is discussed further Sect. 4.7.2in relation to hierarchical RL)
Markov Brains are evolvable neural networks that differ from conventional networks in that each unit or neuron potentially performs a unique computation [71]. As such, they combine elements from GP (i.e., program search) and neuroevolution. In tasks that require temporal memory, Markov Brains develop world modelsthat are localised and sparsely distributed. By contrast, evolved RNNs and LSTM networks tend to smearinformation about the world over the entire network [93]. Markov brains are more robust to sensor noise, possibly due to their distributed world models, however, extensive analysis is now probing the information flow in cognitive and genetic networks to better understand information fragmentationin learned spatiotemporal representations [20, 21].
In GP, functional modularityhas longbeen the focus of approaches attempting to simultaneously build a world modeland define a policyto act by querying the model for prediction. Andre [6] proposes a multi-phase tree-GP algorithm in which each program consists of two branches, one of which will be executed in each phase of the policy’sbehaviour. In the first phase, the policyexplores the world, examining sensory information and possibly reading and writing information to indexed memory, but does not predict action outputs. In the second phase, the agent is blind with respect to the sensor input and must use only its stored representation to produce a multi-step plan of action. The plan is then evaluated and its fitness is determined. Fitness is only evaluated after phase 2, thus phase-specific credit assignmentwas not required. Over multiple evaluations in unique environments, the policylearns a general, modularised (2-phase) memory-prediction policy. A key limitation of this approach is that explicitly decomposing the program representation and evaluation procedure into separate structures and processes for wold-model building and planning implied that the movement in memory is directly tied to movement in the world. With this hard-coding, the generality of the framework is limited because no spatiotemporal compression or abstraction of experience within the memory model is encouraged.
Teller [172] explored a less constrained approach to model-based policies through the use of indexed memoryin GP. In this work, each program includes a memory vector of size which is set to 0 at the start of each episode. Extending the GP function set with parameterised read/write operations allows programs to dynamically access/modify memory by vector indexing as they interact with the environment. Programs in this language, supplemented with indexed memory, can support any data structure and implement any algorithm, i.e., a genetically evolved program in this representation is Turing-complete. However, there is no straightforward heuristic to select the size of , and simply choosing a value so large that memory space is unlikely to saturate removes pressure for evolution to find compact representations for stored information, and again potentially limits the generality of evolved mental models. Furthermore, there is no evidence that this particular representation is an efficientmeans of building memory-prediction machines. To address these issues without resorting to hard representational constraints, subsequent explorationsof temporal memory for GP policies leverage some form of emergentfunctional modularity, discussed next.
Modular temporal memory structures have been explored extensively in TPG, specifically to support efficient policysearch in partially observable visual RL environments such as VizDoom [89] and Dota 2 [151]. An RL policyrepresented by a tangle program graph is essentially a hierarchical network of programs (linear register machines [26]) constructed from the bottom up via compositional evolution (Sect. 4.3). A linear program representation has the advantage that temporal memory can be implemented simply by not clearing memory registers between program execution, thus all programs are stateful. The probabilistic memory framework in [89, 152] extends TPG by adding a global bank of indexed vector memory which is shared by all policies in the population. The motivation behind global shared memory is to encourage all agents to evolve a common/shared concept for what constitutes useful memory content and where to find context-dependent data. Shared memory potentially also aids in explorationsince experiences from multiple unique agents are now communicated/shared globally. Programs are augmented with read/write instructions for memory access. Write operations are probabilistic, with the data being distributed across shared memory to simulate distinct long- and short-term memory regions. However, no explicit functional decomposition is enforced relative to memory management or action prediction, thus each program is free to specialise on one or the other orgeneralise to both. Furthermore, the size of indexed memoryis significantly less than the size of a single observation (screen frame), thus agents are required to select and/or compress relevant information from the observable state into a compact memory model. Empirical evaluation demonstrates that TPG agents have the capacity to develop strategies for solving a multi-faceted navigationproblem that is high dimensional (>76,000 state variables), partially observable, nonstationary, and requires the recallof events from long- and short-term memory. Moreover, the policies are sufficiently general to support navigationunder a previously unseen environment.
Related EvoRL studies explore how to encode temporal memory directly in the tangled program graphstructure [85, 90]. Rather than a global shared memory, each program’s internal statefulmemory bank may be shared by any other program in the samegraph (policy). This removes the need to define parameterised read/write operations or maintain external indexedmemory. Instead, the graph execution now establishes the order in which local banks of memory are read from/written to. Policies in this representation are able to play the Atari breakout video game withoutthe use of frame stacking(i.e., autoregressivestate which sidesteps partial-observability)[85]. In addition, multi-task policies are evolved to solve multiple classic control tasks in which the observation space does not include velocity information, implying partial-observability. Analysing the policy’shierarchical distributed memory reveals accurate modelling of all system velocities, with separate regions of the hierarchycontributing to modelling disjoint tasks (e.g., Cartpole and Pendulum) while similar tasks were modelled in a single region (e.g., Discrete-action Mountain Car and Continuous-action Mountain Car) [90].
4.5.1 Discussion and Open Challenges
Realistic temporal decision and control environments are only partially observable, thus learned memory models are fundamental to cognition and a critical consideration in scaling RL to complex tasks. When exposed to memory-intensive problems, EvoRL has repeatedly shown its capacity for building problem solvers with modularityin their structure and in the nature of information flow from input, through storage, to output. Developing tools to analyse information flow will increase our understanding of this functional modularityand make artificial cognitive systems more interpretable and trustworthy [70]. Analysing how different learning methods (e.g., evolution and gradient descent) differ with respect to constructing memory models (e.g., [69]) will help us leverage these models in hybrid frameworks (e.g., [67]). Furthermore, imagining new ways of evolving memory mechanisms will not only address partial-observability, but also support operation in dynamic environments (e.g., [16]), discussed in the next section.
4.6 Non-stationary and Multi-Task Environments
Autonomous agents deployed in real-world environments will face changing dynamics that require their behaviour to change over time in response. In robotics, dynamic properties such as physical wear-and-tear, as well as characteristics of the external environment such as new terrain or interaction with other agents imply that the system’s transition function, reward function, and observation space potentially change over time. In these situations, a static controllerthat always associates the same observation with the same action is unlikely to be optimal. Real-world agents must be capable of continuously adapting their policyin response to a changing environment. To achieve this capability, the agent must solve two equally important subtasks in real-time: (1) Recognising a change in the environment without an external cue (contains no information about potential changes); and (2) Updating their policyin response. Both tasks must be completed without prior knowledge of the upcoming change, and fast adaptation provides increased reward. Video games are another common example of nonstationary tasks: as the player interacts with the game, non-player-characters may change their behaviour to increase the difficulty of play and new levelsof play are periodically encountered in which the physics of the simulation may abruptly change (e.g., new entities are introduced or the perspective changes from birds-eye-view to first person) [192]. Such changes may require new skills or new internal world models. As such, adaptation and Multi-Task Reinforcement Learning (MTRL) are fundamentally linked. In addition to adaptability, algorithms operating under these conditions must contend with the following challenges [181]: 
1.
Scalability. Jointly learning Ntasks should not take Ntimes longer than learning each task individually, and the resulting multi-task agent should not be Ntimes as complex.
2.
Distraction Dilemma. The magnitude of each task’s reward signalmay be different, causing certain tasks to appear more salient than others.
3.
Catastrophic Forgetting. When learning multiple tasks in sequence, the agent must have a mechanism to avoid unlearning task-specific behaviours that are intermittently important over time.
4.
Negative Transfer. If the environments and objectives are similar, then simultaneously learning multiple tasks might improve the learning or search process through positive inter-task transfer. Conversely, jointly learning multiple dissimilar tasks is likely to make MTRL more difficult than approaching each task individually.
5.
Novel Environmental Dynamics. Agents deployed in many real environments will need to adapt to dynamics not experienced during search/training.
IMPROBED [116] uses an extension of Cartesian Genetic Programming (CGP)to evolve developmental neural networks to solve multi-task problems by alleviating catastrophic forgetting inat least two primary ways: (1) By evolving networks that undergo topological changes during learning and (2) by growing numerous connections between pairs of neurons, thus lessening the influence of individual weighted connections. They point to a large bodyof research indicating that learning and environmental interaction are strongly related to structural changes in neurons, and the most significant period of learning in animals happens in infancy, when the brain is developing. Developmental models are an alternate approach to evolutionary incremental growth(Sect. 4.3), allowing the topology and size of computational networks to emerge rather than having to be predefined. They show how a pair of evolved programs can build a single neural network from which multiple ANNs can be developed, each of which can solve a different computational problem including two RL and two classification problems (incrementally or all at the same time). The principal limitation is that developed networks required a “problem type” input to discriminate between tasks.
TPG can evolve highly generalised policies capable of operating in 6 unique environments from the control literature [90], including OpenAI’s entire Classic Control suite [28]. This requires the policyto support discrete and continuous actions simultaneously. No task-identification inputs are provided, thus policies must identify tasks from the dynamics of state variables alone anddefine control policies for each task. Hierarchical program graphs built through compositional evolution (Sect. 4.3) support multi-task environments through automatic, hierarchical problem decomposition. Policies can recombine multiple generalist and specialist behaviours, and dynamically switch between them at inference time. This allows policies to exploit positive inter-task transferwhen tasks are related, and avoid negative transferbetween disjoint tasks that require specialised behaviours. The resulting policies are competitive with state-of-the-art single-task policies in all 6 environments and can adapt to any task on-the-fly, within a single episode. However in this initial study, it is assumed that populations are exposed to all task dynamics during evolution. The principal open question is how TPGs can be extended to support adaptation to environmental dynamics notexperienced during evolutionary search.
Several approaches based on evolutionary strategies have made progress towards agents that can adapt to unseentasks. SO-CMA [194] employs a transfer methodology that first learns a diverse family of policies simultaneously. In the unseen environment, it searches directly for a policyin the family that performs the best and fine-tunes it further using CMA-ESand a minimal number of samples. SO-CMA can overcome large discrepancies between seen and unseen tasks. ES-MAML [154] trains a meta-policy on a variety of tasks such that agents can adapt to new environment dynamics using only a small number of training samples. The method is demonstrated on nonstationary OpenAI Gym benchmarks. Instance weighted incremental evolutionstrategies (IW-IESs) proposed a method to adjust a previously learned policyto a new one incrementally whenever the environment changes, without any structural assumptions or prior knowledge on the dynamics of the ever changing environment. They incorporate an instance weighting mechanism with ES to facilitate its learning adaptation. During parameter updating, higher weights are assigned to instances that contain more new knowledge, thus encouraging the search distribution to move towards new promising areas of parameter space. IW-IES is able to handle various dynamic environments that change in terms of the reward function, the state transition function, or both in tasks ranging from robot navigationto locomotion. The principal limitation with these ES methods is adaptation speed. While they can all adapt to novel environments, their adaptation is not efficient enough to work online within a single RL episode.
Synaptic plasticity allows natural organisms to adapt to novel situations, and neuroevolutionhas recreated this property in artificial networks to address continual learning in nonstationary environments. [54] evolved neural networks with local synaptic plasticity based on the Hebbian rule, which strengthens the connection proportionally to correlated activation, and compared them to fixed-weight networks in a dynamic evolutionary robotics task. The policies were evolved to turn on a light, and then move to a specified location. Local learning rules helped networks quickly change their policy, transitioning from one task to another. Interestingly, [158] evolved recurrent networks with fixed weights which can also respond differently in changing environmental conditions, and required fewer evaluations than networks evolved with local learning rules. A major question for future research is how to characterise what type of change requires semi-supervised (re)learning, and what types of change require only that the policies be stateful, i.e., capable of storing and/or integrating environmental readings over time. To this end, recent work combining evolution and learning suggests that unsupervised lifetime learning with synaptic plasticity is a more biologically accurate model for learning [144] and will be critical in cases where the agent is required to rapidly adapt to novel, previously unseen dynamics and goals without human intervention[153]. For instance, [126] demonstrated how evolved local learning rules allow a neural network with initially random weights to adapt to unseen environments within a single episode.
4.6.1 Discussion and Open Challenges
Humans routinely adapt to environmental characteristics during our lifetime that are too novel to have influenced evolutionary adaptation (e.g., climate change, technology such as computers, global travel and communication). Rather, evolution through natural selection produced excellent lifelong learningmachines, allowing us to rapidly learn and remember how to solve new problems. EvoRL has great potential to simulate the multitude of biological systems foundational to lifelong learning, some of which are itemised in [100]. Two examples stand out: (1) Not only can we evolve brain-like algorithms, but we can evolve systems responsible for cognition and learning outside the brain such as Gene Regulatory Networks (GRNs). The potential for autonomous robotics is intriguing, since GRNs are biological systems that might control adaptive morphogenesis (e.g., growth and regeneration) anddecision-making (See Sect. 5.1.3 of [40]); (2) Multisensory integration implies that behaviour is informed by multiple types of sensor signal. While actively studied in neuroscience, some artificial systems have not received significant attention in (Evo) RL (e.g., [167]).
4.7 Transformational Learning and Hierarchical Decomposition
RL problems often involve large, complex spaces that are difficult for agents to navigate in time or space. Two methodologies that enable more efficient learning in these settings are Transfer Learningand Hierarchical Reinforcement Learning. Rather than tackling the big, complex problem directly, these approaches decompose the overall problem into more manageable subproblems. The agent first learns to solve these simpler subproblems, the solutions of which can then be leveraged to solve the overall, more complex task. While this is conceptually similar to the discussion of coevolution and credit assignmentin Sect. 4.4.2, here we examine how this divide-and-conquer method facilitates faster learning and convergence by reducing and transforming the problem space.
4.7.1 Transfer Learning
Transfer learningin RL refers to the reuse of solvers from easier source tasks in an attempt to solve a more difficult target task [170]. Unlike multi-task learning explored in Sect. 4.6, transfer learning assumes that source and target tasks are related. As such, it is expected that learning them together (in sequence or in parallel) improves the learning or search process through positive inter-task transfer. For example, transferlearningwas used heavily in the multi-month training of the Dota 2 bot, because training from scratchfor every new game update pushed by the game developer would have been prohibitively costly in time and money [17]. The key issues in transfer learningcan be summarised in terms of: (1) What knowledge is transferred; and (2) How to implement the knowledge transfer between tasks that may differ with respect to sensors, actions, and objectives. Incremental Evolution[60] and Layered Learning[161] have similar motivation and share the same broad issues as transfer learningIncremental evolutionassumes that the source tasks follow a sequence in which each source task is incrementally more complex than the last. For example, [23] defines a behaviour chainto incrementally learn dynamic legged locomotion towardsan object followed by grasping, lifting, and holding of that object in a realistic three-dimensional robotics environment. In this case, a single champion individual is transferred between each task as a seed for the population on the next source task. Thus, variation operators applied to the genotype of this champion provide a starting point for generating the next population. Variations on this approach pass the entire population between consecutive tasks (e.g., [3]) or transfer multiple champion individuals [76]. One potential issue is that an evolutionarily good sequence of source tasks can be quite unintuitive, making the utility of the human-designed source tasks unpredictable or even detrimental [59]. Rather than assume that source tasks represent a sequence of explicitly related problems of incremental complexity, Layered Learning[159] may define source tasks that are independent. For example, in the keep-away soccer game, source tasks may include independent behaviours for get open, pass, and intercept. Each task represents a separate training environment with unique states and actions. Transfer is accomplished through an evolved switching policythat joins independent solvers for each source task [86, 186].
Finally, transfer learningmust also consider what happens when the sensor and action spaces have altered meaning or dimensionality between source and target tasks. A learned inter-task mappingis one method of addressing this issue [171]. In addition, Hyper-NEAThas been proposed as a way to evolve generalised state representations such that a policycan transfer to more complex environments without additional learning, assuming the objective remains the same [179]. In both incremental and layered learning, entire (autonomous) policies are transferred from the source task to target task, where they either continue adapting under the more challenging task environment (incremental evolution)or are reused as-is within a hierarchical switching policy(layered learning)
4.7.2 Hierarchical Reinforcement Learning
The principal goal of Hierarchical Reinforcement Learning (HRL) is to allow sequential decision policies to reason at levels of abstraction above the raw sensory inputs and atomic actions assumed by the policy-environment interface. Temporal abstractionimpliesthat the policycan encapsulate a sequence of actions as a single abstract action, or option [164]. Options are typically associated with specific sub-goals. A higher level decision-maker is now required in order to map environmental situations to options. Once identified, the selected option assumes control until its (sub-)goal state is reached. If a suitable task decomposition is known a priori, then multiple options can be trained individually to solve the (human-specified) sub-goals and then reused within a fixed hierarchyto solve a more complex task [45]. However, sub-goals can also be identified automatically, leading to the automated discovery of options and potentially also learning thehierarchical policy [114].
Recent research has emphasised temporal abstractionin reinforcement learning, specifically the importance of automatically discovering sub-goals and options. This implies the emergence of hierarchical structure [25]. In this perspective, HRL is framed more similarly to learning in biological organisms considering both brain and behaviour
From a psychological perspective, the role of intrinsic motivation, or curiosity [140] in motivating an agent’s explorationof action-sequences not explicitly rewarded by the environment has been suggested as a way of kickstarting option discovery. Indeed, explicitly rewarding surprise(Sect. 4.4.1) and the learning opportunities, or salient events, that result has shown promise when implemented within HRL [101]. Evolution is also assumed to play a role in the development of hierarchically organised behaviour in animals [65, 148], and several hybrid TD/EA methods have appeared in the HRL literature. For example, evolutionary methods have been proposed that search for the useful (intermediate) reward functions in order for TD methods to reach the overall goal more efficiently, or shaping rewards[52, 127]. [51] also demonstrate that once subtasks have been defined and learned using TD methods, GP can be employed to evolve option hierarchies
4.7.3 Discussion and Open Challenges
HRL can be characterised as performing search over two levels: (1) Search in a problem space; and (2) (Meta-)Search of a problem space through abstraction. [35] relate this directly to the Creative Systems Framework [189]:
The first level of search achieves exploratory creativity, such as the analytic discovery of new theorems from axioms, or of a control policyto achieve a task. The second level achieves the more elusive transformational creativity, which consists of a radical change of the domain being investigated.
Taken together, a bi-level search of this nature may facilitate insight, a fundamental aspect of creative problem-solving in which an individual arrives at a solution through a sudden change in perspective (an “Aha!” moment). Critical steps towards computational creativityand insight, namely accumulation of experience, meta-level thinking through abstraction [180], and transfer [30] can be leveraged as engineering design principals for scaling RL algorithms to the most complex tasks. EvoRL, as a meta-search paradigm, is uniquely placed to capture these properties. Existing studies achieve sample-efficient learning and good performance on sparse-reward tasks [1, 112], but more work is needed. Among many open challenges, [35] identify automatic option discovery and lifelong hierarchicallearning as important areas for future research.
4.8 Conclusion
RL is characterised by a number of challenges that all relate to the temporal nature of trial-and-error problem-solving. EvoRL is a powerful meta-search paradigm that has the potential to combine the best of population-based search with individual, lifetime development and temporal-difference learning. This chapter has reviewed many examples in which EvoRL efficiently builds sequential decision agents that are themselves efficient lifelong learningmachines, capable of problem-solving even when the observable space is large and incomplete, error signals are delayed, and environment dynamics are continuously changing in unpredictable ways. EvoRL is especially effective when little prior information is available regarding howto solve a problem. It can search in program space and parameter space, and can thus be employed to learn parameters for a predefined policy, directly build policies from scratch, or search for entire RL frameworks by coevolving policies, environments, and their interaction dynamics. With this flexibilitycomes a massive search space, and thus EvoRL frameworks require careful guidance towards the most promising regions. Future work could form this guidance by observing the hardware and software of the brain, the creative process, and their evolution in the natural world.
References
1.
Sasha, A., Geoff, Nitschke.: Scalable evolutionary hierarchical reinforcement learning. In: Proceedings of the Genetic and Evolutionary Computation Conference Companion, GECCO ’22, pp. 272–275. Association for Computing Machinery, New York, NY, USA (2022)
2.
Adami, C.: Making artificial brains: Components, topology, and optimization. Artif. Life 28(1), 157–166 (2022)
3.
Alexandros, A., Julian, T., Simon  Mark, L.: Evolving controllers for simulated car racing using object oriented genetic programming. In: Proceedings of the 9th Annual Conference on Genetic and Evolutionary Computation, GECCO ’07, pp. 1543–1550. Association for Computing Machinery, New York, NY, USA (2007)
4.
Agogino, A., Tumer, K.: Efficient evaluation functions for evolving coordination. Evol. Comput. 16(2), 257–288 (2008)
5.
Agogino, A., Tumer, K.: Efficient evaluation functions for evolving coordination. Evol. Comput. 16(2), 257–288 (2008)
6.
Andre, D.: Evolution of mapmaking: learning, planning, and memory using genetic programming. In: Proceedings of the First IEEE Conference on Evolutionary Computation. IEEE World Congress on Computational Intelligence, vol. 1, pp. 250–255 (1994)
7.
David, A., Stuart, J.R.: State abstraction for programmable reinforcement learning agents. In: Eighteenth National Conference on Artificial Intelligence, pp. 119–125. American Association for Artificial Intelligence , USA (2002)
8.
André, M.S.B., Douglas, A.A., Helio, J.C.B.: On the characteristics of sequential decision problems and their impact on evolutionary computation and reinforcement learning. In: Pierre, C., Nicolas, M., Pierrick, L., Marc, S., Evelyne, L. (eds.) Artifical Evolution, pp. 194–205. Springer, Berlin (2010)
9.
Bai, H., Cheng, R., Jin, Y.: Evolutionary reinforcement learning: A survey. Intell. Comput. 2, 0025 (2023)
10.
Hui, B., Ruimin, S., Yue, L., Botian, X., Ran, C.: Lamarckian platform: Pushing the boundaries of evolutionary reinforcement learning towards asynchronous commercial games. IEEE Trans. Games 1–14 (2022)
11.
Mark Baldwin, J.: A new factor in evolution. In: Adaptive Individuals in Evolving Populations: Models and Algorithms, pp. 59–80 (1896)
12.
Banzhaf, W., et  al.: Defining and simulating open-ended novelty: Requirements, guidelines, and challenges. Theory Biosci. 135(3), 131–161 (2016)
13.
Wolfgang, B., Peter, N., Robert, E.K., Frank, D.F.: Genetic Programming: An Introduction: On the Automatic Evolution of Computer Programs and its Applications. Morgan Kaufmann Publishers Inc. (1998)
14.
Aspen, H.Y., Anne, G.E.C.: How working memory and reinforcement learning are intertwined: a cognitive, neural, and computational perspective. J. Cogn. Neurosci. 34(4), 551–568 (2022)
15.
