Usually, the bottleneck for evolution is calculating the fitness function  [92], which motivated people to seek methods to approximate its value to speed up the evolutionary process. This approach involvessurrogate models  [92]. Surrogate models have been modeled using different ML methods such as linear regression  [37], Gaussian mixture models  [63], and neural networks  [51]. In this section, we decided to call it a target function instead of a fitness function as ML could be used to approximate not only the fitness function but also features of the phenotype such as time to finish the level, and number of jumps in a playtrace. These features can be used either as behavior characteristics inquality diversityalgorithms  [8, 118] or a diversityscore for divergent search algorithms  [5, 61].
We can divide surrogate modelsbased on how they are trained: online and offline methods. The online method focuses on training the surrogate modelas a part of the evolutionary process. The evolution helps to create the dataset for training the ML model. On the other hand, the offline method focuses on training the ML model beforehand. This makes surrogate modelssimilar to reinforcement learningwhere the online method is similar to off-policy reinforcement learningand the offline method is similar to offline reinforcement learningwhere we collect the data before training the model.
25.6.2.1 Online Methods
In the games domain, most of the work utilizes online trainingdue to the small number of data that can be found to train ML models. Volz et al.  [111] used multi-objective optimization to create balanced cards for the Top Trumps card game (Dubreq, 1978). The goal is to generate a group of unique balanced decks such that there is no dominant strategy to win the game. The surrogate modelhere is a statistical model that predicts the minimum number of simulation that is needed to have an accurate estimation of the win rate. The results showed that generated decks from surrogate modelsare as good as normal simulations and require a lot less computational power. In a similar vein, Zhang et al.  [118] used Deep Surrogate Assisted MAP-Elitesto generate a deck of cards for Hearthstone(Blizzard, 2014). They not only used the surrogate modelto calculate the fitness (the average difference of health between both players) but also the behavior characteristics (the average number of cards in hand and the number of turns till the end of the match) needed for the MAP-ElitesArchive. The model is trained online after a fixed number of iterations to make sure the output is correct and up to date. Bhatt et al.  [8] generalized the system by adding another surrogate modelthat predicts the agent playtrace instead of just the final metrics and tested it on generating mazes and Super Mario Broslevels. The new system works better in comparison with the previous one introduced by Zhang et al.  [118] as the introduced agent prediction helps improve the results.
Somequality diversityand divergent search algorithms need ML as their core element such as Surprise Search[37]. Surprise Search abandons objectivity for the sake of surprise. The surprise score needs to be modeled using a ML algorithm so the evolution can predict new elements. The authors modeled the surprise score using linear regression to predict the genotype of the next generation given the previous generation. In this case, online models are being used as the model gets updated after each generation. Later, Gravina et al.  [38] adjusted the surprise searchalgorithm to maintain quality and not only focus on surprise. They utilized the new algorithm (Constrained Surprise Search)to generate a diverse set of balanced weapons for FPS shooter games. They also showcased that it can be used to generate robot controllers, maze solutions, and new mazes  [39]. Liapis et al.  [61] and Barthet et al.  [5] useda denoising autoencoderto help Constrained Novelty search[59] to find innovative spaceship designs and MineCraft (Mojang Studios, 2009) buildings, respectively. Thedenoising autoencoderis trained online from the found data and then the compressed representation is used to measure the novelty of the generated content.
25.6.2.2 Offline Methods
On the offline side, Karavalos et al.  [51] trained a neural network to predict the win rate and time to finish an FPS match of a given level and the player classes. They trained the model offline on tons of random matches using random levels and random player classes, then later used the trained model as a surrogate modelto evolve different player classes. Migkotzidis and Liapis  [65] used the same model as part of a mixed-initiative tool  [115] that can be used to design balanced levels for FPS games. On the side of affective computing, Barthet et al.  [4] use a variant of Go-Explore called Go-Blend (a quality diversityalgorithmthat keeps track of the best solutions) to explore game trajectories that can mimic different human play styles and arousal levels. They used a simple K-NN algorithm over the AGAIN dataset  [64] to predict the arousal levels during playing a car racing game. Similarly, Shaker et al.  [88] trained an offline surrogate modelon human preference over Mario levels and used it to generate new levels for Super Mario Bros(Nintendo, 1985). Since the search space was not huge, an exhaustive search was used instead of evolution. We included this research as the exhaustive search can be easily replaced with evolution in more complex games.
A different way to use the ML model is to use it to play games instead of directly measuring playability or the attributes and then extracting the needed statisticsfrom the playthrough. For example, Togelius and Schmidhuber  [108] evolved arcade game rules using evolution strategy to evolve the game rules such that the evolved games are fun to play by humans. Togelius and Schmidhuber use Koster’s theory of fun  [56] to estimate how learnable these games are. The learnability is approximated by measuring the improvement of a neural network agent that is trained using reinforcement learning. The generated games from that experiment were interesting but not fun for humans. This is due to the fact that AI agents play differently than humans, and we need models that can model the human experience.
Finally, surrogate modelscan also be used to assist game-playing agents. Olesen et al.  [69] tried to use EML to control the player in a car racing experiment. ML was used to learn a forward model which predicts the next state given a certain state and action. This process is called a World Model[41], where we try to learn an approximate forward modelof a specific environment. In their work, Olesen et al. used a VAE followed by Mixture Density Network  [9] to learn that forward model. Later, they used an evolutionary planning algorithm (Random Mutation Hill Climber (RMHC)) to control the player car to play the game efficiently. The system first gets trained using collected data from random policy,thenlater it gets improved using frames and output from the RMHC algorithm. Similarly, Dockhorn et al.  [23] used hashmaps and decision trees to learn a local forward modelfor Sokoban. A local forward modelis a model that only cares about the local observation around the player character. They use RHEA to play the game using the local forward model. Although the local forward modelhas high accuracy, the local model propagates errors over time which causes the planning agent to not achieve very high scores in playing the games.
25.6.3 Machine Learning as an Operator
Models trained via ML can be used as mutation or crossover operators during evolution. This is particularly useful when the space of mutations is very large, making it unlikely that random perturbations will lead to meaningful changes in the genome. In such a scenario, a ML-based model can be trained beforehand so as to learn useful priors over the space of possible mutations. This is particularly appealing when a large dataset of human examples is available.
As an example, ML models can be trained on mutation trajectories   [55, 58], i.e. the modifications made to an entity over the course of evolution. The end result is a model that can generate levels by modifying levels similar to an evolutionary/search-based generator
In another example, the problem of genetic programming is highly complex. A very naive approach could be to try mutating a piece of code at the string or character level, but this would result in a combinatorially explosive action space for the mutation operator. For this reason, most genetic programming approaches will “bake in” some useful priors into the action space afforded to the operator—for example, by providing a higher level set of available actions that may involve adding functional blocks of code such as if/else statements instead of individual characters or words, while treating the code as a graph or tree structure instead of merely a string.
Large Language Models(LLMs) have shown impressive performance on next-token prediction tasks when trained auto-regressively on massive corpora of human text scraped from the Internet. Despite their simple training scheme, they can be observed during inference to generate text that is coherent at a high level, and with the right prompting, can do things like generating rhyming poems about a particular subject matter in a particular style and answering questions while maintaining long-range dependencies. They have also been fine-tuned on corpora of code, and incorporated into tools such as Github Copilot.
In Evolution through Large Models(ELM, Sect.  10.​10), the authors take advantage of these learned priors, and prompt LLMs to produce embodied agents capable of traversing the terrain in the SodaWorld environment. The LLM used is a diff model, which is trained on a dataset of code changes and their corresponding natural language descriptions in commit messages. ELM uses this diff modelas a mutation operator by randomly selecting from a fixed set of generic commit messages (i.e. “made changes to the code”). This operator is then used insidea quality diversity(QD) loop that searches for a diverse group of ambulating soft robotsin SodaWorld. The LLM can be additionally fine-tuned on the series of mutations accepted to the archiveof elites in QD (an approach similarly applied to environment generation in  [55]).
Fig. 25.5
The MarioGPT level-generation pipeline
An ELM type approach has also been applied to game levels in MarioGPT[97], where GPT-based language models are fine-tuned on a small dataset of Super Mario Bros(Nintendo, 1985) levels (Fig.  25.5). After fine-tuning the language model, novelty searchis used as an outer evolutionary loop to search for a diverse set of novel levels generated by the model. While QD fills an archivespanning over multiple behavior characteristics or measures of interest, novelty searchsimply looks for sufficiently “different” individuals, with respect to those already in the archiveof elites, by computing the mean distance using some distance function in phenotype space. In MarioGPT, the authors consider the mean distance between the paths corresponding to (tree search-generated) solutions for generated levels, where paths are lines over the 2D plane corresponding to the shape of generated levels.
The previous examples are the only ones that we could find specifically on games. It is clear that this area provides ample opportunities for future exploration. For example, one might consider using LLMs for Neural Architecture Search for game-playing agents similar to the work by Chen et al.  [15]. Chen et al.  [15] represented the neural network architecture as a piece of code that defines layers, skip connections, activations, etc. They used the diff modelas a mutation operator to produce new networks. These nets are then evaluated by training them on a labeled dataset (e.g. image classification)and evaluating their test accuracy.
25.7 Augmenting Other Methods with Evolutionary Machine Learning
Finally, EML can be used to support other algorithms like tree search and reinforcement learning. Most of the known work focused on using evolution to find/train neural networks that can support tree search algorithms. Blondie24  [28] is an AI agent that can play checkers very efficiently. It was able to beat 99.61% of the matches that it played against 165 human players. The algorithm uses the Min-Max tree search algorithm  [79] with the support of a neural network as a state evaluator. The neural network was trained using an evolutionary algorithm. In a similar manner, Reisinger et al.  [73] not only evolved the network weights but also the architecture using NEATalgorithm  [96]. The evolved network was used as a state evaluator for the alpha-beta pruning algorithm  [79] to play different board games from the General Game PlayingFramework  [34]. The best-evolved agent was able to beat the random agent in 5 different games from the General Game Playingframework. Finally, Gauci and Stanely  [33] used the HyperNEATalgorithm  [95] to prune some branches for the Min-Max algorithm  [79] besides the alpha-beta pruning. The final agent was able to have a higher win rate and more ties compared to the default alpha-beta pruning algorithm.
We can notice that most of the work in that area is older than 10 years ago, and we could not find any new work that combines EML with other search algorithms. We think that the boom in computation power and the dependence on the backpropagation algorithm is the main cause of that. For example, the Alpha-Go algorithm  [89] uses a neural network as a state evaluator for MCTS agent  [11]. This is similar to the Blondie24 agent with the difference of having more computation power to train a big network using backpropagation  [44]. This does not mean that we should abandon EML for the sake of backpropagation, but it should push us toward using EML in a smarter and different way that backpropagation cannot do. For example, we could try evolving a network that compresses the game state space such that a dynamic programming agent can play the game efficiently.
25.8 Conclusion
As demonstratedby the myriad examplesin this chapter, game research has been enhanced greatly by evolution and ML. The combination of thetwo approaches provides particularly compelling tools for generating game agents and content; the two types of methods naturally complement each other’s strengths andweaknesses. The future of this research area is exciting, as rapid advances in ML technologies will allow us to not only apply new algorithms to existing games but perhaps even to create new kinds of game-based challenges as well (which will, in turn, provide new domains for evaluating new kinds of EML systems).
References
1.
Alvernaz, S., Togelius, J.: Autoencoder-augmented neuroevolutionfor visual doom playing. In: 2017 IEEE Conference on Computational Intelligence and Games (CIG), pp. 1–8. IEEE, MIT Press (2017)
2.
Awiszus, M., Schubert, F., Rosenhahn, B.: Toad-gan: coherent style level generation from a single example. Proc. AAAI Conf. Artif. Intell. Interact. Digit. Entertain. 16, 10–16 (2020)
3.
Baker, B., Akkaya, I., Zhokov, P., Huizinga, J., Tang, J., Ecoffet, A., Houghton, B., Sampedro, R., Clune, J.: Video pretraining (vpt): learning to act by watching unlabeled online videos. In: Advances in Neural Information Processing Systems (2023)
4.
Barthet, M., Khalifa, A., Liapis, A., Yannakakis, G.: Generative personas that behave and experience like humans. In: Proceedings of the 17th International Conference on the Foundations of Digital Games, pp. 1–10 (2022)
5.
Barthet, M., Liapis, A., Yannakakis, G.N.: Open-ended evolution for minecraft building generation. In: IEEE Transactions on Games (2022)
6.
Bellemare, M.G., Naddaf, Y., Veness, J., Bowling, M.: The arcade learning environment: an evaluation platform for general agents. J. Artif. Intell. Res. 47, 253–279 (2013)Crossref
7.
Bengio, Y., Louradour, J., Collobert, R., Weston, J.: Curriculum learning. In: Proceedings of the 26th Annual International Conference on Machine Learning, pp. 41–48 (2009)
8.
Bhatt, V., Tjanaka, B., Fontaine, M.C., Nikolaidis, S.: Deep surrogate assisted generation of environments (2022). arXiv:​2206.​04199
9.
Bishop, C.M.: Mixture density networks. In: Neural Computing Research Group Report (1994)
10.
Bontrager, P., Lin, W., Togelius, J., Risi, S.: Deep interactive evolution. In: Computational Intelligence in Music, Sound, Art and Design: 7th International Conference, EvoMUSART 2018, Parma, Italy, April 4-6, 2018, Proceedings, pp. 267–282. Springer (2018)
11.
Browne, C.B., Powley, E., Whitehouse, D., Lucas, S.M., Cowling, P.I., Rohlfshagen, P., Tavener, S., Perez, D., Samothrakis, S., Colton, S.: A survey of monte carlo tree search methods. IEEE Trans. Comput. Intell. AI Games 4(1), 1–43 (2012)Crossref
12.
Campbell, M., Joseph Hoane, A., Jr., Hsu, F.: Deep blue. Artif. Intell. 134(1–2), 57–83 (2002)CrossrefzbMATH
13.
Cardamone, L., Yannakakis, G.N., Togelius, J., Luca Lanzi, P.: Evolving interesting maps for a first person shooter. In: Applications of Evolutionary Computation: EvoApplications, pp. 63–72. Springer (2011)
14.
Carvelli, C., Grbic, D., Risi, S.: Evolving hypernetworks for game-playing agents. In: Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion, pp. 71–72 (2020)
15.
Chen, A., Dohan, D.M., So, D.R.: Evoprompting: language models for code-level neural architecture search (2023). arXiv:​2302.​14838
16.
Chrabaszcz, P., Loshchilov, I., Hutter, F.: Back to basics: benchmarking canonical evolution strategies for playing atari. In: IJCAI (2018)
17.
Cook, M., Colton, S., Raad, A., Gow, J.: Mechanic miner: reflection-driven game mechanic discovery and level design. In: Applications of Evolutionary Computation: 16th European Conference, EvoApplications 2013, Vienna, Austria, April 3-5, 2013. Proceedings 16, pp. 284–293. Springer (2013)
18.
Cuccu, G., Togelius, J., Cudré-Mauroux, P.: Playing atari with six neurons (2018). arXiv:​1806.​01363
19.
Dahlskog, S., Togelius, J.: Patterns as objectives for level generation. In: Design Patterns in Games (DPG), Chania, Crete, Greece (2013). ACM Digital Library (2013)
20.
Soares de  Lima, E., Feijó, B., Furtado, A.L.: Procedural generation of quests for games using genetic algorithms and automated planning. In: SBGames, pp. 144–153 (2019)
21.
Dennis, M., Jaques, N., Vinitsky, E., Bayen, A., Russell, S., Critch, A., Levine, S.: Emergent complexity and zero-shot transfer via unsupervised environment design. Adv. Neural Inf. Process. Syst. 33, 13049–13061 (2020)
22.
Dharna, A., Togelius, J., Soros, L.B.: Co-generation of game levels and game-playing agents. In: Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment, vol.  16, pp. 203–209 (2020)
23.
Dockhorn, A., Lucas, S.M., Volz, V., Bravi, I., Gaina, R.D., Perez-Liebana, D.: Learning local forward models on unforgiving games. In: 2019 IEEE Conference on Games (CoG), pp. 1–4. IEEE (2019)
24.
Earle, S., Snider, J., Fontaine, M.C., Nikolaidis, S., Togelius, J.: Illuminating diverse neural cellular automata for level generation. In: Proceedings of the Genetic and Evolutionary Computation Conference, pp. 68–76 (2022)
25.
Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K.O., Clune, J.: First return, then explore. Nature 590(7847), 580–586 (2021)Crossref
26.
Eigenfeldt, A.: Corpus-based recombinant composition using a genetic algorithm. Soft Comput. 16, 2049–2056 (2012)Crossref
27.
Elsken, T., Hendrik Metzen, J., Hutter, F.: Neural architecture search: a survey. J Mach. Learn. Res. 20(1), 1997–2017 (2019)MathSciNetzbMATH
28.
Fogel, D.B.: Blondie24: Playing at the Edge of AI. Morgan Kaufmann (2002)
29.
Fontaine, M.C., Liu, R., Khalifa, A., Modi, J., Togelius, J., Hoover, A.K., Nikolaidis, S: Illuminating mario scenes in the latent space of a generative adversarial network. In: Proceedings of the AAAI Conference on Artificial Intelligence, vol.  35, pp. 5922–5930 (2021)
30.
Fontaine, M.C., Togelius, J., Nikolaidis, S., Hoover, A.K.: Covariance matrix adaptation for the rapid illumination of behavior space. In: Proceedings of the 2020 Genetic and Evolutionary Computation Conference, pp. 94–102 (2020)
31.
Gaina, R.D., Lucas, S.M., Perez-Liebana, D.: Rolling horizon evolution enhancements in general video game playing. In: 2017 IEEE Conference on Computational Intelligence and Games (CIG), pp. 88–95. IEEE (2017)
32.
Gallotta, R., Arulkumaran, K., Soros, L.B.: Preference-learning emitters for mixed-initiative quality-diversity algorithms. In: IEEE Transactions on Games, pp. 1–14 (2023)
33.
Gauci, J., Stanley, K.O.: Evolving neural networks for geometric game-tree pruning. In: Proceedings of the 13th annual conference on Genetic and evolutionary computation, pp. 379–386 (2011)
34.
Genesereth, M., Love, N., Pell, B.: General game playing: overview of the aaai competition. AI Mag. 26(2), 62–62 (2005)
35.
Golberg, D.E.: Genetic Algorithms in Search, Optimization, and Machine Learning. Addion wesley (1989)
36.
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Bing, X., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y.: Generative adversarial networks. Commun. ACM 63(11), 139–144 (2020)MathSciNetCrossref
37.
Gravina, D., Liapis, A., Yannakakis, G.: Surprise search: beyond objectives and novelty. Proc. Genet. Evol. Comput. Conf. 2016, 677–684 (2016)
38.
