Dua, D., Graff, C.: UCI machine learning repository (2017)
14.
Fernandez-Delgado, M., Cernadas, E., Barro, S., Amorin, D.: Do we need hundreds of classifiers to solve real-world classification problems? J. Mach. Learn. Res. 15, 3133–3181 (2014)MathSciNetzbMATH
15.
Freitas, A.A.: Data Mining and Knowledge Discovery with Evolutionary Algorithms. Springer (2002)
16.
Freitas, A.A.: A critical review of multi-objective optimization in data mining: a position paper. ACM SIGKDD Explor. Newsl6(2), 77–86 (2004)
17.
Haas, C.: The price of fairness–a framework to explore trade-offs in algorithmic fairness. In: Proceedings of the 40th International Conference on Information Systems (ICIS) (2019)
18.
Hardt, M., Price, E., Srebro, N.: Equality of opportunity in supervised learning. In: Advances in Neural Information Processing Systems, pp. 3315–3323 (2016)
19.
Helmuth, T., Pantridge, E., Spector, L.: Lexicase selection of specialists. In: Proceedings Genetic and Evolutionary Computation Conference (GECCO-2019), pp. 1030–1038. ACM Press (2019)
20.
Kearns, M., Neel, S., Roth, A., Wu, Z.S.: An empirical study of rich subgroup fairness for machine learning. In: Proceedings of the 2019 ACM Conference on Fairness, Accountability and Transparency (FAT), pp. 100–109. ACM (2019)
21.
Kearns, M., Neel, S., Roth, A., Wu, Z.S.: An empirical study of rich subgroup fairness for machine learning. In: Proceedings of the Conference on Fairness, Accountability, and Transparency, pp. 100–109 (2019)
22.
Kenfack, P.J., Khan, A.M., Kazmi, S.A., Hussain, R., Oracevic, A., Khattak, A.M.: Impact of model ensemble on the fairness of classifiers in machine learning. In: Proceedings of the 2021 International Conference on Applied Artificial Intelligence (ICAPAI), pp. 1–6. IEEE (2021)
23.
Kleinberg, J., Mullainathan, S., Raghavan, M.: Inherent trade-offs in the fair determination of risk scores (2016). arXiv:​1609.​05807
24.
Kusner, M.J., Loftus, J., Russell, C., Silva, R.: Counterfactual fairness. In: Advances on Neural Information Processing Systems, vol. 30 (2017)
25.
Kusner, M.J., Loftus, J., Russell, C., Silva, R.: Counterfactual fairness. In: Advances in Neural Information Processing Systems, pp. 4066–4076 (2017)
26.
La  Cava, W., Moore, J.H.: Genetic programming approaches to learning fair classifiers. In: Proceedings of Genetic and Evolutionary Computation Conference (GECCO-2020), pp. 967–975 (2020)
27.
La  Cava, W., Singh, T.R., Taggart, J.P., Suri, S., Moore, J.H.: Learning concise representations for regression by evolving networks of trees. In: Proceedings of the International Conference on Learning Representations (ICLR 2019), 25 Mar 2019, p. 16 (2019). arXiv:​1807.​00981v3[cs.NE]
28.
Li, B., Tang, K., Li, J., Yao, X.: Stochastic ranking algorithm for many-objective optimization based on multiple indicators. IEEE Trans. Evol. Comput. 20(6), 924–938 (2016)Crossref
29.
Li, J., Cheng, K., Wang, S., Morstatter, F., Trevino, R.P., Tang, J., Liu, H.: Feature selection: a data perspective. ACM Comput. Surv. 50(6), 94:1–94:45 (2017)
30.
Lipton, Z., McAuley, J., Chouldechova, A.: Does mitigating ml’s impact disparity require treatment disparity? Advances in Neural Information Processing Systems, vol. 31 (2018)
31.
Mehrabi, N., Morstatter, F., Saxena, N., Lerman, K., Galstyan, A.: A survey on bias and fairness in machine learning (2019)arXiv:​1908.​09635
32.
Pleiss, G., Raghavan, M., Wu, F., Kleinberg, J., Weinberger, K.Q.: On fairness and calibration. In: Proceedings of 31st Conference on Neural Information Processing Systems (NIPS 2017) (2017)
33.
Quadrianto, N., Sharmanska, V.: Recycling privileged learning and distributed matching for fairness. In: Proceedings of the 31st Conference on Neural Information Processing Systems (NIPS 2017), pp. 677–688 (2017)
34.
Rehman, A.U., Nadeem, A., Malik, M.Z.: Fair feature subset selection using multiobjective genetic algorithm. In: Proceedings of the GECCO-22 Companion (Genetic and Evolutionary Computation Conference), pp. 360–363. ACM Press (2022)
35.
Skeem, J.L., Lowenkamp, C.T.: Risk, race, & recidivism: predictive bias and disparate impact. Criminology 54, 680 (2016)
36.
Srinivas, N., Deb, K.: Muiltiobjective optimization using nondominated sorting in genetic algorithms. Evol. Comput. 2(3), 221–248 (1994)Crossref
37.
Telikani, A., Tahmassebi, A., Banzhaf, W., Gandomi, A.H.: Evolutionary machine learning: a survey. ACM Comput. Surv. 54(8), 161:1–161:35 (2021)
38.
Tian, Y., Si, L., Zhang, X., Cheng, R., He, C., Tan, K.C., Jin, Y.: Evolutionary large-scale multi-objective optimization: a survey. ACM Comput. Surv. 54(8), 174:1–174:34 (2021)
39.
Valdivia, A., Sanchez-Monedero, J., Casillas, J.: How fair can we go in machine learning? assessing the boundaries of accuracy and fairness. Int. J. Intell. Syst. 36(4), 1619–1643 (2021)Crossref
40.
Verma, S., Rubin, J.: Fairness definitions explained. In: 2018 IEEE/ACM International Workshop on Software Fairness (FairWare), pp. 1–7. IEEE (2018)
41.
Witten, I.H., Frank, E., Hall, M.A., Pal, C.J.: Data Mining: Practical Machine Learning Tools, 4th edn. Morgan Kaufmann (2016)
42.
Wu, H., Ma, C., Mitra, B., Diaz, F., Liu, X.: A multi-objective optimization framework for multi-stakeholder fairness-aware recommendation. ACM Trans. Inf. Syst. 41(2) (2022)
43.
Zemel, R., Wu, Y., Swersky, K., Pitassi, T., Dwork, C.: Learning fair representations. In: International Conference on Machine Learning, pp. 325–333 (2013)
44.
Zhang, C., Liu, C., Zhang, X., Almpanidis, G.: An up-to-date comparison of state-of-the-art classification algorithms. Expert. Syst. 82, 128–150 (2017)Crossref
45.
Zhang, Q., Liu, J., Zhang, Z., Wen, J., Mao, B., Yao, X.: Mitigating unfairness via evolutionary multi-objective ensemble learning. IEEE Trans. Evol. Comput. (2022). https://​doi.​org/​10.​1109/​TEVC.​2022.​3209544Crossref
46.
Zoller, M.A., Huber, M.F.: Benchmark and survey of automated machine learning frameworks. J. Artif. Intell. Res. 70, 409–472 (2021)MathSciNetCrossrefzbMATHPart VApplications of Evolutionary Machine Learning
In which we bring together a few of the contemporary application areas for evolutionary computation and machine learning: Medicine, Robotics, Finance, Science and Engineering, Environmental Science, Games and Control.©  The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.  2024
W. Banzhaf et al.(eds.)Handbook of Evolutionary Machine LearningGenetic and Evolutionary Computationhttps://doi.org/10.1007/978-981-99-3814-8_18
18.  Evolutionary Machine Learning in Science and Engineering
Jianjun  Hu1, Yuqi  Song1, Sadman  Sadeed  Omee1, Lai  Wei1, Rongzhi  Dong1and Siddharth  Gianey1
(1)
Department of Computer Science and Engineering, University of South Carolina, Columbia, SC  29201, USA
Jianjun  Hu
Email: jianjunh@cse.sc.edu
Abstract
Evolutionary machine learning (EML) has been increasingly applied to solving diverse science and engineering problems due to the global search, optimization, and multi-objective optimization capabilities of evolutionary algorithms and the strong modeling capability of complex functions and processes by machine learning (ML) and especially deep neural network models. They are widely used to solve modeling, prediction, control, and pattern detectionproblems. Especially EML algorithms are used for solving inverse designproblems ranging from neural network architecture search, inverse materials design, control system design, and discoveryof differential equations.
18.1 Introduction
Several common fundamental research themes frequently arise in many science and engineering domains such as modeling physical and chemical processes, prediction, pattern classification, abnormality recognition, generation of structures, control, and inverse design. In these problems, the traditional analytical models are increasingly replaced or complemented by data-driven machine ML and especially deep neural network models  [70], due to their strong capability to learn complex nonlinear relationships and inherent representations that lead to high-performance prediction models. For example, ab initio crystal structure prediction(CSP) has been a long-time challenging problem due to the expensive first principle calculations needed to evaluate the candidate structures during the search. However, currently, significant progress is being made in learning neural network-based interatomic potentials to speed up the CSP or molecular dynamics simulation process [60]. As a result, ML and especially deep learning (DL) have been transforming almost every discipline of science and engineering.
While deep neural network models are good at modeling, their gradient-descent-based training algorithms can only be applied to differentiable networks. However, there are many science and engineering problems that need strong global (multi-objective) optimization or search capability in vast design space, e.g., of materials and molecules or searching discrete structures such as neural network architectures. In this regard, it is natural to hybridize ML models with evolutionary algorithms to achieve synergistic high performance in problem-solving. Evolutionary computation (EC) has been applied to various stages of ML and DL for model search, feature selection, or hyperparameter tuning. It has played an increasing role in a range of scientific research tasks due to its global search and multi-objective optimization capabilities such as crystal structure predictionor inverse design[5] in which a surrogate performance evaluation model is trained and then used as the objective function in inverse designsearch. With these complementary roles of ML and evolutionary computation, there emerges the EML paradigm with unique capabilities and applications in diverse disciplines.
EML methods can be understood from different perspectives. From the ML problem point of view, EML has been used in solving supervised learning, unsupervised learning, and reinforcement learningproblems. For example, it has been used for clustering [54], classification [106], regression [33, 87], and ensemble learning [40]. From an algorithm and model point of view, EML can be classified into two categories including: (1) EC as ML tools, in which evolutionary algorithms are directly used to solve diverse ML problems such as clustering, classification [41], or regression (2) EC for ML, in which EC is used to improve the model design (such as network architecture as shown in the evolutionary DL [126]), training of ML models, hyperparameter search, feature engineering, and explainability
Some prominent applications of EML in science and engineering are shown in Fig.  18.1. These applications have some unique characteristics. First, most physical, chemical, and engineering processes are highly nonlinear and are difficult to model explicitly, so neural networks have thus been exploited to model such complex processes. Similarly, the complexity of patterns from these systems has also called for the application of ML and DL in these areas. So both EC as ML and EC for ML have been widely used in science and engineering for clustering, classification, regression, pattern detection, etc. However, there is a special category of application of EML in science and engineering: the (inverse) design problem, which ranges from neural network architecture [15, 66, 111] and parameter design, to partial differential equationdiscovery [121], inverse materials design [122], neural control system design [84], crystal structure prediction[37], and engineering system design. In these problems, usually, the objective function or performance evaluation is modeled using an ML or neural network model, and then genetic algorithms (GAs) are used to search the candidate solutions in the design space using ML as the objective function. This problem-solving strategy also applies to the problem of discovering physics equations, in which the terms of (partial) differential equations can be discovered by evolutionary algorithms and then assembled either by GAs or by DL.
Fig. 18.1
Categories of problems suitable for solving using EML
18.2 Applications of EML in Science and Engineering
EML has gained widespread recognition and has been extensively utilized in diverse fields of science and engineering. In the subsequent sections, we provide a comprehensive overview of the manifold applications of EML in various domains, including physics and materials sciences, chemistry, astronomy, biology, geography, and other branches of engineering. The comprehensive breakdown of the contents pertaining to the applications of EML in these domains is graphically represented in Fig.  18.1
18.2.1 EML for Physics and Materials Sciences
The application of ML and especially DL in physics and materials science has been accelerating due to the complexity in manually modeling the structure-property relationship, the challenge in ab initio simulation for materials characterization, and the challenge in sampling the huge chemical space. To address these issues, ML and deep neural networks are now commonly used to learn the interatomic potentials for Density Functional Theory (DFT)or molecular dynamics simulation, which can then be combined with global optimization algorithms for crystal structure prediction[8]. Neural networks are also trained with ab initio data to speed up materials property calculation so that they can be used as fast surrogate modelsfor inverse designof materials using evolutionary algorithms [5]. Evolutionary algorithms can also be combined with ML to discover physical laws as represented by partial differential equations[119, 121]. Finally, evolutionary algorithms are also routinely used for training special neural network models due to their gradient-free global optimization capability.
18.2.1.1 Discovering Physical Laws and Equations
Discovering physical laws representedas mathematical models such as partial differential equations (PDE)is one of the most challenging tasks in physics. Data-driven methods have been routinely used for PDE discovery, but they usually require all potential terms to be specified. Xu et al. [119, 121] proposed a novel method, DLGA-PDE for PDE discovery without such constraints. It works by training a DL model of the physical process for generating meta-data and derivativesand then using a GA to search for the combination of such terms. Their method has been shown to achieve good performance in the discovery of the Korteweg-de Vries (KdV) equation, the Burgers equation, the wave equation, and the Chaffee-Infante equation, all with an incomplete term library and even with noisy and limited data. In [26], Chen et al. used an evolutionary strategy to train a discrete feed-forward convolutional neural network model for modeling variational wave functions for correlated many-bodyquantum systems. They found that networks can converge with high accuracy to the analytically known sign structures of ordered phases. In [120], a robust PDE discovery framework called the robust DL GA (R-DLGA) was proposed, which combines the physics-informed neural network (PINN) and DLGA for potential terms discovery. The terms discovered by DLGA are added to the loss functionof the PINN as physical constraints to improve the accuracy of the derivativecalculation. The authors [121] further proposed an EML method for DL of parametric partial differential equationsfrom sparse and noisy data. The EML method has also been applied to learn models that balance accuracy with parsimony in classical mechanics and the melting temperature prediction of materials [38]. The EML approach allowed them to discover interpretable physical laws from data based on parsimonious neural networks (PNNs) combined with evolutionary optimization. The EML approach has also been applied to learn parametric partial differential equations includingthe Burgers equation, the convection-diffusion equation, the wave equation, and the KdV equation from sparse and noisy data [121], in which the neural network is first trained to calculate derivativesand generate meta-data, which solves the problem of sparse noisy data. Then, the GA is used to discover the form of PDEs and corresponding coefficients.
18.2.1.2 Crystal Structure Prediction
Oneof the major challenges in materials science is to predict a structure given only its composition. While several crystal structure predictionalgorithms based on global optimization and first principle (e.g., Density Functional Theory (DFT)) havebeen proposed, the computational complexity of such DFT simulations have made it difficult to predict structures for most materials. To address this issue, a series of research combined deep neural networks with GAs for crystal phase determination [8, 37]. In [8], Artrith et al. trained a specialized ML neural network potential using around 1000 first-principles calculations, which can help sample low-energy atomic configurations in the entire amorphous LixSi phase space. The result is comparable to that of ANN trained with extensive molecular dynamics simulations with 45 000 first-principles calculations. Such neural network potentials (including graph neural network potentials) have since been further improved or developed and applied with evolutionary algorithms [17, 28]. In [130], Bayesian optimization is combined with graph neural network (GNN) potential to do crystal structure relaxation. The GNN is then later combined with a GA for de novo crystal structure prediction[31]. Wanzenböck et al.  [76] proposed an algorithm that explores the rich phase diagram of TiOx overlayer structures on SrTiO3(110) by combining the covariance matrix adaptation evolution strategy(CMA-ES) and a neural network force field (NNFF) as a surrogate energy model, which dramatically reduces the computational resources needed by DFT simulation. While most neural network potentials are trained before the genetic search of crystal phases, it is also possible to conduct both simultaneously, which is how active learning works. In [91], Podryabinkin et al. proposed a methodology for crystal structure predictionbased on the evolutionary algorithm and the active learning of neural network interatomic potentials. Their approach allows for an automated construction of an interatomic interaction model from scratch achieving a speedup of several orders of magnitude. They have benchmarked their algorithms on crystal structure predictionof carbon, high-pressure phases of sodium, and boron allotropes, including those that have more than 100 atoms in the primitive cell, all with satisfactory results. Kang et al. [60] trained a deep neural network interatomic potential model and combined it with a GA for crystal structure prediction. By avoiding the expensive DFT calculations of formation energy and harnessing the speed and accuracy of neural network potentials (NNPs), their algorithm navigates configurational spaces times faster than DFT-based methods. Their SPINNER algorithm has identified more stable phases in many cases than the data-mined template-based method and DFT-based evolutionary algorithm methods.
18.2.1.3 Hyperparameter Tuning
Another major application of EML in physics and materials is hyperparameter tuning of ML models as shown in [109]. Both particle swarm optimization (PSO) and GAs are routinely used to select optimal hyperparameter values autonomously. Tani and Rand evaluated how PSO and GA can improve the performance of their XGBoost ML model for the ATLAS Higgs boson ML challenge (HBC) [3], which represents a typical application of ML algorithms to the field of high energy physics. The task of the HBC is to separate the Standard Model (SM) Higgs boson signal from the large SM background. They showed that compared to using the default hyperparameters, the optimization of the hyperparameter values by GA or PSO improves the sensitivity of the data analysis, by 12-13%, demonstrating that the optimization of hyperparameters is a worthwhile task for data analyses in the field of HEP. In [50], DL techniques are applied to learn the physics of extensive air showers in which the inner structure of the neural network is optimized through the use of GAs.
18.2.1.4 Inverse Design
Another major application of EML inphysics is the inverse designin which the ML models are used to learn the relationship between structures and physical property while evolutionary algorithms are used to search the design space. In [32, 46], Comin and Hartschuh combined neural networks with a GA for optimizing spectral-phase shaping of an incident field to achieve second harmonic generation hotspot switching in plasmonic nanoantennas design. They first trained a neural network to predict the relative intensity of the second-harmonic hotspots of the nanoantenna for a given spectral phase and then used a GA to generate a wide range of nanoantenna designs to be fed into the neural network. Taking advantage of the multi-objective optimization feature of the GA, Li et al. [75] applied a multi-objective GA to the optimization of the apertures of the National Synchrotron Light Source II (NSLI-II) Storage Ring. To maintain the diversityof the population of the GA, a K-means clustering algorithm is applied to the population to group individuals into clusters of different fitness levels. Then individuals from the “Good” and “Poor” clusters are mixed with the “Best/elite” individuals to ensure population quality improvement without losing diversity. In [29], Chen et al. proposed an EML approach for physics-guided ML-based inverse designof acoustic metamaterials. They used a multi-layer perceptron (MLP)neural network to map the wave-field-to-wave propagation relationship and then used it as a surrogate modelfor GA for inverse designof the metabeam. While deep neural networks are widely used as surrogate models, they are not able to tune the output to encompass a range of input states. In this case, a nonlinear symbolic regressionmodel by genetic programming is more desirable, which is used in metamaterials design [5]. In their work, the actual design is implemented using the MLP-based autoencodernetwork model. The EML-based inverse designhas also been applied to designing solid oxide fuel cells (SOFCs) [122], in which a deep neural network is used to map the current density, anode flow rate, cathode flow rate, and temperatures to outputs that reflect the efficiency and thermal behavior of the SOFC cell. Moreover, the EML has also been used in inverse designof photonics in [52].
There are several other novel applications of EML in physics problems. In [26], Chen et al. used a MLPnetwork to approximate a novel class of variational wave functions for correlated many-bodyquantum systems. They encode the all-important rugged sign structure of a quantum wave function in a convolutional neural network with discrete output, which is then trained with a gradient-free evolution strategy (ES) algorithm rather than the commonly used back-propagation algorithm. They found that while the stochastic gradient descent (SGD) algorithm is better for optimizing continuous functions, the ES method is the better choice for optimizing the variational wave function while SGD is no longer applicable. In [116], a multi-objective GA is combined with deep neural network models for improving the performance and durability of direct internal reforming solid oxide fuel cells.
18.2.2 EML for Chemistry
Evolutionary algorithms (EAs) are generic, population-based, metaheuristic optimization methods. The mechanisms by which EAs operate are inspired by biological evolutionary operations such as selection, mutation, recombination, and reproduction. Combined with ML, GAs provide a novel tool for the investigation of molecule design, optimization, and molecular dynamic simulation
18.2.2.1 EML for Molecule Design and Optimization
