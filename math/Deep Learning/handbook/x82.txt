55.
Tjeng, V., Xiao, K.Y., Tedrake, R.: Evaluating robustness of neural networks with mixed integer programming. In: International Conference on Learning Representations (2019)
56.
Tolstikhin, I.O., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T., Yung, J., Steiner, A., Keysers, D., Uszkoreit, J., Lucic, M., Dosovitskiy, A.: MLP-Mixer: an all-MLP architecture for vision (2021). arXiv:​ 2105.​01601
57.
Tramèr, F., Carlini, N., Brendel, W., Madry, A.: On adaptive attacks to adversarial example defenses. In: Conference on Neural Information Processing Systems (NeurIPS) (2020)
58.
Valentim, I., Lourenço, N., Antunes, N.: Adversarial robustness assessment of neuroevolution approaches. In: 2022 IEEE Congress on Evolutionary Computation (CEC) (2022)
59.
Vargas, D.V., Kotyan, S.: Evolving robust neural architectures to defend from adversarial attacks (2019). arXiv:​1906.​11667
60.
Webb, G.I., Hyde, R., Cao, H., Nguyen, H.-L., Petitjean, F.: Characterizing concept drift. Data Min. Knowl. Disc. 30(4), 964–994 (2016)MathSciNetCrossrefzbMATH
61.
Xie, X., Liu, Y., Sun, Y., Yen, G.G., Xue, B., Zhang, M.: BenchENAS: a benchmarking platform for evolutionary neural architecture search. IEEE Trans. Evol. Comput. 26(6), 1473–1485 (2022)Crossref
62.
Yang, J., Wang, P., Zou, D., Zhou, Z., Ding, K., Peng, W., Wang, H., Chen, G., Li, B., Sun, Y., Du, X., Zhou, K., Zhang, W., Hendrycks, D., Li, Y., Liu, Z.: OpenOOD: benchmarking generalized out-of-distribution detection. In: Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (2022)
63.
Yin, D., Gontijo  Lopes, R., Shlens, J., Cubuk, E.D., Gilmer, J.: A fourier perspective on model robustness in computer vision. In: Advances in Neural Information Processing Systems, vol. 32 (2019)
64.
Yuan, X., He, P., Zhu, Q., Li, X.: Adversarial examples: attacks and defenses for deep learning. IEEE Trans. Neural Netw. Learn. Syst. 30(9), 2805–2824 (2019)MathSciNetCrossref
65.
Zagoruyko, S., Komodakis, N.: Wide residual networks. In: Proceedings of the British Machine Vision Conference (BMVC). BMVA Press (2016)
66.
Zoph, B., Vasudevan, V., Shlens, J., Le, Q.V.: Learning transferable architectures for scalable image recognition. In: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 8697–8710 (2018)
Footnotes
1
In the ML literature, it is common to find references to a validation setof data. Typically, that validation set is used to tune model hyperparameters and perform model selection. Model selection is, to a certain degree, related to model validation, but, ultimately, the goal of each of these tasks differs. Model validation is concerned with fully specified models.
2
https://​github.​com/​RobustBench/​robustbench
3
https://​github.​com/​fillassuncao/​denser-models
4
https://​github.​com/​ianwhale/​nsga-net
©  The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.  2024
W. Banzhaf et al.(eds.)Handbook of Evolutionary Machine LearningGenetic and Evolutionary Computationhttps://doi.org/10.1007/978-981-99-3814-8_16
16.  Evolutionary Approaches to Explainable Machine Learning
Ryan  Zhou1and Ting  Hu1
(1)
School of Computing, Queen’s University, Kingston, ON, K7L 2N8, Canada
Ting  Hu
Email: ting.hu@queensu.ca
Abstract
Machine learning models are increasingly being used in critical sectors, but their black-box nature has raised concerns about accountability and trust. The field of explainable artificial intelligence (XAI) or explainable machine learning (XML) has emerged in response to the need for human understanding of these models. Evolutionary computing, as a family of powerful optimization and learning tools, has significant potential to contribute to XAI/XML. In this chapter, we provide a brief introduction to XAI/XML and review various techniques in current use for explaining machine learning models. We then focus on how evolutionary computing can be used in XAI/XML, and review some approaches which incorporate EC techniques. We also discuss some open challenges in XAI/XMLand opportunities for future research in this field using EC. Our aim is to demonstrate that evolutionary computing is well-suited for addressing current problems in explainabilityand to encourage further explorationof these methods to contribute to the development of more transparent, trustworthy, and accountable machine learning models.
16.1 Introduction
As the use of machine learning models becomes increasingly widespread in various domains, including critical sectors of society such as finance and healthcare, there is a growing need for human understanding of such models. Although machine learning can detect complex patterns and relationships in data, decisions based on the predictions made by these models can have real-world impacts on human lives. The use of machine learning models in high-stakes applications such as medicine, job hiring, and criminal justice has raised concerns about the fairness, transparency, and accountability of these models. Therefore, it is essential not only to develop accurate prediction models but also to understand and explain how these predictions are being made.
The field of explainable artificial intelligence (XAI) or explainable machine learning (XML) has emerged in response to this need  [37]. XAI research aims to develop methods to explain the decisions, predictions, or recommendations made by machine learning models in a way that is understandable to humans. These explanations act as a foundation to build trust and improve the robustness of a model by highlighting biases and failures, allow researchers to better understand, validate and debug the model, ensure compliance with regulations, and improve human-machine interaction by giving users a better understanding of when they can rely on a model’s decisions.
In this book chapter, we aim to provide an overview of XAI/XML and the role of evolutionary computing (EC) in this field. In Sect.  16.2, we introduce the concept of XML and review current techniques used in the field. In Sect.  16.3, we will focus on evolutionary techniques for explaining machine learning models. Specifically, we will cover data visualization, feature selectionand engineering, local and global explanations, counterfactual and adversarial examples. We will also discuss evolutionary methods designed specifically for explaining deep learning models, as well as evolutionary methods for assessing the quality of explanations themselves. In Sect.  16.4, we identify some challenges remaining in the field of XAI/XML and discuss future opportunities for incorporating EC. Finally, Sect.  16.5provides concluding remarks for the chapter.
16.2 Explainable Machine Learning
Explainabilityand the fields of XAI/XML are concerned with extracting insightsfrom machine learning models in order to make them more transparent, interpretable and understandable to humans. The goal of XAI/XML is to develop methods that can provide clear explanations of the decision-making process and enable humans to understand what relationships a model has learned and how the model arrived at its conclusions.
More concretely, the types of questions we wish to answer with an explanation include  [4]:
Are the patterns the model is drawing on to make its prediction the ones we expect?
Why did the model make this prediction instead of a different one, and what would it take to make it change its prediction?
Is the model biased and are the decisions made by the model fair?
Explanations can be provided in many forms; examples include visualizations, numerical values, data instances, or text explanations [37].
16.2.1 Interpretability Versus  Explainability
The terms interpretabilityand explainabilityare often used interchangeably by researchers. However, in this chapter we distinguish between them as referring to two different but related aspects of attempting to understand a model  [30, 41].
For our purposes, interpretabilityrefers to the level of transparencyand ease of understanding of a model’s decision-making process. An interpretablemodel is one whose decisions can be traced and understood by a human, for example, by having a simple symbolic representation. In other words, a model is considered interpretable if its decision-making process can be understood simply by inspecting the model. For example, small decision trees are often considered interpretable because it is feasible for a human to follow the exact steps leading to the prediction.
On the other hand, even if we cannot trace the exact logic, a model can still be considered explainableif a human-understandable explanation can be provided for what the model is doing or why a decision is made. The more comprehensive and understandable the explanation is, the more explainable the overall system is. Explanations can be provided for the overall patterns and relationships learned by a model, or for the logic used to produce an individual prediction. Some practical methods for providing explanations include evaluating feature importance, visualizing intermediate representations, or comparing the prediction to be explained with samples from the training set. An explanation essentially acts as an interface between the human and the model, attempting to approximate the behavior or decisions made by the model in a way that is comprehensible to humans. As such, this “model of the model” must be both an accurate proxy for the model in the areas of interest but also be understandable itself.
These two terms, interpretabilityand explainability, also roughly correspond to two paradigms for improving the understandability of a model. The first approach, often known as “intrinsic interpretability”, “glass-box” or “interpretabilityby design”, focuses on the interpretabilityof the model and aims to improve it by simplifying or otherwise restricting its complexity  [34]. A simple enough model allows the exact logic to be easily followed and understood by humans, but over-reducing the model complexitycan also restrict the performance of the model. In some cases, it might not be possible to construct a simple enough model that can still effectively solve the problem at hand.
The second approach, known as “post-hoc” or “black-box” methods, focuses on explainabilityand aims to provide explanations by analyzing the model after it has been trained. This approach does not restrict the performance of the model in any way, but also makes it more challenging to provide informative explanations. These explanation methods are typically independent of the original model and are learned after the initial model has been trained  [34]. In this chapter, we will primarily focus on this approach to understanding models. However, in some cases, an explanation for a complex model can take the form of simpler model; in this case, it is important to consider the interpretabilityof the explanation as well.
It is also worth noting that these two goals are not mutually exclusive, and both approaches can be used to better understand a model. The overall goal of explainabilityis to develop better tools to understand machine learning models, while the goal of interpretabilityis to construct models that can be understood using the tools we have.
16.2.2 Explanation Methods
There are several types of explanations, which focus on different aspects of the modeling process (Fig.  16.1). In this section, we will provide an overview of various categories, and describe some examples of each. This overview is not meant to be exhaustive, but rather to highlight the various areas where XAI techniques can be applied. For a more intensive survey of current methods in XAI, we direct the reader to recent reviews [11, 42] on the topic.
Fig. 16.1
Overview of the process of building a machine learning model, showing areas where explanations (magnifying glasses) are often applied. Examples of methods in each category are described in Sect.  16.2. Also shown is the intrinsic interpretability approach (cogwheel), where models are designed to be interpretable from the start. All these methods can be used together to form a more complete picture of a model’s behavior
16.2.2.1 Explaining Data
Althoughthis category is sometimes omitted in discussions of explainable machine learning, it is worth mentioning as part of the overall pipeline. Methods under this category do not necessarily explain the model itself but aim to explain the underlying data that a model is trained on, focusing on understanding the data distribution and its characteristics. Techniques such as exploratory analysis, data visualization and dimensionality reductioncan be used to gain a better understanding of the patterns in the underlying data that the model might learn, as well as identify any potential biases. Examples of these techniques include Principal Component Analysis (PCA)  [18, 39] and t-Distributed Stochastic Neighbor Embedding (t-SNE)  [54], which reduce the dimensionality of data to allow for easy visualization. In addition, methods such as clustering and outlier detectioncan help identify patterns or anomalies in the data that may impact the model’s performance, and can aid in feature selectionand engineering. These explanations can help identify data quality issues, biases, and preprocessing requirements, as well as build trust.
16.2.2.2 Explaining Features
This approach aims to explain the dependence of a model on each feature it uses. For example, feature importancereturns a score that represents the significance of each feature to the model. This helps to identify which features have the greatest impact on the model’s predictions and provides insightsinto how the model is making decisions. This type of explanation can also be used to verify whether the model is behaving as expected—for example, by checking whether it is using the same features a human would solve the problem. In the case of a computer vision model, this type of explanation can be used to determine if the features being used to classify a particular image as a cat make sense, or if the model is using spurious patterns in the data, such as identifying the cat based on its surroundings. This type of explanation can also aid in optimizing models and performing feature selectionby identifying less important features.
Some models, such as decision trees and tree ensembles like random forests, provide feature importancemeasures  [6]. For other models, Shapley additive explanations (SHAP)  [31] attempts to provide a universal method for assessing feature importancethat can be applied to most machine learning models. This is based on the Shapley value, a concept from cooperative game theory that assigns a value to each player in a game based on their contribution to the overall outcome. In the context of machine learning, the “players” are the features in the data, and the “game” is the prediction task. The Shapley value scores each feature based on its contribution to each prediction. The exact calculation of Shapley values is usually computationally impractical, as it involves evaluating every possible combination of features. However, SHAP proposes approximating these values using sampling and regression, making the estimation of feature importancecomputationally feasible. This method is widely used in the field of XAI.
16.2.2.3 Explaining Model Internals
Methodsin this category attempt to explain the internal function of the model, for example, by inspecting the structure of a tree or the weights in a neural network. These approaches can either attempt to explain the entire model, or to understand smaller components of the model.
Explaining the full model can be done by training a secondary model which both approximatesthe original model and is more interpretable. An example of this approach was proposed by Lakkaraju et al.  [26]. Their approach approximatesthe behavior of the model using a small number of decision sets, providing an interpretable proxy for the entire model. While approximating the full model in this and similar ways is effective for smaller models, producing accurate but interpretable proxy modelsbecomes increasingly difficult as the size of the model increases. For large models with sufficiently complex behavior, the proxy modelwill either be a poor approximation or will itself be so large that it becomes difficult to interpret.
This difficulty has led to the development of methods which attempt to explain smaller sub-components of a large model. Examining sub-components can allow us to break down the overall function in a modular way, or to identify the parts of a model which are responsible for certain decisions. For example, one way to explain the function of a neuron in a neural network is by finding or constructing an input that will maximize its activation  [32]. This produces an idealized version of the input that the neuron activates on.
In recent years, many methods have been developed for explaining the internals of deep learning models  [43]. This is due to the rise in popularity of such models and their inherent black-box nature and large size, making explanations for them challenging but particularly important. As an example of one such method, Interpretable Lens Variable Models  [1] train an invertible mappingfrom a complex internal representation inside a neural network (the latent spacein a generative or discriminative model) to a simpler, interpretable one. A user supplies some side information in the form of a few examples which illustrate the properties they are interested in being represented explicitly, such as rotation and scaling. Then, the mappingis learned while constraining certain dimensions in the interpretable representation to obey a linear relationship with the side information. Since the mappingis invertible, the output of the model can also be controlled by making changes to the interpretable representation. The authors’ experiments showed that this method can enable the user to control these properties of the images generated by the model in an intuitive way.
16.2.2.4 Explaining Predictions
This typeof approach aims to explain a specific prediction made by a model. As such, the explanation only needs to capture the behavior of the model with respect to the prediction in question, rather than the model as a whole.
One popular approach in this category is Local Interpretable Model-agnostic Explanations (LIME)  [40]. LIME explains a prediction by sampling a set of instances similar to the input to be explained. It obtains predictions for each of these instances and then fits a linear model to the sampled set of inputs and predictions. This creates a linear approximation for the model in the local neighborhood surrounding the instance to be explained. Although this explanation does not necessarily reflect the global behavior of the model, it is locally faithful and allows us to understand the behavior of the model around that point.
Counterfactualexplanations are another type of explanation which provide information through a hypothetical example in which the model would have made a different decision. For example, “the model would have approved a loan if your income were $5000 higher” is acounterfactualillustrating how the input would need to be changed in order to get a different result from the model  [56]. This type of explanation is intuitive and can be performed on a model in a black-box manner, without any access to a model’s internals. One advantage of counterfactualexplanations is that they can provide users with recourse, a concrete set of changes that could be made to change the decision to a different one  [23]. Additionally, because they directly operate on the model’s evaluation, they are always faithful to the model’s true behavior. However, because they consist of single instances or data points, they provide less insightinto the model’s overall behavior compared to a more comprehensive explanation.
Diverse CounterfactualExplanations (DiCE)  [35] is one common method of constructing counterfactual predictions. The aim of this method is to produce counterfactualswhich are valid (produce a different result when fed into the model), proximal (are similar to the input), and diverse (different from each other). Diversityis desirable here as it increases the likelihood of finding a useful explanation and provides a more complete picture of the model’s behavior. DiCE generates a diverse set of counterfactualexamples using a diversity metric based on determinantal point processes  [24], a probabilistic model which can solve subset selection problems under diversityconstraints. This diversity constraint forces the various examples apart, while an additional proximity constraint forces the examples to lie close to the original input. The method also attempts to make the counterfactualexamples differ from the input in as few features as possible (feature sparsity).
16.2.2.5 Other Considerations
When choosing an explanation method, there are some other factors to consider, such as whether a method is model-specificor model-agnostic, and whether it provides globalor localexplanations.
Whether an explanation is considered model-specific or model-agnostic is based on whether the technique is restricted to certain types of models, or if it can be applied to multiple types of models. For instance, some methods of generating feature importancemeasures  [6] are specific to random forests and cannot be directly used to generate feature importancefor a neural network. Conversely, some methods are designed for extracting information from the internal representation of neural networks, and cannot be applied to random forests. These methods are model-specific. On the other hand, Shapley additive explanations (SHAP) can be applied to assess feature importancefor any of the commonly used machine learning models, making them model-agnostic.
In addition, explanations can be local or global. Local explanationsaim to provide an explanation for a specific input or group of inputs, while global explanationsaim to explain the behavior of the entire model. For instance, a global explanationtechnique such as partial dependence plots  [16] or feature importanceidentifies influential features across the entire training dataset or model. On the other hand, LIME fits its linear model to the neighborhood of a specific prediction, making it a local explanationas it only deals with the behavior of the model in the specific region near the prediction. While global explanationsmay appear to be preferable to local explanations, in practice explanations are always constrained by the amount of information a human can grasp at one time. As such, while global explanationscan provide information about overall trends in the data or model, local explanationsare useful for providing more detail about specific aspects of interest.
16.3 Evolutionary Approaches to Explainable Machine Learning16.3.1 Why Use EC?
Evolutionary computing (EC) is an approach to automatic adaptation inspired by the principles of biological evolution that has been successfully applied to various fields, including optimization, learning, engineering design, and artificial life. This approach mimics the process of natural selection, allowing the computer to evolve and optimize solutions to complex problems by combining and improving upon existing solutions. Common techniques within EC include evolutionary algorithms such as genetic algorithms, genetic programming, evolution strategies, and swarm intelligence algorithms such as particle swarm optimization. These techniques use iterative processes of selection, recombination, mutation, and adaptation to improve the performance of the models.
EC has unique strengths that make it well-suited for handling the challenges of XAI. First of all, EC can work with symbolic representations or interpretable modelssuch as decision trees or rule systems. This can be used to guarantee interpretabilityof the evolved representation. If EC is used to evolve an explanation using interpretable components, the explanation will be interpretable. To leverage this, a common approach is to evolve an interpretable approximation of the model, in whole or in part.
Evolutionary methods are also highly flexible, being able to perform black-box optimization, and usually being derivative-free. As a result, they can be applied to a variety of models without requiring knowledge of the model’s internal logic—for example, when access to a model is only available through an API which returns only the predictions. This flexibilityalso enables the optimization of unusual or customized metrics without requiring in-depth knowledge of how to optimize the metric. Additionally, the flexibilityof evolutionary methods allows them to be used to create hybrid methods with other algorithms or to serve as meta-learning methods.
EC is advantageous for XAI as it can handle multiple objectives simultaneously. This is important as explanations by nature require both faithfulness to the model as well as interpretabilityto humans. The population-based nature of evolutionary algorithms also enables explicit use of diversitymetrics. This allows us to explore a range of different explanations, increasing the chances of finding a useful one.
We will now delve into current methods which employ EC for producing explanations, and provide an overview of these approaches and their applications. There are a number of different ways to generate explanations, and as before, we will discuss them based on the stage in the machine learning modeling process in which they can be applied, highlighting works of interest. For an extensive survey on these methods, we invite the reader to explore recent reviews  [4, 33] on the use of evolutionary methods in XAI.
16.3.2 Explaining Data
EC can be used to explain databy means of dimensionality reductionand visualization. One approach is GP-tSNE  [27], which adapts the classic t-SNE  [54] algorithm to use evolved trees to provide an interpretable mappingfrom the original data points to the embedded points. Similarly, Schofield and Lensen  [46] use tree-GP to produce an interpretable mappingfor Uniform Manifold Approximation and Projection (UMAP). By producing an explicit mapping function rather than simply the embedded points, we can not only make the process more transparent but also reuse the mapping on new data.
In some cases, we may want to use the lower dimensional representation for prediction as well as visualization. This is useful for interpretabilityas it allows us to visualize exactly the same data representation that the model sees. Therefore, another approach is to construct features which are both amenable to visualization and well-suited for downstream tasks. Icke and Rosenberg  [22] proposed a multi-objective GP algorithm to optimize three objective measures desirable for constructed features—classifiability, visual interpretabilityand semantic interpretability. Similarly, Cano et al.  [8] developed a method using multi-objective GP to construct features for visualization and downstream analysis, optimizing for six classification and visualization metrics. The classification metrics (accuracy, AUCand Cohen’s kappa rate) aim to improve the performance of the downstream classifier, while the visualization metrics (C-index, Davies-Bouldin index, Dunn’s index) aim to improve the clustering and separability of the features.
16.3.3 Feature Selection and Feature Engineering
Featureselectionis a common preprocessing step where a relevant subset of features is selected from the original dataset. This is used to improve the performance of the model, but also has benefits for interpretabilityby narrowing down the features the model can draw on. As an explanation, feature selectionshares some similarities with feature importance, which identifies the features a model is drawing on, but instead restricts the model explicitly so it can only draw on the chosen features.
Genetic algorithms are a straightforward and effective approach to feature selection, with a natural representation in the form of strings of 1s and 0s, making them a popular choice for feature selection[44, 49, 59]. Genetic programming can also be used for feature selectionsince the inclusion of features in a tree or linear genetic program is intrinsically evolved with the program  [19, 20, 48]. For an in-depth review of genetic programming methods, we refer the reader to  [58]. Swarm intelligence methods, such as particle swarm optimization, have been applied to feature selectionas well  [60]. For a more detailed review of these methods, we direct the reader to  [38]. In addition to selecting features for a model, feature selectioncan also be used to improve data understandingby integrating it with techniques such as clustering   [15].
Feature engineering, also referred to as feature construction, is a related approach that involves building higher-level condensed features out of basic features. Genetic programming can be used to evolve these higher-level features for downstream tasks such as classification and regression  [25, 29, 36, 55]. This can also help improve the interpretabilityof a model, since this can reduce a large number of low-level features to a smaller number of higher-level features which may be easier to understand for humans. Moreover, it removes some of the modeling out of the black-box and into a transparent step beforehand, thereby reducing the amount of explanation needed.
These methods also share many similarities with dimensionality reductiontechniques, and in some cases can fall under both categories. Uriot et al.  [53] compared a variety of multi-tree-GP algorithms for dimensionality reduction, as well as a tree-based autoencoderarchitecture. In the multi-tree representation, each individual in the population is a collection of trees with each tree mapping the input to one feature in the latent dimension. In order to reconstruct the input for the autoencoder, a multi-tree decoder is simultaneously evolved with one tree per input dimension. Their results showed that GP-based dimensionality reductionwas on par with the conventional methods they tested (PCA, LLE, and Isomap).
16.3.4 Model Extraction
Thisapproach, also known as a global surrogate model, aims to approximatea black-box model with an evolved interpretable model. This idea is closely related to knowledge distillation  [7, 17] in deep learning, but rather than simply making the model smaller we also want to make it more interpretable.
