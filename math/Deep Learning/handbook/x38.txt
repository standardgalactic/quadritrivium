In recent years, significant advancements have been made in developing high-performance DNNs that have achieved remarkable results. There is a common belief that “the deeper, the better”, meaning that deeper models often perform better than shallower ones  [54]. Therefore, researchers focus on developing deeper DNNs to achieve better accuracy. However, the explosive growth of edge devices such as mobile phones, wearables, and robotic devices in the current Internet of Things (IoT)era has driven the research to quest machine learning platforms that not only prioritize accuracy but also optimize storage and computational requirements. For these IoT devices, the primary focus is on the computational resources that the model will consume, not the optimal performance that the model will achieve. Thus, many DNN models with excellent performance cannot be directly applied to these IoT devices because of their computational complexity. As a result, designing models under the resource constraints of IoT devices has become an advanced research topic.
There are two ways to design such models: manually with domain expertise and automatically using the NASmethod. Manual design focused on lightweight models like MobileNet  [22] and ShuffleNet  [66] for CNNs, which perform well on specific datasets but struggle with new datasets. Multiobjective optimization methods are commonly used in NAS to design lightweight DNN models  [20] automatically. The multiobjective EC can simultaneously optimize competing objectives, such as accuracy and other important objectives, under limited resources. For example, NSGA-Net  [37] is a classical ENAS algorithm that applies the nondominated sorting genetic algorithm (NSGA-II)to optimize both accuracy and complexity. Moreover, to address the problem that multiobjective EC cannot find a feasible solution under the specific constraint, Li  et al. proposed GA-based CH-CNN  [31], which automatically designs CNNarchitectures and achieves state-of-the-art performance.
However, there are still some challenges on this topic. First, there are various types of DNNs, including DBNs, RNNs, and GNNs, each with significant differences. There is no universal constraint optimization method for all networks. Second, real-world applications often require constraints on multiple metrics, such as energy consumption, spectrum efficiency, and latency. Finally, different hardware platforms have preferences for different network architectures, so it is necessary to consider the specific constraints of each hardware platform.
9.4.4 Interpretability of  ENAS
Generally, neural networks arerecognized as black-box models with weak interpretability. This shortcoming greatly limits the application of neural networks in some crucial fields, such as security, health, and finance, because these fields have high requirements for interpretability. Although there were some attempts to visualize the feature extraction process of the neural networks  [64], the effect/impact is limited. The process is still uninterpretable due to large quantities of the learned features  [15].
In order to obtain interpretable solutions, in the ENAS community, GP is a commonly adopted technique, which has long been praised for its outstanding ability to design interpretable models. Specifically, GP is a kind of evolutionary method for automatically designing tree-structured models  [27]. Commonly, the models obtained by GP algorithms are much simpler than neural networks, and their tree-like structures also further enhance their interpretability
Consequently, some works  [4, 15, 16] utilized GP to design neural networks automatically. Furthermore, they also provided some methods to analyze the interpretabilityof the evolved solutions. For example, Evans  et al.  [15] explained that the convolution filters in their evolved solutions played the role of edge detection in the JAFFE dataset  [7], and the large presence of white color in the output resulting from the convolution operation could help the classification. In their subsequent work  [16], they also explained that the minimum value of a specific area in the hand image extracted by the aggregation function could be used to determine whether the hand is open or closed. Additionally, Bi  et al.  [4] visualized the features after each function of the searched model. Their research revealed that each branch of the tree-like solutions can generate different salient features which are discriminative for face classification.
Despite their interpretability, most efforts are limited to generating shallow neural networks for simple tasks whose feature number is relatively small. It is hard to transfer existing methods to complex tasks with hundreds of input variablesfor four reasons: Firstly, when facing complex tasks, the flexible representation of GP tends to generate overcomplex models that overfit the training data, and the performance of the solutions may degrade. Secondly, complex tasks require larger search spaces containing redundant expressions, and GP often leads to unnecessarily large solutions which are difficult to analyze. Thirdly, the search space grows exponentially as the dimension of the input variablesincreases, resulting in extremely high computational complexity. Last but not the least, a few GP-based methods can fully use the GPUsfor acceleration, making the algorithms time-consuming.
9.4.5 Theoretical Analysis of  ENAS
Despite the success of ENAS, there is still a significant lack of research on theoretical foundations. In the last two decades, there has been a significant increase in EC theoretical studies  [67], particularly on the runtime of EAs, which represents the average computational complexity and is thus a core theoretical issue in randomized search heuristics. Most runtime studies begin by analyzing different mutation operators on pseudo-Boolean functions  [10]. Furthermore, to improve the ability of EA global exploration, researchers have theoretically demonstrated the importance of the crossover operator in this regard. Specifically, recent works  [11] have proven that crossover can provide a noticeable speedup on classical problems like OneMaxand . Whatever the runtime study goal, the idea behind the above runtime proof is a mathematical evaluation of the runtime boundsusing well-known tools, e.g., Markov chain, Chernoff, hitting time, and takeover time.
Most of the studies about EC runtime analysisare based on solving classical benchmark problems. However, as a complex (mostly combinatorial) optimization problem, ENAS has more bottlenecks that must be considered first. One notable bottleneck is the lack of benchmark functions that are crucial in evaluating and comparing optimization algorithms. There is an urgent need to propose insightful benchmark functions tailored to the ENAS. In addition, since no work focuses on the theory of ENAS, bridging the gap between the ENAS algorithm and theoretical research tools and methods is another urgent need. Various works have been devoted to the general EAs runtime analysismethodologies  [67], including the fitness-level method, the switch analysis, and drift method. By leveraging these methodologies, future research can explore the theoretical analysis of ENAS algorithms. In a nutshell, theoretical research on ENAS is a feasible and significant topic that can advance the development of ENAS and accelerate its acceptance by the general machine learning community.
9.5 Conclusions
This chapter provides a comprehensive introduction to ENAS from four aspects. In the first section, we showed that it is a trend to design neural architectures automatically. To address the problem, NAS is proposed. Compared with RL-based NASand gradient-based NAS, ENAS is an outstanding method because it requires neither a prior construction of the supernetnor domain-rich expertise. In the second section, to begin with, ENAS was detailed with its three core components: search space, search strategy, and performance evaluation. After that, a step-by-step example was given, following the standard process of population initialization, population updating, and fitness evaluation. In the third section, state-of-the-art ENAS techniques were discussed from the perspective of its core components. In the fourth section, advanced topics of ENAS were addressed, including benchmarks of ENAS, efficient evaluation, resource-constraint ENAS, interpretabilityof ENAS, and theoretical analysis of ENAS.
The success of state-of-the-art ENAS methods has sparked discussions on various advanced topics, aiming to advance the field further. These topics include the establishment of a benchmark platform to facilitate fair comparisons of ENAS techniques, the development of efficient evaluationtechniques for efficient and accurate assessments, the explorationof resource-constrained ENAS algorithms tailored for IoT devices, the investigation of interpretabilitymethods for complex tasks in ENAS, and the theoretical analysis of ENAS algorithms. Diving into these advanced topics will play a crucial role in accelerating the development of ENAS algorithms.
References
1.
Assunção, F., Correia, J., Conceição, R., Pimenta, M.J.M., Tomé, B., Lourenço, N., Machado, P.: Automatic design of artificial neural networks for gamma-ray detection. IEEE Access 7, 110531–110540 (2019)Crossref
2.
Bäck, T., Fogel, D.B., Michalewicz, Z.: Handbook of evolutionary computation. Release 97(1), B1 (1997)zbMATH
3.
Bender, G., Kindermans, P.J., Zoph, B., Vasudevan, V., Le, Q.: Understanding and simplifying one-shot architecture search. In: International Conference on Machine Learning, pp. 550–559. PMLR (2018)
4.
Bi, Y., Xue, B., Zhang, M.: An evolutionary deep learning approach using genetic programming with convolution operators for image classification. In: 2019 IEEE Congress on Evolutionary Computation (CEC), pp. 3197–3204. IEEE (2019)
5.
Broni-Bediako, C., Murata, Y., Mormille, L.H., Atsumi, M.: Evolutionary NAS with gene expression programming of cellular encoding. In: 2020 IEEE Symposium Series on Computational Intelligence (SSCI), pp. 2670–2676. IEEE (2020)
6.
Chen, Z., Zhou, Y., Huang, Z.: Auto-creation of effective neural network architecture by evolutionary algorithm and resnet for image classification. In: 2019 IEEE International Conference on Systems, Man and Cybernetics (SMC), pp. 3895–3900. IEEE2019
7.
Cheng, F., Jiangsheng, Yu., Xiong, H.: Facial expression recognition in Jaffe dataset based on Gaussian process classification. IEEE Trans. Neural Netw. 21(10), 1685–1690 (2010)Crossref
8.
Deng, B., Yan, J., Lin, D.: Peephole: Predicting network performance before training (2017). arXiv:​1712.​03351
9.
Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: Bert: pre-training of deep bidirectional transformers for language understanding (2018). arXiv:​1810.​04805
10.
Doerr, B., Doerr, C.: Theory of parameter control for discrete black-box optimization: Provable performance gains through dynamic parameter choices. Theory of Evolutionary Computation, pp. 271–321 (2020)
11.
Doerr, B., Happ, E., Klein, C.: Crossover can provably be useful in evolutionary computation. In: Proceedings of the 10th Annual Conference on Genetic and Evolutionary Computation, pp. 539–546 (2008)
12.
Dong, X., Yang, Y.: Nas-bench-201: extending the scope of reproducible neural architecture search. In: International Conference on Learning Representations (2019)
13.
Elsken, T., Metzen, J.H., Hutter, F.: Simple and efficient architecture search for convolutional neural networks (2017). arXiv:​1711.​04528
14.
Elsken, T., Metzen, J.H., Hutter, F.: Neural architecture search: a survey. J. Mach. Learn. Res. 20(1), 1997–2017 (2019)
15.
Evans, B., Al-Sahaf, H., Xue, B., Zhang, M.: Evolutionary deep learning: a genetic programming approach to image classification. In: 2018 IEEE Congress on Evolutionary Computation (CEC), pp. 1–6. IEEE (2018)
16.
Evans, B.P., Al-Sahaf, H., Xue, B., Zhang, M.: Genetic programming and gradient descent: A memetic approach to binary image classification (2019). arXiv:​1909.​13030
17.
Fan, Z., Wei, J., Zhu, G., Mo, J., Li, W.: Evolutionary neural architecture search for retinal vessel segmentation (2020). arXiv:​2001.​06678
18.
Floreano, D., Dürr, P., Mattiussi, C.: Neuroevolution: from architectures to learning. Evol. Intel. 1(1), 47–62 (2008)Crossref
19.
Fujino, S., Naoki, M., Matsumoto, K.: Deep convolutional networks for human sketches by means of the evolutionary deep learning. In: 2017 Joint 17th World Congress of International Fuzzy Systems Association and 9th International Conference on Soft Computing and Intelligent Systems (IFSA-SCIS), pp. 1–5. IEEE (2017)
20.
Gunantara, N.: A review of multi-objective optimization: methods and its applications. Cogent Eng. 5(1), 1502242 (2018)Crossref
21.
He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 770–778 (2016)
22.
Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M., Adam, H.: Mobilenets: efficient convolutional neural networks for mobile vision applications (2017). arXiv:​1704.​04861
23.
Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q.: Densely connected convolutional networks. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4700–4708 (2017)
24.
Irwin-Harris, W., Sun, Y., Xue, B., Zhang, M.: A graph-based encoding for evolutionary convolutional neural network architecture design. In: 2019 IEEE Congress on Evolutionary Computation (CEC), pp. 546–553. IEEE (2019)
25.
Johner, F.M., Wassner, J.: Efficient evolutionary architecture search for CNN optimization on gtsrb. In: 2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA), pp. 56–61. IEEE (2019)
26.
Kaelbling, L.P., Littman, M.L., Moore, A.W.: Reinforcement learning: a survey. J. Artif. Intell. Res. 4, 237–285 (1996)
27.
Koza, J.R.: Genetic programming as a means for programming computers by natural selection. Stat. Comput. 4(2), 87–112 (1994)
28.
Krizhevsky, A., Hinton, G.: Learning multiple layers of features from tiny images. Handbook Syst. Autoimmune Dis. 1(4) (2009)
29.
Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. Commun. ACM 60(6), 84–90 (2017)
30.
LeCun, Y., Bengio, Y., Hinton, G.: Deep learning. Nature 521(7553), 436–444 (2015)Crossref
31.
Li, S., Sun, Y., Yen, G.G., Zhang, M.: Automatic design of convolutional neural network architectures under resource constraints, IEEE Trans. Neural Netw. Learn. Syst. (2021)
32.
Liu, H., Simonyan, K., Yang, Y.: Darts: Differentiable architecture search (2018). arXiv:​1806.​09055
33.
Liu, J., Gong, M., Miao, Q., Wang, X., Li, H.: Structure learning for deep neural networks based on multiobjective optimization. IEEE Trans. Neural Netw. Learn. Syst. 29(6), 2450–2463 (2017)MathSciNetCrossref
34.
Liu, P., El Basha, M.D., Li, Y., Xiao, Y., Sanelli, P.C., Fang, R.: Deep evolutionary networks with expedited genetic algorithms for medical image denoising. Med. Image Anal. 54, 306–315 (2019)
35.
Liu, Y., Sun, Y., Xue, B., Zhang, M., Yen, G.G., Tan, K.C.: A survey on evolutionary neural architecture search, IEEE Trans. Neural Netw. Learn. Syst. (2021)
36.
Loni, M., Sinaei, S., Zoljodi, A., Daneshtalab, M., Sjödin, M.: Deepmaker: a multi-objective optimization framework for deep neural networks in embedded systems. Microprocess. Microsyst. 73, 102989 (2020)Crossref
37.
Lu, Z., Whalen, I., Boddeti, V., Dhebar, Y., Deb, K., Goodman, E., Banzhaf, W.: NSGA-Net: neural architecture search using multi-objective genetic algorithm. In: Proceedings of the Genetic and Evolutionary Computation conference, pp. 419–427 (2019)
38.
Lu, Z., Whalen, I., Dhebar, Y., Deb, K., Goodman, E.D., Banzhaf, W., Boddeti, V.N.: Multiobjective evolutionary design of deep convolutional neural networks for image classification. IEEE Trans. Evol. Comput. 25(2), 277–291 (2020)
39.
Rawal, A., Miikkulainen, R.: From nodes to networks: evolving recurrent neural networks (2018). arXiv:​1803.​04439
40.
Real, E., Moore, S., Selle, A., Saxena, S., Suematsu, Y.L., Tan, J., Le, Q.V., Kurakin, A.: Large-scale evolution of image classifiers. In: International Conference on Machine Learning, pp. 2902–2911. PMLR (2017)
41.
Ruder, S.: An overview of gradient descent optimization algorithms (2016). arXiv:​1609.​04747
42.
Russakovsky, O., Deng, J., Hao, S., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al.: Imagenet large scale visual recognition challenge. Int. J. Comput. Vision 115(3), 211–252 (2015)MathSciNetCrossref
43.
