Platel, M.D., Clergue, M., Collard, P.: Maximum homologous crossover for linear genetic programming. In: European Conference on Genetic Programming, pp. 194–203. Springer (2003)
91.
Real, E., Aggarwal, A., Huang, Y., Le, Q.V.: Regularized evolution for image classifier architecture search. In: Proceedings of the AAAI Conference on Artificial Intelligence, vol.  33, pp. 4780–4789 (2019)
92.
Sakprasat, S., Sinclair, M.C.: Classification rule mining for automatic credit approval using genetic programming. In: IEEE Congress on Evolutionary Computation, pp. 548–555 (2007)
93.
Sapra, D., Pimentel, A.D.: An evolutionary optimization algorithm for gradually saturating objective functions. In: Proceedings of the Genetic and Evolutionary Computation Conference, pp. 886–893 (2020)
94.
Shrivastava, N.A., Khosravi, A., Panigrahi, B.K.: Prediction interval estimation of electricity prices using PSO-tuned support vector machines. IEEE Trans. Industr. Inf. 11(2), 322–331 (2015). https://​doi.​org/​10.​1109/​TII.​2015.​2389625Crossref
95.
Silva, S., Costa, E.: Dynamic limits for bloat control in genetic programming and a review of past and current bloat theories. Genet. Program Evolvable Mach. 10(2), 141–179 (2009)
96.
Sun, C., Jin, Y., Zeng, J., Yu, Y.: A two-layer surrogate-assisted particle swarm optimization algorithm. Soft. Comput. 19(6), 1461–1475 (2015)
97.
Sun, Y., Yen, G.G., Yi, Z.: Evolving unsupervised deep neural networks for learning meaningful representations. IEEE Trans. Evol. Comput. 23(1), 89–103 (2019)
98.
Tackett, W.A.: Recombination, selection, and the genetic construction of computer programs. Ph.D. thesis, University of Southern California, Department of Electrical Engineering Systems, USA (1994)
99.
Tanigawa, T., Zhao, Q.: A study on efficient generation of decision trees using genetic programming. In: Proceedings of the 2nd Annual Conference on Genetic and Evolutionary Computation, GECCO’00, p. 1047-1052. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA (2000)
100.
Tong, H., Huang, C., Minku, L.L., Yao, X.: Surrogate models in evolutionary single-objective optimization: A new taxonomy and experimental study. Inf. Sci. 562, 414–437 (2021)MathSciNet
101.
Tran, B., Xue, B., Zhang, M.: Class dependent multiple feature construction using genetic programming for high-dimensional data. In: Australasian Joint Conference on Artificial Intelligence, pp. 182–194. Springer (2017)
102.
Tran, B., Xue, B., Zhang, M.: Genetic programming for multiple-feature construction on high-dimensional classification. Pattern Recogn. 93, 404–417 (2019)
103.
Tran, C.T., Zhang, M., Andreae, P.: Multiple imputation for missing data using genetic programming. In: Proceedings of the Annual Conference on Genetic and Evolutionary Computation, pp. 583–590 (2015)
104.
Tran, C.T., Zhang, M., Andreae, P.: Directly evolving classifiers for missing data using genetic programming. In: IEEE Congress on Evolutionary Computation (CEC), pp. 5278–5285 (2016)
105.
Tran, C.T., Zhang, M., Andreae, P.: A genetic programming-based imputation method for classification with missing data. In: European Conference on Genetic Programming, pp. 149–163. Springer (2016)
106.
Tran, C.T., Zhang, M., Andreae, P., Xue, B.: Directly constructing multiple features for classification with missing data using genetic programming with interval functions. In: Proceedings of the 2016 on Genetic and Evolutionary Computation Conference Companion, pp. 69–70 (2016)
107.
Tsakonas, A., Dounias, G., Jantzen, J., Axer, H., Bjerregaard, B., von Keyserlingk, D.G.: Evolving rule-based systems in two medical domains using genetic programming. Artif. Intell. Med. 32(3), 195–216 (2004)
108.
Urbanowicz, R.J., Browne, W.N.: Introduction to Learning Classifier Systems. Springer (2017)
109.
Urbanowicz, R.J., Granizo-Mackenzie, A., Moore, J.H.: An analysis pipeline with statistical and visualization-guided knowledge discovery for Michigan-style learning classifier systems. IEEE Comput. Intell. Mag. 7(4), 35–45 (2012)
110.
Urbanowicz, R.J., Meeker, M., La Cava, W., Olson, R.S., Moore, J.H.: Relief-based feature selection: Introduction and review. J. Biomed. Inform. 85, 189–203 (2018)
111.
Urquhart, M., Ljungskog, E., Sebben, S.: Surrogate-based optimisation using adaptively scaled radial basis functions. Appl. Soft Comput. 88, 106050 (2020)
112.
Virgolin, M., Alderliesten, T., Bosman, P.A.: On explaining machine learning models by evolving crucial and compact features. Swarm Evol. Comput. 53, 100640 (2020)
113.
Vladislavleva, E.J., Smits, G.F., Den Hertog, D.: Order of nonlinearity as a complexity measure for models generated by symbolic regression via pareto genetic programming. IEEE Trans. Evol. Comput. 13(2), 333–349 (2008)
114.
Wang, B., Sun, Y., Xue, B., Zhang, M.: A hybrid differential evolution approach to designing deep convolutional neural networks for image classification. In: Australasian Joint Conference on Artificial Intelligence, pp. 237–250. Springer (2018)
115.
Wang, H., Jin, Y., Jansen, J.O.: Data-driven surrogate-assisted multiobjective evolutionary optimization of a trauma system. IEEE Trans. Evol. Comput. 20(6), 939–952 (2016)
116.
Wang, P., Xue, B., Liang, J., Zhang, M.: Differential evolution based feature selection: A niching-based multi-objective approach. IEEE Trans. Evolut. Comput. 1–1 (2022). https://​doi.​org/​10.​1109/​TEVC.​2022.​3168052
117.
Wang, S., Mei, Y., Zhang, M., Yao, X.: Genetic programming with niching for uncertain capacitated arc routing problem. IEEE Trans. Evol. Comput. 26(1), 73–87 (2021)
118.
Winkler, S., Affenzeller, M., Wagner, S.: Advanced genetic programming based machine learning. J. Math. Modell. Algorithms 6(3), 455–480 (2007)MathSciNetzbMATH
119.
Wongseree, W., Chaiyaratana, N., Vichittumaros, K., Winichagoon, P., Fucharoen, S.: Thalassaemia classification by neural networks and genetic programming. Inf. Sci. 177(3), 771–786 (2007)
120.
Wood, J., Nguyen, B.H., Xue, B., Zhang, M., Killeen, D.: Automated fish classification using unprocessed fatty acid chromatographic data: a machine learning approach. In: Proceedings of the Australasian Joint Conference on Artificial Intelligence, pp. 516–529. Springer (2022)
121.
Xie, L., Yuille, A.: Genetic cnn. In: Proceedings of the IEEE International Conference on Computer Vision, pp. 1379–1388 (2017)
122.
Xue, B., Zhang, M., Browne, W.N., Yao, X.: A survey on evolutionary computation approaches to feature selection. IEEE Trans. Evol. Comput. 20(4), 606–626 (2015)
123.
Xue, Y., Zhu, H., Liang, J., Słowik, A.: Adaptive crossover operator based multi-objective binary genetic algorithm for feature selection in classification. Knowl.-Based Syst. 227, 107218 (2021)
124.
Yan, L., Dodier, R.H., Mozer, M., Wolniewicz, R.H.: Optimizing classifier performance via an approximation to the wilcoxon-mann-whitney statistic. In: Proceedings of the International Conference on Machine Learning, pp. 848–855 (2003)
125.
Yuan, X., Liu, Z., Miao, Z., Zhao, Z., Zhou, F., Song, Y.: Fault diagnosis of analog circuits based on IH-PSO optimized support vector machine. IEEE Access 7, 137945–137958 (2019). https://​doi.​org/​10.​1109/​ACCESS.​2019.​2943071Crossref
126.
Zeng, N., Qiu, H., Wang, Z., Liu, W., Zhang, H., Li, Y.: A new switching-delayed-PSO-based optimized SVM algorithm for diagnosis of alzheimer’s disease. Neurocomputing 320, 195–202 (2018)
127.
Zhang, B.T., Mühlenbein, H.: Balancing accuracy and parsimony in genetic programming. Evol. Comput. 3(1), 17–38 (1995)
128.
Zhang, M., Ciesielski, V.: Genetic programming for multiple class object detection. In: Advanced Topics in Artificial Intelligence: 12th Australian Joint Conference on Artificial Intelligence, AI’99 Sydney, Australia, December 6–10, 1999 Proceedings 12, pp. 180–192. Springer (1999)
129.
Zhang, M., Gao, X., Lou, W.: A new crossover operator in genetic programming for object classification. IEEE Trans. Syst. Man Cybernet. Part B (Cybernet.) 37(5), 1332–1343 (2007)
130.
Zhang, M., Smart, W.: Multiclass object classification using genetic programming. In: Workshops on Applications of Evolutionary Computation, pp. 369–378. Springer (2004)
131.
Zhang, M., Smart, W.: Using gaussian distribution to construct fitness functions in genetic programming for multiclass object classification. Pattern Recogn. Lett. 27(11), 1266–1274 (2006)
132.
Zhang, M., Wong, P.: Genetic programming for medical classification: a program simplification approach. Genet. Program Evolvable Mach. 9(3), 229–255 (2008)
133.
Zhang, S., Qin, Z., Ling, C., Sheng, S.: “missing is useful’’: missing values in cost-sensitive decision trees. IEEE Trans. Knowl. Data Eng. 17(12), 1689–1693 (2005)
134.
Zhong, W., Zhuang, Y., Sun, J., Gu, J.: A load prediction model for cloud computing using PSO-based weighted wavelet support vector machine. Appl. Intell. 48(11), 4072–4083 (2018)
135.
Zhou, Z.H.: Cost-sensitive learning. In: International Conference on Modeling Decisions for Artificial Intelligence, pp. 17–18. Springer (2011)©  The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.  2024
W. Banzhaf et al.(eds.)Handbook of Evolutionary Machine LearningGenetic and Evolutionary Computationhttps://doi.org/10.1007/978-981-99-3814-8_8
8.  Evolutionary Ensemble Learning
Malcolm  I.  Heywood1
(1)
Faculty of Computer Science, Dalhousie University, Halifax, NS, Canada
Malcolm  I.  Heywood
Email: mheywood@dal.ca
Abstract
Evolutionary Ensemble Learning (EEL) provides a general approach for scaling evolutionary learning algorithms to increasingly complex tasks. This is generally achieved by developing a diverse complement of models that provide solutions to different (yet overlapping) aspects of the task. This chapter reviews the topic of EEL by considering two basic application contexts that were initially developed independently: (1) ensembles as applied to classification and regression problems and (2) multi-agent systemsas typically applied to reinforcement learningtasks. We show that common research themes have developed from the two communities, resulting in outcomes applicable to both application contexts. More recent developments reviewed include EEL frameworks that support variable-sized ensembles, scaling to high cardinality or dimensionality, and operation under dynamic environments. Looking to the future we point out that the versatility of EEL can lead to developments that support interpretable solutions and lifelong/continuous learning.
