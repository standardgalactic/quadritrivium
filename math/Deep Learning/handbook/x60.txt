Fig. 11.10
Amount of niches filled across seeds.The figure shows the percentage of all niches (1,728 in total) that are filled by the end of ELM runs across different seeds. Results are averaged across three independent runs for each seed. In general, nearly all seeds fill the map, although the Square seedproceeds more slowly than other seeds
Beyond diversity, the quality of solutions is also important. A gross measure of quality is the maximum fitness discovered by runs, shown in Fig.  11.11. A more nuanced metric that takes both quality and diversityinto account is the QD score [50], calculated as the sum of the performance of all champions in the final map. This metric, shown averaged over runs in Fig. 11.12, rewards both quality (having higher scores in each niche)and diversity(having discovered more niches), and thus serves as a succinct measure of ELM’s goal of accumulating diverse, high-quality solutions (and in later stages in the pipeline, of how well an LLM has modeled the distribution of solutions that ELM has uncovered). Attainment of QD differs across seeds, while the CPPN seed uncovers diversitymost quickly, the Radial seedgenerates higher quality solutions on average. The relationship between the seed and the products of search is complex and deserves further future study (see also Appendix D of [36],  p. 49 for further analysis of seed robustness).
Fig. 11.11
Maximum fitness across seeds.The maximum performance attained on average by different seeds is shown. The results suggest that ELM’s capacity to find high-fitness solutions is somewhat robust to seed design
Fig. 11.12
Quality diversity score across seeds.Shown is the average final QD score attained by runs initialized from different seeds. The conclusion is that fine-tuning the diff modelhas a significant impact on attained QD score, as does the choice of seed
Fine-tuning the diff modelon accepted diffs from an initial series of runs greatly increased performance (Fig.   11.13), while Sodarace-generating programs are out of distribution for the pretrained diff model(applying a Python encoding to this domain is a novel enterprise), fine-tuning effectively aligns the diff modelwith the domain, an interesting result. Figure  11.13c shows how the fine-tuned diff model produces a significantly higher percentage of diffs that are valid (i.e., able to be applied) and runnable (i.e., the patched program is executable). Because of their higher performance, the outputs of runs applying the fine-tuned diff modelare the ones passed to later stages in the pipeline.
Fig. 11.13
The impact of fine-tuning the diff model on the performance of ELM.For both the pretrained diff modeland the fine-tuned one, shown are athe number of niches reached, bQD score of the produced map, and cpercentage of valid/runnable diffs proposed. The experiments demonstrate that fine-tuning the diff modelimproves performance of the evolutionary process across all three metrics
Note that further rounds of fine-tuning are possible (e.g., fine-tuning the diff modelagain from the improved products of the first round); however, preliminary experiments showed diminishing returns. Future work could explore how to continually improve such models, such as by identifying and encouraging more impactful perturbations instead of including and weighting equally all accepted diffs.
The seeds and fine-tuned diff modelalso qualitatively impact the kinds of solutions discovered by ELM. While the Radial seedperforms well quantitatively (in terms of quality and diversity), it turns out that its products tend to exploit chaotic dynamicsthat seem overfit to the flat terrain (this hypothesis is tentatively validated in the Stage 3 experiments). The Square and CPPN seeds in contrast are more likely to output inventions that leverage more predictable dynamics. For these reasons, the Radial seedruns were not ultimately used in future stages.
A video selection of the highest quality Sodaracers from these initial runs that showcases the considerable diversityuncovered can be viewed at [72]. An example of a lineage of Sodaracers progressing from the Square seedto a high-quality final Sodaracer can be seen at [73]. In short, ELM shows that by combining an intelligent LLM-based mutation operator with a QD algorithm it is possible to generate hundreds of thousands of working training examples in a completely novel domain where no data was previously available.
11.6 Pipeline Stage 2: Language Model Training
The product of Stage 1 is a collection of programs, whereas Stage 3 RL requires an initial model that can output valid Sodaracer-generating programs. Thus, the second stage of the invention pipeline fine-tunes an LLM on the products of ELM, which serves as the initialization for an RL-based conditional inventor. To do so first requires compiling the results of Stage 1 into a fine-tuning dataset.
While there are many ways to distill a dataset of programs from runs of ELM, a simple thresholded approach is adopted here (although see Appendix E of [36],  p. 50 for another simple approach that did not work in practice). The main idea is to append all reasonably capable solutions for each niche
In more detail, from each run all solutions ever admitted to the map are included, subject to meeting a minimal bar for performance. Some parts of the behavior space offer more stringent challenges (i.e., it is more difficult to locomote when required to be tall but not wide and to have low mass), and yet in some terrains encountered in Stage 3, those kinds of solutions may yet be most effective despite their low level of absolute performance. Thus, for each niche, the maximum performance across all runs is calculated, and the minimal bar for inclusion is set as a percentage of that per-niche score. With a higher percentage threshold, less data is included, but the quality of that data will be higher.
As noted in the previous section, solutions from the Radial seedwere qualitatively chaotic. Furthermore, preliminary experiments suggest that such chaotic behavior significantly harms downstream Stage 3 performance. For these reasons, Radial runs of ELM were excluded from the LLM datasets. Datasets for each of the remaining treatments were compiled from nine runs from ELM with the fine-tuned diff model(three runs for each of the Square, CPPN-Fixed, and CPPN-Mutable seeds). In total, the 50% cutoff threshold dataset consisted of 280  K examples, and the 80% cutoff threshold dataset contained a subset of 95  K of those examples.
A variety of pretrained code-generating models were then fine-tuned with these examples (using the standard LLM log-probability loss), leaving out 5% of the data to serve as a test set. Models ranging from 0.1 to 680  M parameters were trained (architectural details for these models can be seen in Appendix C of [36],  p. 48). Also, as a control to support the hypothesis that Sodaracemodels benefit from code-generation pretraining, a 300  M model was also trained instead from a random initialization (signified with “RI” in charts that follow).
Minimum test losses (i.e., loss on generated Sodaracers held-out from the fine-tuning dataset) of the 80% Percentage Threshold models are shown in Fig.  11.14. The 50% Percentage Threshold models exhibit qualitatively similar results across model size (but as both thresholds represent different datasets, loss values are not directly comparable between them). The conclusions are that model sizes above 85  M may not better fit the data, and that random initialization does hurt performance relative to fine-tuning from a model pretrained on code.
Fig. 11.14
Test loss across model sizes.The minimum test loss achieved by training runs on the 80% Percentage Threshold dataset across model sizes is shown. Model sizes above 85  M may not better fit the data, and random initialization hurts performance
However, loss is not the whole story. The interesting question for Stage 2 is whether the LLMs trained from the data generated in Stage 1 can generate the same diversityand quality of data. Therefore, the QD score metric and number of nichesdiscovered (both of which were also reported for Stage 1) are calculated for samples taken from trained LLMs. Because these metrics can be maximized by a model that memorizes the data, and because empirically QD score was more correlated with loss on the training set rather than the test set, the LLM checkpoint for each model is selected on the basis of lowest training loss. In particular, 1,024 samples are taken from each model, which are then evaluated and inserted into a new MAP-Elitesmap. For comparison, the same metrics are calculated using the Stage 1 dataset, by taking the same number of samples from it and evaluating them in the same way. These results are shown in Fig.  11.15, highlighting that the model samples achieve a similar level of performance as dataset samples, suggesting that they have modeled the data well. Also, there is a slight but consistent QD benefit from models trained on the 80% cutoff dataset, reflecting the higher average QD of that dataset.
Fig. 11.15
Measuring the quality and diversity of model samples.Two metrics evaluating samples from trained LLMs are shown (across model size and training dataset): athe percentage of nichesdiscovered and bthe QD score achieved. The 80% threshold dataset is on average less diverse but of higher quality than the 50% threshold dataset, and induces the same properties in models trained upon it. There is not a trend in increasing quality or diversityas model size increases beyond 85  M, and random initialization hurts performance
Fig. 11.16
Generalization tests.In this test, the model is asked to complete samples, taken from the first half of the dataset from the unseen runs. These unseen originals are shown in the videos [74]. From top to bottom: Wheel, from radial seed; Galloper, from square seed; Runner, from CPPN seed
A natural further question is how well the model will do when taken out of distribution, i.e., how well has it really internalized the dynamics of Sodarace? That is, the training and test set for fine-tuning are taken from the same runs, and thus the model will likely have encountered all of the motifs in the test set, and so it may not be a representative test of how well the model will generalize in the future. A preliminary test in this spirit is to take the first half of the Python programs describing several inventions from unseen runs, and explore the capacity of different models to generate functional completions. Though the Radial seedusually produced chaotic Sodaracers, in one preliminary run of ELM with the Radial seed, a functional wheel was discovered. As noted previously data from this run (or any other radial runs) was not used to train the models in Stage 2, nor was it used to fine-tune the diff modelin Stage 1; thus the ability to complete the wheel can serve as a proxy for generalization. Similarly, two other high-performing individuals were taken from other preliminary runs of the CPPN seed and the Square seed, to create a set of three out-of-distribution completion tests. See Fig.  11.16for visualizations of these walkers, including videos; source code for these generalization examples can be found in Appendix F of [36],  p. 50). Note that further tests of generalization are documented in Appendix H, [36],  p. 55.
For each of the three completion tasks, 1,024 completion samples are taken from each model and then evaluated in simulation. In contrast to the in-distribution metrics, in this generalization-focused test, performance was more correlated with the model’s test loss rather than training loss, and thus what checkpoint to evaluate for each model was selected on the basis of lowest test loss. Results are shown in Fig.  11.17, highlighting that larger models, and those trained on the 80% threshold, generally perform better at this task. Note that the randomly initialized (RI) 300  M model significantly underperforms, providing more evidence that pretraining on code provides a valuable prior.
Fig. 11.17
Out-of-distribution completion performance.Shown is the percentage of the original solutions’ performance that is attained by completions from trained LLMs. The percentage shown is the maximum attained over 1,024 independent completion samples from each model. The results are averaged over three out-of-distribution solutions (taken from runs not included in LLM training). The conclusion is that the 80% threshold models perform better than the 50% threshold, and that there is no obvious trend in performance once model size reaches 85  M parameters
Videos of the best performing sample for the Wheel completion from each model are at [75] (for the 80% threshold dataset; the random-initialized 300  M model is not shown because it generated no valid samples for this completion). For the Galloper and Runner completions, the structure and/or behavior of completions often does not match the original sample (especially for the Galloper). In the video [76], a higher performing completion is shown for both of the Galloper and the Runner.
Overall, these results show that an LLM can effectively integrate synthetic data generated through ELM in a novel domain.
11.7 Pipeline Stage 3: Conditional RL
In the final stage, reinforcement learning(RL) is invoked to fine-tune the pretrained LLM output by Stage 2 of the pipeline. The goal is to produce a model that outputs Python programs representing Sodaracers in response to particular terrains. Importantly, the output of Stage 2 is an unconditionalmodel, in the sense that it samples Sodaracers from a distribution defined by the output of Stage 1, without considering the terrain in which the samples will be deployed. The first step in Stage 3 is thus to convert the model to a conditionalone, i.e.,  a model that accepts terrains as inputs, and produces samples of Sodaracers in response.
To achieve this functional form, we first introduce the notion of a terrain embedding network(TEN). The role of the TEN is to map a representation of the terrain to a representation that can be used by the model to sample conditionally. In particular, the output of TENs is a vector (or sequence of vectors) in d, the dimension in which the model embeds tokens. That way, the output of the TEN can be treated as the activation from a given prefix, and the model can proceed in effect now sampling conditioned on the output of the TEN.
Concretely, an unconditional auto-regressive LLM defines a sampling distribution over a sequence of tokens as . In this stage, we introduce the additional module , which represents terrains tin . As , we can consider the resulting conditional model without further modification:
(11.1)
This approach is similar to the controllable transformer proposed by [30], but with the conditional codes being the output of a TEN, rather than particular tokens from the existing vocabulary.
Given a distribution over terrains p(t), an RL setting is constructed to train the parameters of the TEN and further fine-tune the LLM parameters to the conditional setting. In particular, an episode now consists of sampling , and sampling a program from the conditional distribution defined in (11.1). The program is converted to a Sodaracer, evaluated in simulation with the terrain t, and the reward is defined as the absolute distance traversed by the Sodaracer in a given period of time.
11.7.1 Terrain Distributions
In this experiment, the distribution over terrains that the model is exposed to is chosen to explore the viability of producing conditional inventors with the Invention Pipeline. The future vision is to lay the groundwork for the ability to deploy agents capable of conditional invention in rich, potentially multi-agent environments that support the development of open-ended processes. In such settings, it stands to reason that learning to output complex artifacts conditioned on observations of the environment would be a prerequisite to ongoing open-ended innovation.
However, in preliminary experiments in the Sodaracedomain, learning tended to “gravitate” toward collapsed solutions, wherein a single program is produced that achieves reasonable performance on a subset of the terrains in the distribution support. To reduce the viability of such an outcome and simulate a scenario where conditionality is essential, a small and discrete set of terrains for which a single program cannotachieve good performance provides a test where conditional solutions should be significantly more advantageous.
In the experiments, uniform distributions are considered over sets of terrains as illustrated in Fig.  11.18. Two subsets are considered, both of which contain left-walland right-wall. One set additionally contains tunnel, and the other includes bumpy. These sets were specifically chosen such that the models are incapable of producing a single Sodaracer that achieves good performance on all terrains; to maximize the learning objective, the model must leverage the TEN to incorporate conditionality
Fig. 11.18
Terrains used in experiments.A small set of terrains from which distributions that force conditionalitycan be constructed. The terrains are aleft-wall, bright-wall, cbumpy, and dtunnel. The Sodaracers produced by the models are incapable of scaling the walls in left-walland right-wall, and therefore must produce different Sodaracers for these two terrains. Similarly, achieving good performance in the tunnelterrain can only be achieved with short Sodaracers, which struggle to locomote as quickly as taller ones, encouraging the model to distinguish between these terrains. Finally, Sodaracers that are proficient in locomotionon flat terrains tend to perform poorly on bumpy, encouraging the model to produce yet another Sodaracer for this terrain. In contrast to tunnel, which requires Sodaracers with a particular morphology, achieving good performance on bumpyrequires modifying the way Sodaracers locomote. Example Sodaracers are added to the figures to illustrate the scale of the terrains
11.7.2 Parametrizing TENs
Two parametrizations for the TEN are explored.
11.7.2.1 Discrete Codes
The terrain distribution has a discrete and finite support. As such, a simple parametrization wherein the terrains are treated as additional tokens in the existing vocabulary, and the embedding for each terrain is learned separately may be used. The advantage of such a parametrization is that it introduces a relatively small number of new parameters to be optimized with RL, and it is conceptually simple to understand and debug. However, the main disadvantages of such a parameterization are that
the number of parameters scales with the size of the terrain set and
it does not allow the model to naturally generalize to unseen terrains at test time, which may be an important constraint for downstream open-ended processes.
11.7.2.2 ResNets
An alternative parametrization is visual representations of the terrains, which can then be processed by visual recognition models. In particular, a ResNet50 [27] embeds images into as a TEN when experimenting with visual representations of terrains. The main advantages of this parametrization are that it is quite general, could conceivably be used in multiple settings (e.g., teaching a code-generating LLM to write programs in response to visual input), and in theory can generalize to unseen terrains. The main drawback of this approach is that it introduces a large number of new parameters that must be optimized using a sparse RL signal. Conversely, for large terrain distributions, this approach makes it possible to amortize the number of additional parameters necessary for designing conditional inventors.
11.7.3 Experimental Details and Results
Each RL episode consists of sampling a batch of terrains from the distribution, producing samples from the conditional LLM, and evaluating them in simulation to produce the reward.
Proximal policyoptimization [55] is the RL algorithm, in conjunction withgeneralized advantage estimation [54], with default hyperparameters. In preliminary experiments, we found it important to add a KL term (between the policynetwork and the pretrained LLM from Stage 2) to the reward function, as proposed by [17, 65]. The value network is parametrized as a scalar-function version of the policynetwork, i.e., a separate LLM with a separate prepended TEN initialized from the Stage 2 models. Figure  11.19illustrates the architectures and pipelines for the policy and value-function networks. Each iteration consists of batches of 1,024 samples (distributed over 32 GPUs), and training runs consist of 100 iterations.
Fig. 11.19
Illustration of the RL architecture.The conditional policy (a) and value function (b) are depicted, both augmented with a separate TEN for terrain embeddings. The policy is conditioned on a particular terrain (via the TEN) and prompt, and produces a sample, which is interpreted as Python code. The code is then used to compile a Sodaracer, which is evaluated in a simulation to produce a reward R. The value function is conditioned on the same terrain (via its own TEN) and prompt, and outputs an estimation of the value (V) of every token output by the policy sample. During learning, the value function is trained to predict advantages estimated using GAE [54]
RL is run on pretrained, 300  M-parameter LLMs trained with datasets having cutoff thresholds in {50%, 80%}. Recall thatwe use the cutoff threshold to control the trade-off between data quality and quantity, such that higher thresholds result in smaller pretraining datasets with a higher density of quality instances. For each dataset and terrain distribution combination, three runs are performed using different seeds, and the performance is averaged over samples from the resulting model for each terrain, from over all runs, though we exclude a small number of runs that diverged during training. To compute a measure of the performance of datasets and pretrained LLMs, we invoke test-time compute: 1,024 Sodaracers are sampled uniformly and evaluated from each dataset/model (recallthat there is one model for both cutoff thresholds), and the best performing Sodaracer is considered for each terrain. Figures  11.20and 11.21detail the results of these experiments with the tunneland bumpydistributions, respectively.
In short, Figs.  11.20and 11.21help us understand whether RL is able to discover conditional solutions, which we interpret as conditional inventors of Sodaracers that are capable of locomoting on particular terrains. Moreover, Figs.  11.20and 11.21enable us to compare the performance of Sodaracers produced at different stages of the pipeline, and how performance is affected by the choice of cutoff threshold. A particularly interesting question is whether RL is able to consistently improve upon the performance of test-time compute with the pretrained models produced in Stage  2.
Fig. 11.20
Comparing performance of models and datasets across the stages of the pipeline on the terrain distribution including thetunnel. Results are detailed when training the LM using a dataset with a cutoff of a50%, and b80%. Stage 3 models are able to discover conditional solutions in both cases, consistently perform comparably to test-time compute on the dataset, and better than the Stage 2 pretrained LMs. For the 80% cutoff threshold, while performance is better than with 50% at all stages, the pipeline struggles to improve performance over that of the dataset. Conversely, for the 50% cutoff threshold, the Stage 3 (discrete) model improves upon all stages, demonstrating the ability of the pipeline to improve the performance of the models
Fig. 11.21
Comparing performance of models and datasets across the stages of the pipeline on the terrain distribution includingbumpyResults are detailed when training the LM using a dataset with a cutoff of a50% and b80%. Similar trends can be seen to those in Fig.  11.20: Stage 3 models are able to discover conditional solutions and consistently perform comparably to test-time compute on the dataset, improving upon Stage 2 models. For the 80% cutoff threshold, the pipeline achieves comparable performance to that of the dataset, while improving performance for the 50% cutoff threshold
The RL procedure was at times brittle: training sometimes diverged, and some results were inconsistent. Divergencetended to be more frequent using the ResNet TENs, which is unsurprising considering the ResNets introduce many more parameters to the model, which are in turn trained with an extremely impoverished distribution of images (one for each terrain in the distribution).
Despite the fragility, RL fine-tuning is successful in producing conditional inventors in this domain: the models tend to produce a single Sodaracer for each terrain, which differ across terrains in the distribution. Importantly, the produced Sodaracers achieve good performance for the conditioned terrain, while failing to locomote on the other terrains. Videos showcase Sodaracers invented for the Tunnel distribution [77] and for the bumpy distribution [78]. In short, the main result is the outputs of Stage 3, and thus the complete pipelines are conditional inventors of the desired form.
Moreover, in most cases, the RL models are comparable to or better than the best performing Sodaracers sampled from the dataset or the pretrained LLM. This consistency implies that Stage 3 enables the models to learn to use the TENs in conjunction with the LLMs, and further can fine-tune the models’ outputs to improve performance, though not always by significant margins.
Models trained with a cutoff of 80% tend to achieve slightly better performance, and proved more stable during training, though the differences are not significant. This result implies that the trade-off between data quality and quantity may play a role in downstream tasks (such as RL fine-tuning), a point that warrants further investigation in future work. One interesting avenue for research in this direction is to consider pretraining procedures that include information regarding the quality of the instances (where such information is available), e.g.,  as proposed by [14].
Finally, we note that “collapsed” solutions in which the same Sodaracer is produced every time a particular terrain is observed (as opposed to significantly different samples each time the same terrain is seen) are sensible in this setting, as there should exist a dominant Sodaracer for each terrain. However, interestingly, in true open-ended systems this property may not hold: if the environment is constantly shifting, which excludes the existence of single, dominant inventions. In such a setting, the stochasticity of the model is expected to be beneficial, enabling the model to adapt and produce a diversityof useful solutions.
11.7.4 Qualitative Observations
Severalinteresting structures and solution classes were qualitatively observed throughout the experiments, which provide additional insightinto the pipeline’s ability to conditionally invent solutions to different terrains. One such example is the emergence of very shortSodaracers, which arose in response to the tunnelterrain. The video visualizations [79] highlight examples of such Sodaracers produced in response to tunnel
Another interesting class of Sodaracers appeared in earlier experiments with ELM; a wheel-like structure emerged during the evolutionary process, and persevered throughout the pipeline. During Stage 3, the wheel proved particularly adept at locomoting in the bumpyterrain, and consistently emerged as the solution to bumpyproduced by the Stage 3 models for that terrain across RL runs. Unfortunately, the wheel did not re-emerge in the ELM runs used in the final experiments in this chapter. The video [80] demonstrates several solutions of this form discovered by RL when trained with the bumpyterrain distribution as well as the tunneldistribution. For contrast, this video [81] shows failure modeson bumpy for several Sodaracers effective in locomoting on flat terrains.
Such qualitative observationsprovide further evidence that the pipeline is capable of producing interesting inventors and creative solutions to problems, even in a simplified domain that is not open-ended. We hypothesize that when unleashed in more complex domains, this capability of conditional invention will contribute to the open-endednessof the induced process by continually introducing new objects to the environment, and thus changing its properties for other agents.
11.8 Discussion and Conclusion
An important difference between natural evolution and most of EC is the very beginning—nature began with a single “example” or seed, the first cell on Earth, which was already bestowed with critical initial functionality and information. In contrast, runs in EC usually begin with randomized configurations with little or no useful information. Because programming languages like Python for humans are natural modalities for formalizing complicated ideas and relationships, such a program could serve as a seed more in the spirit of nature. However, the problem then is that arbitrary mutations to an already-formulated program are very unlikely to be useful.
A few years ago, the idea that the mutation operator could “know” how to perturb such programs in reasonable and promising ways would be fanciful, but, as shown in this chapter, the emergence of LLMs has now made such capabilities a reality. The MAP-Elitesalgorithm combined with ELM easily bootstrapsdatasets of hundreds of thousands of examples in a completely foreign domain (to the initial LLM) from initial human-written seeds. The validityof this generated data is confirmed by the invention pipeline that follows—conditional LLMs were ultimately trained starting from this data that cannot be trained from scratch.
More broadly, the main idea introduced here is that LLMs trained on code open up a significant new kind of intelligent GP enabled by ELM that is no longer at the mercy of the raw search landscape induced by code. While the experiment in this chapter points to a set of implications for open-endedness, deep learning, and RL, the potential applications are numerous and many previous challenges in the GP field could be revisited with this new tool.
The experiment in this chapter shows that intelligent LLM-based mutation operators can successfully drive explorationby being combined with other search algorithms (e.g., MAP-Elitesin this work). Furthermore, optimizing such mutation operators based on the quality of their output during the search itself appears to make them work even better for exploration. Not only are the discoveries of such search potentially useful in their own right (like wheels in the Sodaracedomain), but they offer an entirely new option for generating example data or optimizing existing solutions in domains where data is sparse or non-existent. For example, such search through LLM-based perturbation could feasibly be applied to optimize the MAP-Elitessearch algorithm itself, or for LLM architecture and hyperparameter search.
From the perspective of open-endedness, the challenge in principle is that the search is by definition continually and even intentionally shifting out of distribution. As soon as a new invention or DCT is achieved, open-endednessdemands that its now-familiar comfort zone be at least partially abandoned for new frontiers. The experiment here wherein LLMs trained from simple flat-ground walkers were able to leverage that knowledge to appropriately generate specialized walkers for different terrains shows just this kind of informed leap to a new frontier. If such a process of leaps upon leaps can be made to continue indefinitely, then an unbounded explosion of emergent complexity could be within reach.
One important question for future work is the extent to which the resultant model can interpolate or extrapolate to examples (i.e., environments) outside its training distribution. While RL can harness existing knowledge in the LLM to bootstrapinto new tasks, extrapolating principles from such knowledge is much harder and likely to require further weight updates through additional learning. It is possible that a sophisticated future open-ended system would entangle both continual evolutionand RL for DCTs together.
Overall, the hope is that the simple core insightthat the efficacy of mutation in GP can now dramatically improve through ELM will inspire a broad array of novel applications and research directions. The observation that EC can benefit directly and dramatically from advances in deep learning (and deep learning from EC further down the invention pipeline) can also help to motivate the further pursuit of synergies between the fields.
References
1.
Detailed description of unified format. Detailed-Unified.html. https://​www.​gnu.​org/​software/​diffutils/​manual/​html_​node/​
2.
Open AI blogpost: New GPT-3 capabilities: Edit and insert (2022). https://​openai.​com/​blog/​gpt-3-edit-insert/​
3.
Baker, B., Kanitscheider, I., Markov, T., Wu, Y., Powell, G., McGrew, B., Mordatch, I.: Emergent tool use from multi-agent autocurricula (2019). arXiv:​1909.​07528
4.
Banzhaf, W., Nordin, P., Keller, R.E., Francone, F.D.: Genetic Programming - An Introduction. Morgan Kaufmann Publishers Inc. (1998)
5.
Bedau, M.A., McCaskill, J.S., Packard, N.H., Rasmussen, S., Adami, C., Green, D.G., Ikegami, T., Kaneko, K., Ray, T.S.: Open problems in artificial life. Artif. Life 6(4), 363–376 (2000)Crossref
6.
Bentley, P.J., Kumar, S.: Three ways to grow designs: a comparison of embryogenies for an evolutionary design problem. In: Proceedings of the Genetic and Evolutionary Computation Conference (GECCO-1999), pp. 35–43 (1999)
7.
Bommasani, R., Hudson, D.A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M.S., Bohg, J., Bosselut, A., Brunskill, E., et  al.: On the opportunities and risks of foundation models (2021). arXiv:​2108.​07258
8.
Bongard, J.C., Pfeifer, R.: Repeated structure and dissociation of genotypic and phenotypic complexity in artificial ontogeny. In: Spector, L., Goodman, E.D., Wu, A., Langdon, W.B., Voigt, H.-M., Gen, M., Sen, S., Dorigo, M., Pezeshk, S., Garzon, M.H., Burke, E. (eds.) Genetic and Evolutionary Computation Conference, pp. 829–836 (2001)
9.
