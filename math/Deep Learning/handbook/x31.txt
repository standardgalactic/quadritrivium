[211]
Intrusion detection
[66, 111, 196]
Trading agents
[130, 136]
8.6 Applications and  Future Research
Table  8.1provides a review of specific application developments that have benefited from adopting an evolutionary ensemble learning approach. Thus, aside from the application of evolutionary ensemble methods to a wide range of regression and classification problems (summarized in Sect.  8.2), we note that the underlying requirements for constructing ensembles are also the requirements forfeature construction/engineering using wrapperor filter methods[77]. Specifically, feature constructionrequires that a diverse set of features be engineered to improve the performance of a regression or classification task. Indeed, many evolutionary approaches to feature engineeringassume a multi-tree representation, e.g. [7, 21, 148, 206]. Thus, the number of ensemble participants (n) represents the number of features constructed [7, 116, 148, 183]. More recently, multiple features (participants) have been engineered per class (e.g. [56, 127, 206]) or features (participants) are evolved that are capable of transferring between different environments [8]. Multidimensional genetic programming approaches the (feature construction)task from a different perspective by attempting to discover a low-dimensional space appropriate for describing all aspects of the task [39, 139]. In general, what is particularly impressive with ensemble solutions to the feature constructionproblem is the capacity to discover very accurate low-dimensional feature spaces from applications described in terms of higher dimensions [56, 148, 171] or from low cardinality datasets [9].
Future work might consider the use of ensemble diversitymeasures originally developed from the perspective of feature construction. For example, limitations may appear when relying on pairwise (feature) diversity measures [90] or attribute frequency importance [49], whereas a permutation-based sensitivity analysis can avoid the linear correlation constraint (e.g. [12, 49, 93]). Future work might further consider the utility of permutation schemes for interpretable solutions [57].
In general, scalability represents an underlying theme for sEEL. One approach might be to make use of the increasing availability of parallel computing platforms, such as cloud [18, 68, 212] or GPU[17]. Alternatively, methods for algorithmically reducing the number of evaluations might be adopted, such assurrogate models [36] or active learning [185]. Both of the latter have been benchmarked on computer vision benchmarks such as CIFAR resulting in much simpler solutions than currently available using deep learning. In addition, organizing ensemble agents as a stack/cascade was shown to scale sEEL to data cardinalities in the hundreds of thousands in less than 30  s on a regular CPU [228]. Future work might continue to investigate how different ways of composing ensembles trades off accuracy versus training efficiency versus interpretability[35].
A related application of sEEL is that of streaming dataforecasting and classification [83]. Some properties that make the streaming data environment challenging yet appropriate for sEEL might include.
Non-stationary nature of the underlying task (drift and/or shift) which might imply that mechanisms need to be identified for detecting the onset of change and reacting appropriately [64, 65, 67]. Ensembles are capable of reacting to changes more effectively than non-ensemble approaches because the implicit modularityenables specific participants to be retired/replaced as their performance degrades. This then leads to solutions that are more adaptable than without the use of ensembles [210].
Anytime nature of deployment implies that in time seriesclassification a champion classifier has to be available for labelling the nextexemplar before any model has encountered it. This means that the champion classifier might vary over the course of time.
Imbalanced or limited availability of label information. Given that streaming data is typically experienced on a continuous basis (there is no ‘end’ to network or stock market data), models are constructed from the content of a sliding window, i.e. a finite number of exemplars. This can lead to different strategies being adopted for retaining data beyond the most recent window content, e.g. data subset archiving and ensemble archiving [19, 111].
To date, streaming ensemble methods have been applied to applications in tradingagents [130, 136], intrusion detection [66, 111], electricity utilization [131], satellite data [64], and churn detection [211]. Specifically, Mabu et al. assume a graph representation that explicitly captures dynamics of stock trading, whereas Loginov and Heywood coevolve an ensemble of technical indicators and decision trees for currency trading and utility forecasting. Both Folino et al. and Khanchi et al. emphasize the ability of ensembles to dynamically react to changes to the underlying properties of the streaming data. A related topic is that of dynamical systems identification in which each (independent) variable has a participant evolved and a suitable aggregation function applied [1]. Particular challenges in this setting might include evolving explainable solutions. Other tasks with dynamic properties that have deployed evolutionary ensemble approaches include different formulations of scheduling tasks that are addressed through the evolution of dispatching rules, e.g. [58–60, 80, 163].
Task transferand multi-task learning also represent areas in which rEEL can potentially produce advances to the state-of-the-art. Task transferis typically assumed when the ultimate objective is too complex to solve ‘tabula rasa’ [219]. Instead, solutions are first evolved to solve simpler source tasks before the ultimate target task is encountered [202]. Likewise, multitask learningrequires that solutions to multiple tasks are discovered such that a single champion for all tasks is discovered. This can be particularly difficult as, aside from the difficulty of solving each task, the agent has to establish what environment it is in. Current results indicate that EEL approaches are well placed to incrementally absorb multiple source tasks [8, 101, 103, 149, 186–188] as well as solve multiple tasks simultaneously [28, 98, 104, 108, 109].
Future challenges might include extending these results to environments requiring lifelong/continuous learning (e.g. [179]) and addressing pathologiessuch as catastrophic forgetting(e.g. [114]) or returning solutions that are interpretable [57, 174]. Given the explicit use of structured representations in EEL, interpretable solutions might represent a potentially significant new development for EEL. Moreover, some of the opaque properties of individual participants might be amenable to simplification using techniques developed for interpretable AI (e.g. model debugging using adversarial learning or perturbation-based analysis [57]). Indeed, there is already a rich history of employing competitive coevolution(the EC approach to adversarial learning) to develop more robust solutions to computer security applications[157].
Multi-agent systemswill continue to develop, particularly with respect to evolutionary robotics [53]. One avenue that is beginning to see some results is with regards to the evolution of communication [140] or stigmergy [225] in multi-agent systems. In particular, Mingo and Aler demonstrate that agents can evolve spatial languages with specific syntactical properties using the evolution of grammatical evolution [155]. As the number of agents and objects increases, then the sophistication of the evolved language also increases [140]. Developments of this nature may lead to agents teaching agents [178] and/ or forms of problem-solving that uniquely reflect the mixed ability of the agents to perform different tasks [10].
In addition, multi-agent approaches have appeared in gaming applications in which group behaviours might be desirable. The RoboCup competition represented an early example [15, 133], with more recent works using specific aspects of the full-team competition as smaller scale benchmarks, e.g. keepaway [101, 152, 213, 219], half field offence [101, 103] or five-a-side soccer [75]. First-person video games have also been used to demonstrate the development of squad behaviours using EEL [197] and confirmed that teams increasingly make use of communication as the amount of visual information decreases [52]. Likewise, partially observable environments have also been used to demonstrate: (1) navigationbehaviours under visual reinforcement problems [108, 188, 189] and (2) time seriesprediction [106, 108] and (3) agents able to solve multiple control problems simultaneously [109]. The resulting EEL graph structures demonstrate an emergent division of duties between, for example, participants that write to memory (a specialist) and those that read (everyone) or the types of tasks addressed by different parts of the graph-based ensemble.
8.7 Discussion
EEL in general has a long and rich history in which the desire to scale evolutionary computation to increasingly more demanding tasks represents an underlying motivation. To do so, the divide-and-conquer approach to problem-solving is assumed as a general principle. However, in doing so, several pathologiespotentially appear/need recognition of which level of selectionand diversitymaintenance represent reoccurring themes. The level of selectionreflects the fact that a solution is composed of multiple participants, whereas the performance function might only operate at one level. Moreover, gene linkagecan appear between participants and diversity can appear at multiple ‘levels’, making credit assignmentdifficult to measure. In addition, EEL as applied to multi-agent systemscannot just assume that teams will be heterogeneous. Instead, specific combinations of different types of agents might be the norm.
Historically, supervised learning applications of EEL have assumed a fixed-sized ensemble defined by a multi-tree representation(Sect.  8.2). This means that the participants are always heterogeneous and the unit of selectionis that of the team. However, as demonstrated by the OET algorithm (Sect.  8.3), this might represent a sub-optimal model of selection. Conversely, multi-agent tasks often assume cooperative coevolutionas the starting point for defining a teamof agents. The cooperative coevolutionary model not only provides a wider opportunity for developing mechanisms for answering the level of selectionquestion but also potentially introduces multiple pathologies(Sect.  8.4). Attempting to develop variable-sized ensembles means that a participant has to distinguish between learning context (decomposing the state/input space into regions) versus suggesting an action (Sect.  8.5). Some of the benefits of adopting a variable-sized teamare that the evolved ensemble imparts additional knowledge about what was learnt. However, pathologiessuch as hitchhiking might result in bloated ensembles unless mitigation strategies are taken.
The future of EEL will likely continue to grow with the development of applications on the one hand and challenges to machine learning as a whole on the other. EEL is central to a bodyof work on feature constructionthat is now leading to the adoption of task transfertechniques, e.g. high-dimensional tasks with missing information and/or low cardinality. Likewise, EEL has repeatedly been successfully applied to streaming datain general, but the number of new streaming dataapplications continues to grow (e.g. IoT, social media, e-commerce). Streaming dataapplications also point to the concept of lifelong (continuous) learning in which case there is potentially no end to the learning process.
EEL as formulated to address multi-agent or reinforcement learningproblems is able to answer questions about hybrid homogeneous–heterogeneous team compositionas well as variable-size ensembles. Incorporating hierarchical relationships into EEL means that participants who systematically mispredict can defer their decision to another (specialist) team that concentrates on resolving this ambiguity. Approaches for discovering graph ensembles provide a further opportunity for establishing structures that might be appropriate for continuous learning and interpretable solutions. A wide range of empirical results has already established that participants are significantly less complex than monolithic solutions (e.g. when using SVM or deep learning). Moreover, the appropriate selection of the ensemble aggregation operation (e.g. winner-tasks-all or the additive operator) provides explicit support for interpretable solutions [174]. This in combination with parsimonious participants may lead to truly scalable ensembles that support low-dimensional saliency, i.e. do not rely on post hoc ‘explanations’. In short, there is still ‘plenty of room’ in the divide-and-conquer approach to evolutionarymachine learning.
Acknowledgements
The author gratefully acknowledges support from the NSERC Discovery program (Canada).
References
1.
Abdelbari, H., Shafi, K.: A genetic programming ensemble method for learning dynamical system models. In: Proceedings of the International Conference on Computer Modeling and Simulation, pp. 47–51. ACM (2017)
2.
Agapitos, A., Loughran, R., Nicolau, M., Lucas, S.M., O’Neill, M., Brabazon, A.: A survey of statistical machine learning elements in genetic programming. IEEE Trans. Evol. Comput. 23(6), 1029–1048 (2019)
3.
Agapitos, A., O’Neill, M., Brabazon, A.: Ensemble bayesian model averaging in genetic programming. In: Proceedings of the IEEE Congress on Evolutionary Computation, pp. 2451–2458. IEEE (2014)
4.
Agogino, A.K., Parker, C.H., Tumer, K.: Evolving distributed resource sharing for cubesat constellations. In: Proceedings of the Genetic and Evolutionary Computation Conference, pp. 1015–1022. ACM (2012)
5.
Agogino, A.K., Tumer, K.: Evolving distributed agents for managing air traffic. In: Proceedings of the Genetic and Evolutionary Computation Conference, pp. 1888–1895. ACM (2007)
6.
Agogino, A.K., Tumer, K.: Efficient evaluation functions for evolving coordination. Evol. Comput. 16(2), 257–288 (2008)
7.
Ain, Q.U., Al-Sahaf, H., Xue, B., Zhang, M.: A genetic programming approach to feature construction for ensemble learning in skin cancer detection. In: C.A.C. Coello (ed.) Proceedings of the Genetic and Evolutionary Computation Conference, pp. 1186–1194. ACM (2020)
8.
Al-Helali, B., Chen, Q., Xue, B., Zhang, M.: Multitree genetic programming with new operators for transfer learning in symbolic regression with incomplete data. IEEE Trans. Evol. Comput. 25(6), 1049–1063 (2021)
9.
Al-Sahaf, H., Al-Sahaf, A., Xue, B., Zhang, M.: Automatically evolving texture image descriptors using the multitree representation in genetic programming using few instances. Evol. Comput. 29(3), 331–366 (2021)
10.
Albrecht, S.V., Liemhetcharat, S., Stone, P.: Special issue on multi-agent interaction without prior coordination: guest editorial. Autonomous Agents and Multi Agent Systems 31(4), 765–766 (2017)
11.
Ali, S., Majid, A.: Can-evo-ens: Classifier stacking based evolutionary ensemble system for prediction of human breast cancer using amino acid sequences. J. Biomed. Inform. 54, 256–269 (2015)
12.
Altmann, A., Toloşi, L., Sander, O., Lengauer, T.: Permutation importance: a corrected feature importance measure. Bioinformatics 26(10), 1340–1347 (2010)
13.
Alvarez, I.M., Nguyen, T.B., Browne, W.N., Zhang, M.: A layered learning approach to scaling in learning classifier systems for boolean problems. CoRR abs/2006.01415(2020)
14.
Amaral, R., Ianta, A., Bayer, C., Smith, R.J., Heywood, M.I.: Benchmarking genetic programming in a multi-action reinforcement learning locomotion task. In: Proceedings of the Genetic and Evolutionary Computation Conference. ACM (2022)
15.
Andre, D., Teller, A.: Evolving team darwin united. In: RoboCup-98: Robot Soccer World Cup II, LNCS, vol. 1604, pp. 346–351. Springer (1998)
16.
Angeline, P.J., Saunders, G.M., Pollack, J.B.: An evolutionary algorithm that constructs recurrent neural networks. IEEE Trans. Neural Networks 5(1), 54–65 (1994)
17.
Arnaldo, I., Veeramachaneni, K., O’Reilly, U.: Flash: A GP-GPU ensemble learning system for handling large datasets. In: Proceedings of the European Conference on Genetic Programming, LNCS, vol. 8599, pp. 13–24 (2014)
18.
Arnaldo, I., Veeramachaneni, K., Song, A., O’Reilly, U.: Bring your own learner: A cloud-based, data-parallel commons for machine learning. IEEE Comput. Intell. Mag. 10(1), 20–32 (2015)
19.
Atwater, A., Heywood, M.I.: Benchmarking Pareto archiving heuristics in the presence of concept drift: diversity versus age. In: Proceedings of the Genetic and Evolutionary Computation Conference, pp. 885–892. ACM (2013)
20.
Axelrod, R.: Evolution of co-operation. Basic Books (1984)
21.
Badran, K.M.S., Rockett, P.I.: Multi-class pattern classification using single, multi-dimensional feature-space feature extraction evolved by multi-objective genetic programming and its application to network intrusion detection. Genet. Program Evolvable Mach. 13(1), 33–63 (2012)
22.
Bahçeci, E., Miikkulainen, R.: Transfer of evolved pattern-based heuristics in games. In: Proceedings of the IEEE Symposium on Computational Intelligence and Games, pp. 220–227. IEEE (2008)
23.
Bayer, C., Amaral, R., Smith, R.J., Ianta, A., Heywood, M.I.: Finding simple solutions to multi-task visual reinforcement learning problems with tangled program graphs. In: Genetic Programming Theory and Practice, vol. XVIII, pp. 1–19 (2022)
24.
Belkin, M., Hsu, D., Ma, S., Mandal, S.: Reconciling modern machine learning and the bias-variance trade-off. Proceedings of the National Academy of Science 116(32), 15849–15854 (2019)MathSciNetzbMATH
25.
Bhowan, U., Johnston, M., Zhang, M., Yao, X.: Evolving diverse ensembles using genetic programming for classification with unbalanced data. IEEE Trans. Evol. Comput. 17(3), 368–386 (2013)
26.
Bhowan, U., Johnston, M., Zhang, M., Yao, X.: Reusing genetic programming for ensemble selection in classification of unbalanced data. IEEE Trans. Evol. Comput. 18(6), 893–908 (2014)
27.
Bi, Y., Xue, B., Zhang, M.: A divide-and-conquer genetic programming algorithm with ensembles for image classification. IEEE Trans. Evol. Comput. 25(6), 1148–1162 (2021)
28.
Bi, Y., Xue, B., Zhang, M.: Learning and sharing: A multitask genetic programming approach to image feature learning. IEEE Trans. Evol. Comput. 26(2), 218–232 (2022)
29.
Bishop, C.M.: Neural Networks for Pattern Recognition. Oxford University Press (1995)
30.
Bongard, J.C.: The legion system: A novel approach to evolving heterogeneity for collective problem solving. In: Proceedings of the European Conference on Genetic Programming, LNCS, vol. 1802, pp. 16–28. Springer (2000)
31.
Brameier, M., Banzhaf, W.: Evolving teams of predictors with linear genetic programming. Genet. Program Evolvable Mach. 2(4), 381–407 (2001)zbMATH
32.
Brave, S.: The evolution of memory and mental models using genetic programming. In: Proceedings of the Annual Conference on Genetic Programming. Morgan Kaufmann (1996)
33.
Breiman, L.: Bagging predictors. Mach. Learn. 24(2), 123–140 (1996)zbMATH
34.
Breiman, L.: Arcing classifier. Annuals of. Statistics 26(3), 801–849 (1998)MathSciNetzbMATH
35.
Cagnini, H.E.L., Freitas, A.A., Barros, R.C.: An evolutionary algorithm for learning interpretable ensembles of classifiers. In: Proceedings of the Brazilian Conference on Intelligent Systems, Lecture Notes in Computer Science, vol. 12319, pp. 18–33. Springer (2020)
36.
Cardoso, R.P., Hart, E., Kurka, D.B., Pitt, J.: Augmenting novelty search with a surrogate model to engineer meta-diversity in ensembles of classifiers. In: Proceedings of the European Conference on Applications of Evolutionary Computation, LNCS, vol. 13224, pp. 418–434 (2022)
37.
Cardoso, R.P., Hart, E., Kurka, D.B., Pitt, J.V.: Using novelty search to explicitly create diversity in ensembles of classifiers. In: Proceedings of the Genetic and Evolutionary Computation Conference, pp. 849–857. ACM (2021)
