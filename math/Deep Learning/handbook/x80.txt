FGSM
28.85
16.38
35.08
52.09
51.86
55.06
FGSM-10
11.15
6.28
9.45
25.19
22.70
27.13
BIM-10
0.02
0.00
0.00
0.16
0.00
0.02
BIM-50
0.00
0.00
0.00
0.00
0.00
0.00
FGM
47.69
44.86
48.62
61.46
60.82
64.24
BIM-10
2.02
30.77
0.23
3.10
0.75
2.67
BIM-50
0.16
24.13
0.00
0.26
0.01
0.38
BIM-100
0.09
21.76
0.00
0.12
0.00
0.24
PGD-50-10
0.08
18.10
0.00
0.14
0.01
0.23
15.4.6 Analyzing Misclassifications Through Confusion Matrices
To complement our analysis, we looked into the confusion matrices of each model under different attacks and found that even when the same attack brings the accuracy of all models to zero (e.g., the -constrained BIM-50 attack), different patterns in their misclassifications can still emerge.
As shown in Fig.  15.5a, under an -bounded BIM-50 attack, the misclassifications produced by WRN-28-10 for each class are spread out across the remaining classes. Moreover, this model seems to favor mainly two classes, bird and cat, with a large number of examples being misclassified as such (14.87 and 18.29 %, respectively). Under the -constrained BIM-100 attack (Fig.  15.5b), the misclassifications of images from classes that represent a means of transportation(airplane, automobile, ship, and truck) are more clustered together.
Fig. 15.5
Confusion matrices for the WRN-28-10 model under two attacks
Figure  15.6a shows that, under the -constrained BIM-50 attack, the predictions of DENSER can be clearly grouped into clusters, with most examples from one class being misclassified into a smaller subset of the other classes than with the baseline model. Images that represent an animal are mainly misclassified as another animal, while images of a means of transportationare misclassified as some other vehicle. Another common mistake made by DENSER is to classify birds as airplanes, and vice versa. The confusion matrix of DENSER under the BIM-100 attack with perturbations is shown in Fig.  15.6b. Contrary to the attack, the BIM-100 attack is unable to decrease the accuracy to zero, and so, some perturbed images are correctly classified. Other than that, similar patterns emerge between the two types of perturbations. The automobile class is the most difficult to attack with perturbations, while it is easier to cause a misclassification of airplane instances.
Fig. 15.6
Confusion matrices for the DENSER model under two attacks
Fig. 15.7
Confusion matrices for the NSGA-Net models under the BIM-50 attack with perturbations
Fig. 15.8
Confusion matrices for the NSGA-Net models under the BIM-100 attack with perturbations
According to Fig.  15.7a, and similar to the baseline model, most examples are also misclassified as bird and cat with NSGA-M. However, misclassifications of a single class are less spread out between the remaining ones, especially in the case of bird, cat, ship, and truck. The three NSGA-Net models from the NASNet search space show similar patterns in their confusion matrices. The main distinguishing factor is the spread of the misclassifications of each class: NSGA-mB misclassifies the majority of the examples from one class into fewer classes than NSGA-mC (check, for instance, the ship class), and NSGA-mA is in the middle of the spectrum. Similar to NSGA-M, most misclassifications of these three models also fall into the cat class (especially with NSGA-mA). However, the tendency of bird being the second most (mis)predicted class is not observed with these models. A prevailing mistake made by all NSGA-Net models is to classify dog instances as cats.
The confusion matrices for the BIM-100 attack with perturbations exhibit similar patterns, as shown in Fig.  15.8. In comparison with the BIM-50 attack with constraints, less examples are misclassified by the three models from the micro search space as cat instances, especially in the case of NSGA-mB. With NSGA-M and NSGA-mB, some changes are also observed with the misclassification of examples that originally belong to the ship class.
15.5 Open Challenges
Although there have been several proposals to generate adversarial examplesusing evolutionary methods, EML approaches that aim at improving the adversarial robustnessof ML models, namely ANNs, are scarce. That being said, there is a growing bodyof work on the intersection of the two fields, which makes it necessary to keep track of the progress made. That naturally entails that we need means to compare approaches against one another, namely through well-established benchmarks. Existing tools only tackle these problems independently. For instance, RobustBench  [10] benchmarks adversarialrobustness but is not concerned with how the models are designed, as long as the prerequisites for being evaluated with the proposed method are met. Conversely, BenchENAS[61] benchmarks ENASapproaches but does not support adversarial robustnessassessments. Both benchmarking tasks already face several challenges when considered on their own, and so, such issues are only expected to be amplified with a benchmark that addresses the two tasks simultaneously, especially given the diversitywithin the approaches that may be a target for evaluation.
In the adversarial robustnessresearch field, there have been discussions on the suitability and relevance of norm-constrained perturbations, especially in terms of -norms acting as good (or bad) proxies of human perception  [19]. Moreover, the underlying assumption that the perturbations should be small or imperceptible may be irrelevant under certain scenarios  [19, 24].
Given the wide variety of threat modelsand the different facets of robustness, as described in Sect.  15.3, it is also an open issue to understand how, and if, a model being robust to a certain type of perturbations affects its robustness to some other kind of inputs. The work by Yin et al. [63] dives into the study of these trade-offs, but further analysis is needed, especially since new methods to improve robustness (either adversarial or some other form) are bound to appear in the literature.
Understanding the role played by certain architectural characteristics of the ANNs in their adversarial robustness, an aspect partially addressed in Benz et al. [5] and Huang et al. [27], still needs further investigation. We argue that gathering such insightsis a crucial stepping stone to EML approaches that can create robust ANNs. On the other hand, incorporating adversarial robustnessas an optimization objective of an ENAS approach is a challenge in itself. For instance, several components of the adversarial robustnessfitness assessment strategy must be defined, such as the methods used to craft adversarial samples or the stages at which those samples should be generated. Going back to the need to then benchmark different proposals, all these design choices undoubtedly make creating such a benchmark a daunting task.
15.6 Discussion and Conclusions
The true value of ML models is their ability to generalize and perform well on data that has not been seen during training. Thus, when building these models, it is crucial to estimate how good they will do in production, i.e., estimate their generalization performance. In this chapter, we presented a general framework to validate models designed by evolutionary techniques. Establishing fair comparisons with similar approaches found in the literature is another fundamental task when designing new methods. Therefore, we also reviewed a recent initiative to benchmark ENASapproaches.
Following these good practices allows us to get a clearer picture of the current state-of-the-art and advances in the field. In fact, several models designed by evolutionary methods, in particular ANNs, have achieved impressive results in terms of predictive accuracy. However, new concerns arise with this success and the potential application of evolved models in real-world scenarios. Namely, the models should be robust, i.e., they should produce correct outputs even if presented with data that somewhat deviates from what was seen during training. From the many facets of robustness, we focused on robustness against adversarial examples, which are carefully crafted to fool a model by adding small, often imperceptible, perturbations to benign samples.
The vulnerabilityof manually designed ANNs to adversarial exampleshas been extensively studied, but the same cannot be said about evolved ANNs. As such, we conducted a study to evaluate the adversarial robustnessof CNNsdesigned by two NE approaches, namely DENSER and NSGA-Net. Although the models were validated from a predictive accuracyperspective, the original works did not contemplate any adversarial robustnessassessment, nor were the models explicitly designed to be robust in that sense.
The results show that the DENSER and the NSGA-Net models are susceptible to adversarial attacks, with the accuracy almost always dropping to zero (or close to zero). This constitutes clear evidence that more efforts should be made toward incorporating this kind of analysis in the design and validation of NE approaches. Therefore, the standard scenario where the main (and often single) focus is on predictive accuracyover clean data should be extended to accommodate other concerns.
At the same time, the DENSER model is, to a certain degree, robust to perturbations. This behavior was not explicitly enforced by the evolutionary process, but it shows that future approaches may succeed in discovering novel architectures that are inherently more robust. Designing such a method is a path we are currently investigating, starting from a more in-depth analysis of the DENSER model and its architectural characteristics. This analysis may reveal some of the ingredients responsible for the slightly higher robustness under that threat model, valuable knowledge that can guide key design choices of a robustness-aware NE method.
In this study, we tried to be as faithful as possible to the original works, but that came with some disadvantages. Namely, each model was trained under slightly different configurations, including distinct data pre-processing procedures. Following the discussion on benchmarking EML methods, it would be interesting to re-train all the models under the exact same conditions so as to, for instance, make sure that the observed differences truly arise from architectural aspects. Furthermore, we analyzed pre-trained models for a single datasetand strictly considered white-box attacks. Those two factors constitute clear targets for improvement in future work. Nevertheless, current frameworks, like BenchENAS[61], are not prepared to support adversarial robustnessassessments yet. Moreover, once the need to compare robustness-aware evolutionary approaches arises, even further challenges will have to be overcome to build a sound benchmark.
Acknowledgements
This work is funded by the FCT—Foundation for Science and Technology, I.P./MCTES through national funds (PIDDAC), within the scope of CISUC R&D Unit—UIDB/00326/2020 or project code UIDP/00326/2020. It is also partially supported by the Portuguese Recovery and Resilience Plan (PRR) through project C645008882-00000055, Center for Responsible AI. The first author is also partially funded by FCT under the individual grant UI/BD/151047/2021.
References
1.
Alzantot, M., Sharma, Y., Chakraborty, S., Zhang, H., Hsieh, C.-J., Srivastava, M.B.: GenAttack: practical black-box attacks with gradient-free optimization. In: Proceedings of the Genetic and Evolutionary Computation Conference, pp. 1111–1119 (2019)
2.
Andriushchenko, M., Croce, F., Flammarion, N., Hein, M.: Square attack: a query-efficient black-box adversarial attack via random search. In: ECCV (2020)
3.
Assunção, F., Lourenço, N., Machado, P., Ribeiro, B.: DENSER: deep evolutionary network structured representation (2018). arXiv:​1801.​01563
4.
Athalye, A., Carlini, N., Wagner, D.: Obfuscated gradients give a false sense of security: circumventing defenses to adversarial examples. In: International Conference on Machine Learning (2018)
