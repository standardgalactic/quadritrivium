12.4.1.1 Classification of Evolutionary HW-Aware NAS Methods
Table  12.4showcases the key properties of selected evolutionary hardware-aware NASmethods. The search is conducted either at the macro level(i.e., the entire CNNis encoded in the chromosome) or the subnetworklevel (also known as a blockor a cell), in which only a subnetwork is optimized by an EA. The resulting subnetwork can be used multiple times in the final CNN. These NASmethods are often called a micro-levelNAS. The search space can also be reduced to a few hyperparameters of a given pre-designed CNN architecture (see hyperpin the Search Spacecolumn in Table  12.4). Column SuperNetinforms whether a supernet is used. The design objectives are listed in the Objectivescolumn; the accuracy is not mentioned as it is always involved. The Estimation Methodcolumn tells us if at all and how particular hardware parametersare estimated. We observe that latency (Lat) and Energy are often estimated rather than measured. If the accuracy (Acc) is estimated, then an NN-based predictor (surrogate) is almost always utilized for this purpose  [38, 73]. The Target deviceinforms about the target hardware platform(s). Finally, column Data Setlists the problems/data set(s) used for evaluation. It has to be noticed that, in addition to image classification, some other tasks are approached.
Some multi-objective NAS methods(e.g.,   [11, 18, 38, 39]) only optimize the number of FLOPs, which is not highly correlated with the real hardware parameterssuch as latency and energy. We included these methods in Table  12.4to cover the whole scope of methods in this area. The following paragraphs briefly present some recent hardware-aware NASmethods utilizing evolutionary algorithms.
12.4.1.2 Selected Evolutionary NAS Methods
The first evolutionary NASmethods such as  [59] did not consider any hardware parametersduring the evolution. Later, the NAS has become a truly multi-objective method.
The Lamarckian Evolutionary algorithm for Multi-Objective Neural Architecture DEsign (LEMONADE)  [18] is a multi-objective NAS. It first selects a subset of architectures, assigning a higher probability to architectures that would fill gaps on the Pareto frontfor the objectives that can easily be evaluated (e.g., the number of parameters); then, it trains and evaluates only this subset to save computational resources during the architecture search. It proposes a Lamarckian inheritance mechanism that generates child networks that are warm-started with the predictive performanceof their trained parents. This is accomplished by using (approximate) network morphism operators for generating children. Within 5 days on 16 GPUs, LEMONADE discovers architectures that are competitive in terms of predictive performanceand resource consumption with hand-designed networks, such as MobileNet-V2.
Schorn et al.  [62] also employ a set of evaluation functions for the prediction of energy consumption, latency, and required bandwidth of DNNs on hardware, solely based on the topology of neural architecture to avoid the need for expensive simulations or training of candidate CNNs. Furthermore, they also consider error resilience as one of the objectives. Error resilience is seen as the robustness of the neural network classifier against perturbations in its neuron activation values. Such perturbations can be the result of random hardware faults, such as radiation-induced bit-flips. Hence, random bit-flip error simulations are used to evaluate the actual resilience of the obtained set of neural networks. Evolved CNNsachieve about a to lower data corruption rate at 0.5% bit error rate in the feature maps of the network in comparison with MobileNet-V2.
GoldenNAS  [40] introduces a novel dynamic channel scaling scheme to enable the channel-level search, a progressive space shrinking method to progressively shrink the search space toward target hardware, and an adaptive batch normalization technique to enable the depthwise adaptiveness of CNNsunder dynamic environments. GoldenNAS adopts the weight-sharing technique based on the supernetparadigm, where the supernetis derived from ShuffleNetV2. Multiobjective EA samples the supernetto obtain CNNsshowing suitable trade-offs between accuracy and latency for various hardware platforms—GPU(Nvidia Quadro GV100), CPU (Intel Xeon Gold 6136), and edge device (Nvidia Jetson Xavier). Latency is modeled analytically. Note that supernet training for 100 epochs takes about 70 GPU hours, and each stage of the progressive space shrinking takes about 22 GPU hours. The evolutionary search process requires about 6 GPUhours to finish.
Lu et al.  [37] utilizes latency monotonicity (i.e., the observation that the architecture latency rankings on different devices are often correlated on other devices) to reuse models from one proxy device on several target devices (several mobile and non-mobile devices tested in this paper). It avoids building a latency predictor for each target device. Hence, only one latency predictor based on an MLPwith four layers is used. The search space is built up on MobileNet-V2 with multiplier 1.3, with the channel number in each block fixed. The optimization involves the depth of each stage, the kernel size of convolutional layers, and the expansion ratio of each block. The depth can be chosen from , kernel size can be , and candidate expansion ratios are . There are five stages whose configurations can be searched. The one-shot NASutilizes Once-For-All network  [6] as a supernet. EA is searching for optimal architectures using one proxy device with 1000 individuals in the population and 50 generations for each latency constraint. The evolutionary search takes less than 30  s for each run.
APQ  [73] also exploits the supernet. It utilizes a joint model architecture-pruning-quantization search. A mixed quantization is applied after extracting pruned subnetworks from the supernet. An energy/latency look-up table is used to provide the hardware feedback during the search.
12.4.2 NAS with Hardware Co-design
As emphasized by Lin et al.  [35], NASwith hardware co-designopens a new search space—hardware configurations—to deeply co-optimize the CNNarchitecture and its hardware implementation with the aim of delivering the most suitable trade-offs between the accuracy and hardware parameters. In addition to the CNN architecture and weights, the hardware configuration is optimized, which can involve optimizing the bit widths, quantization levels, PE array size, buffer size, MAC circuit configuration (e.g., by utilizing approximate multipliers), data floworganization, tiling strategy, loop order, memory subsystem parameters, preferences for the high-level synthesis tools, etc.
12.4.2.1 A Single Search Algorithm
A straightforward approach is to add the hardware parametersto the chromosome which describes the CNNarchitecture, and extend thus the search space of the original NASalgorithm.
For example, Pinos et al.  [56] evolved CNNarchitecture together with the selection of suitable approximate multipliers for particular CNN layers to reduce power consumption. The method, EvoApproxNAS, is based on CGPin which each node represents a network layer and a layer can use one of 35 multipliers. Figure  12.8shows resulting Pareto frontsfrom four independent experiments in which EvoApproxNAS utilized four different sets of multipliers in convolutional layers: 88-bit accurate (blue), 8-bit accurate (orange), 88-bit approximate (red), and 8-bit approximate (green). By combining all these Pareto fronts, one can observe those approximate multipliers allow EvoApproxNAS to reach the best trade-offs between accuracy and energy of multiplication for almost all investigated regions of parameters.
Fig. 12.8
The energy-accuracy trade-offs obtained by EvoApproxNAS on CIFAR-10for four sets of multipliers that can be used by convolutional layers. Results are compared with various ResNet networks optimized with the ALWANN method  [50]
12.4.2.2 Two Search Algorithms
The approach presented in the previous section leads to a time-consuming search process due to the prohibitively huge joint space composed of the coupled yet different CNN architectureand hardware configuration spaces with extremely sparse optima.
To reduce the search cost, the problem is often decoupled. Two search algorithms are now employed. Algorithm is a common NASand works in the space of CNNarchitectures. Algorithm then performs the search in the space of hardware configurations. It can again be based on an EA; however, other search techniques have been utilized in the literature  [63]. The search algorithms can interact in different ways, for example: 
1.
samples a CNNmodel . No training of is performed.
2.
is executed to find the most suitable hardware configuration (satisfying all hardware constraintsimposed by the specification) for 
3.
If no suitable hardware configuration is obtained, is discarded, and step (1) is taken again.
4.
If satisfies all constraints, then is trained and then tested on the test data to get its accuracy 5.
Steps (1) to (4) are repeated until a suitable solution is not reached.
This approach is especially useful if finding a suitable hardware configuration for takes significantly less time than the training of . On the other hand, if a super net is employed, it is not necessary to train candidate CNNarchitectures (subnets), and a search in the hardware configuration space can be conducted for architectures showing acceptable accuracy. From the NAS-hardware co-designmethods, surveyed by Sekanina  [63], Table  12.5lists those utilizing an evolutionary approach for (the NAS Methodcolumn) or (the HW opt. method). Selected hardware parametersthat are optimized are listed in the Design parameterscolumn. The remaining columns have the same meaning as in Table  12.4. Note that the use of EAs is relatively unexplored in this new area as documented by only three items in Table  12.5
Table 12.5
Evolutionary NASmethods with hardware co-design
Method
Ref.
NAS method
Objective
HW opt. method
Design parameters
Target device
Data set
QNAS
[34]
EA, supernet
EDP
EA
#PE, mem. params
ASIC
ImgNet, C-10
NAAS
[35]
Gradient, supernet
EDP
EA
#PE, mem. params., compiler mapping
TPU, ASIC
ImgNet, C-10
Pinos et  al.
[56]
EA
Energy
in NAS
Approximate multiplier type
ASIC
C-10, SVHN
QNAS  [34] focuses on optimizing the parameters of a mixed-precision systolic-array-like architecture (the array size, buffer input/weight/output size) while searching the quantized neural architecture. It includes an EA-based hardware architecture search and a one-shot supernet-based quantized neural architecture search. First, a suite of neural architectures is sampled as a benchmark to find the hardware architecture that achieves the best performance on the benchmark. The hardware architecture is fixed, and the quantized neural architecture search (QNAS) is then performed to determine the neural architecture and quantization policy. The quantized neural network is composed of multiple ResNet blocks.
NAAS  [35] holistically searches the neural network architecture and accelerator architecture, and unlike other methods (e.g.,   [27]), compiler mapping. The accelerator search space is defined by the number of processing elements, local memory size, global buffer size, memory bandwidth, and connectivity parameters. NAAS employs EA to optimize these parameters as well as the compiler mapping(the execution order and the tiling size). It introduces a special encoding, called importance-based encoding, for the accelerator space and the compiler mappingstrategies to avoid enumerating all possible situations and representing them by indexes. First, NAAS generates a pool of accelerator candidates. For each accelerator candidate, a network architecture is sampled from a pre-trained network  [6] that satisfies the pre-defined accuracy requirement. Since each subnet is well-trained, the accuracy evaluation is fast. Finally, the compiler mappingstrategy is sought for the network candidate on the corresponding accelerator candidate.
Figure  12.9shows the impact of various approaches in optimizing the accuracy and Energy-Delay-Product (EDP) of ImageNetclassifiers based on ResNet-50 and implemented on an Eyeriss-like chip. The original implementation (black point) of ResNet-50 (no NAS employed) is improved by a hardware search algorithm from QNAS  [34] (green point). Additional improvement is provided by NAAS performing the hardware and compiler mappingco-search (orange point). The best trade-offs are reported for NAAS utilizing the hardware, compiler mapping, and CNNarchitecture co-search (blue points). These results (adopted from  [35]) demonstrate that exploiting more design spaces can lead to better CNNimplementations.
Fig. 12.9
Normalized EDP and top-1 accuracy (on ImageNet)obtained by NAS methods for CNNsrunning on an ASIC  [35]: NAAS co-optimizing HW, compiler mapping, and NN architecture (blue); NAAS co-optimizing HW and compiler mapping (orange); hardware search from QNAS (green); No NAS conducted (black).
Adopted from  [35]
12.5 Conclusions and Open Challenges
We surveyed evolutionary approaches developed to optimize hardware implementations of CNNsand the NASmethods utilizing EAs. The optimization of various components and implementation principles of hardware acceleratorswith EAs seems to be a useful strategy because the relationships among all internal variables in complex systems such as hardware acceleratorsare highly nonlinear and corresponding search spaces are hard to explore.
Introducing the hardware search space in NAS algorithms has led to more efficient implementations of CNNson particular hardware platforms. However, several search algorithms working in the space of weights, neural architectures, and hardware configurations have to be coordinated, making the entire method complicated. Successful adoption of EAs in these applications requires utilizing not only modern multi-objective evolutionary designand optimization methods but also state-of-the-art (surrogate) modelingand simulation techniques to get reliable information about the underlying hardware quickly.
In the following sub-sections, we outline the challenges that are critical for the successful development in this area.
12.5.1 Benchmarking and Reproducibility
As hardware-aware NASmethodsare multi-objective, their fair assessment consists of evaluating multiple parameters of resulting implementations of CNNsand the design cost(time). It thus leads to an expensive construction and comparison of Pareto frontsin multidimensional spaces, which is often hard to perform because of incomplete information about some NAS methods. To support a fair benchmarking methodology and accelerate the development of new NASmethods, the open-source data sets containing many pre-trained and evaluated CNNsfrom well-defined search spaces were introduced in the literature, e.g., NAS-Bench-201 [16]. In addition to the accuracy for each design point in the search space, hardware parameters(such as latency and power) are also precomputed for some hardware accelerators[31]. Hence, new NAS algorithms can quickly be developed and evaluated for pre-defined search spaces. We see a lot of space for further opening the whole field to a broader community of researchers and practitioners by sharing NASimplementation source codes, data generated by NAS methods, data measured on real accelerators, and data obtained from simulations of various configurations of hardware accelerators. This effort should also improve the reproducibilityof results in this area.
12.5.2 Security and Reliability
In additionto optimizing the quality of service, performance, and power consumption, other objectives must be considered when hardware-accelerated DNNs are deployed in real-world systems. DNN systems are highly vulnerable to securityand reliabilitythreats at both the cloud and the edge. Security attacks include inserting random or crafted noise into the data, inserting malicious components into the system hardware, polluting inputs with imperceptible noise during inference, and monitoring system-side channels to deduce the underlying model  [64]. Reliability issuesinclude process variation during hardware fabrication, memory errors, and specific environmental conditions around the system that compromise reliabilityduring training and inference. Shafique et al.  [64] surveyed the threats and their respective countermeasures. One example of reliability-aware EA-based NAS—paper [62]—was discussed in Sect.  12.4. We expect a lot of research that could potentially utilize evolutionary algorithms in the areas of securityand reliabilityof CNNaccelerators.
12.5.3 Unconventional Hardware Platforms
Emerging technologies suchas memristive crossbarsor in-memory computingare investigated for CNNaccelerators to reduce power consumption and other critical parameters  [2, 66]. A very specialized simulatoris usually developed to analyze the properties of these unconventional circuits and systems. The simulator can be connected with a NASalgorithm to find best-performing CNN-accelerator pairs. For example, PABO  [54] uses NASconnected with a memristive crossbar-based CNNaccelerator, where the CNNis mapped across the on-chip crossbar storage spatially. NAS4RRAM is a NAS method for optimizing CNNsand Resistive Random Access Memory (RRAM)-based accelerators  [77]. NACIM  [26] jointly explores device, circuit, and architecture design space and also takes device variation into account to find the most robust neural architectures, coupled with the most efficient hardware design for an in-memory computingASIC. In the future, more exotic hardware platforms for CNNscould be introduced (e.g., similar to the nanoparticle networks configured using evolutionary algorithms for solving simple problems  [5]) to provide richer and deeper interaction of machine learning and configurable physical materio
12.5.4 Design Cost
The evolutionary NAS methodis a computationally expensive approach requiring many core hours producing considerable emissions. We expect many new approaches to reduce the computation cost in all directions, including efficient search algorithms, network training algorithms, hardware simulation, and benchmarking strategies.
Acknowledgements
This work was supported by the Czech science foundation projectAutomated design of hardware accelerators for resource-aware machine learningunder number 21-13001S.
References
1.
Barone, S., Traiola, M., Barbareschi, M., Bosio, A.: Multi-objective application-driven approximate design method. IEEE Access 9, 86975–86993 (2021)Crossref
2.
Bavikadi, S., Dhavlle, A., Ganguly, A., Haridass, A., Hendy, H., Merkel, C., Reddi, V.J., Sutradhar, P.R., Joseph, A., Pudukotai Dinakarrao, S.M.: A survey on machine learning accelerators and evolutionary hardware platforms. IEEE Design & Test 39(3), 91–116 (2022)Crossref
3.
Benmeziane, H., El  Maghraoui, K., Ouarnoughi, H., Niar, S., Wistuba, M., Wang, N.: Hardware-aware neural architecture search: survey and taxonomy. In: Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pp. 4322–4329. International Joint Conferences on Artificial Intelligence Organization (2021). Survey Track
4.
Bingham, G., Macke, W., Miikkulainen, R.: Evolutionary optimization of deep learning activation functions. In: Proceedings of the 2020 Genetic and Evolutionary Computation Conference, GECCO ’20, pp. 289–296. ACM (2020)
5.
Bose, S.K., Lawrence, C.P., Liu, Z., Makarenko, K.S., van Damme, R.M.J., Broersma, H.J., van der Wiel, W.G.: Evolution of a designless nanoparticle network into reconfigurable boolean logic. Nat. Nanotechnol. 10, 1048–1052 (2015)Crossref
6.
Cai, H., Gan, C., Wang, T., Zhang, Z., Han, S.: Once-for-all: train one network and specialize it for efficient deployment. In: 8th International Conference on Learning Representations, ICLR. OpenReview.net (2020)
7.
Cai, H., Zhu, L., Han, S.: Proxylessnas: direct neural architecture search on target task and hardware. In: 7th International Conference on Learning Representations, ICLR. OpenReview.net (2019)
8.
