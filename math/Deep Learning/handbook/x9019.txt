144.
Yu, W., Li, B., Jia, H., Zhang, M., Wang, D.: Application of multi-objective genetic algorithm to optimize energy efficiency and thermal comfort in building design. Energy Build. 88, 135–143 (2015)
145.
Zhen, X., Enze, Z., Qingwei, C.: Rotary unmanned aerial vehicles path planning in rough terrain based on multi-objective particle swarm optimization. J. Syst. Eng. Electron. 31(1), 130–141 (2020)
146.
Zhou, Y., Fan, D., Zhang, B., Li, R., Noack, B.R.: Artificial intelligence control of a turbulent jet. J. Fluid Mech. 897:A27, 1–46 (2020)
147.
Zupančič, J., Filipič, B., Gams, M.: Genetic-programming-based multi-objective optimization of strategies for home energy-management systems. Energy. 203, 117769 (2020)©  The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.  2024
W. Banzhaf et al.(eds.)Handbook of Evolutionary Machine LearningGenetic and Evolutionary Computationhttps://doi.org/10.1007/978-981-99-3814-8_23
23.  Evolutionary Machine Learning in Robotics
Eric  Medvet1, Giorgia  Nadizar1, Federico  Pigozzi1and Erica  Salvato1
(1)
University of Trieste, Trieste, Italy
Eric  Medvet
Email: emedvet@units.it
Abstract
In this chapter, we survey the most significant applications of EML to robotics. We first highlight the salient characteristics of the field in terms of what can be optimized and with what aims and constraints. Then we survey the large literature concerning the optimization, by the means of evolutionary computation, of artificial neural networks, traditionally considered a form of machine learning, used for controlling the robots: for easing the comprehension, we categorize the various approaches along different axes, as, e.g., the robotic task, the representation of the solutions, the evolutionary algorithm being employed. We then survey the many usages of evolutionary computation for optimizing the morphology of the robots, including those that tackle the challenging task of optimizing the morphology and the controllerat the same time. Finally, we discuss the reality gapproblem that consists in a potential mismatch between the quality of solutions found in simulations and their quality observed in reality.
23.1 Robot Optimization and its Peculiarities
The fieldof robotics involves the design, construction, and operation of robots. In this chapter, we define as robot any agent, be it real or simulated, which can interact with an environment. We require said interaction to be bidirectional, meaning that the agent can affect the environment with its actions, and is in turn affected by the environment, either through sensory perceptions or simple mechanical effects.
A robot is defined by its bodyand its controller, which are deeply interconnected. The controller(often called brain) is responsible for making decisions concerning the actions to be made. Oftentimes, such decisions consist of computing control values which are sent to the body, which then actuates them. The bodyof the agent involves all the physical aspects of the robot, such as its external aspect, its constituting materials, or its modules and connecting parts (joints). In addition, the bodyis responsible for actuating actions, and also for proprioception and environmental awareness.
In general, the actions of a robot are driven toward the achievement of a goal, which we define as the taskof the agent. Within this paradigm, it is possible, and often desirable, to optimizethe agent for a task, that is, achieving a design that enables the robot to most successfully accomplish its goal. However, due to the complexity of the design process, given the usually extremely vast search space, handcrafting satisfying solutions is practically unfeasible. Therefore, automatic optimization techniques play a fundamental role in assisting designers in the achievement of successful robots. Among those, evolutionary algorithms (EAs) stand out for their effectiveness and efficiency in exploring the solution space, yielding to extremely successful results with relatively little human effort. In particular, in most cases, they only require to define a suitable measure of quality (i.e., the fitness) of the solution under optimization (i.e., the robot): while defining such fitness is not always easy  [34], it is in general much easier than attempting to manually design the solution.
The application of evolutionary computation (EC) to robotics constitutes the field of evolutionary robotics(ER)  [36, 129]: in a broad sense, ER consists in optimizing the robot (or some of its parts) for a given task, using EC. In this chapter, we deal with EML in robotics, which can be considered the sub-field of ER where ML is somehow involved. As a matter of fact, the vast majority of EML applications to robotics deal with the optimization of a robot controllerwhich is based on an artificial neural network (ANN): indeed, ANNs are traditionally considered an artifact belonging to the field of machine learning (ML). As a consequence, the largest part of this chapter, namely Sect.  23.2, is devoted to surveying the bodyof literature dealing with evolutionary optimization of ANN-based controllers forrobotic agents. There are, however, other artifacts that can be employed as controllers and can be optimized with EC: for example, behavioral trees can be evolved using genetic programming. From the point of view of the definition given in this book, they can be considered at the boundary of EML. We survey some of these approaches in Sect.  23.2.2
From a broader point of view, robotics is a field that exhibits a few peculiarities that are relevant to the usage of EC as a form of optimization.
First, optimization can be performed at various levels, targeting different features of a robot. Namely, most applications of EML in robotics are either aimed at optimizing the controller, the bodyof the agent, or both, but there are also some more peculiar examples of EML in robotics, as we will see in Sect.  23.4. It is worth to note that, even when just the controller is subjected to optimization, the body still plays a relevant role, because it actively takes part in the way the robot interacts with the environment by processing perception to decide actions to be performed; in other words, the bodyis capable of performing some morphological computation  [63]. This phenomenon is captured by the embodied cognitionparadigm  [140].
Second, robots are not static artifacts, but change over time. In the simplest case, the only thing that changes is their spatial configuration, i.e., their position within the environment or the relative position of their components. In more complex scenarios, they can suffer malfunctions  [96, 105] or even grow over time  [94, 120]. What is of key importance, however, is that a robot existence (or life) is not instantaneous: hence, the evolution time-scale may interact with the life time-scale and adaptation can occur at both levels. This opportunity has been exploited by researchers for combining EC with learning, morphological development, or other forms of adaptation in order to obtain robots that are eventually better in performing their task.
Third, robots, and the environment they are immersed in, are usually complex and sometimes dangerous. The straightforward application of the main EC steps, i.e., evaluation, selection, and variation, is often not feasible using the real robot in its real environment, because the evaluation stage is too costly, not scalable, or even dangerous (for the robot itself, the environment, or other human operators involved in the optimization process). For this reason, the optimization is often carried out in simulation, possibly exploiting computing machinery that allows fast computation and well fits the inherent feature of EAs to be massively parallelizable. However, the simulation is rarely capable of capturing each subtle aspect of the reality and this can eventually result in an optimized robot whose behavior in reality is different to the one observed in simulation. This problem is known as the reality gapproblem: we discuss it in Sect.  23.5
23.2 EML for Controller Optimization
The controller of a robot is responsible for deciding which actions need to be performed. In many practical cases, this boils down to computing the control values that are sent to the bodyto guide actuation. Since robots are embodiedagents, it is often convenient to equip them with sensors of various kinds, whose readings can be used as feedback by the controllerto effectively guide movements (closed-loop controllers). For these controllers, the control values are, broadly speaking, a function of the sensor readings and possibly of past experience, in case there is some form of memory.
In the field of ML, ANNs are everywhere being deployed to approximate functions for solving a very diverse variety of tasks. The robotics domain is no exception, and ANNs are among the most used tool within the agent controllerto compute its next actions. Since engineering ANNs is not an easy task, requiring a lot of domain expert knowledge, experience, and trial and error, EAs often come in handy for obtaining well-optimized ANNs that can successfully control artificial agents. More in details, neuroevolution (NE)has been beneficially applied both for neural architecture search (NAS)and for training various flavors of ANNs (see Sect.  23.2.1). In addition, EAs have also been combined with Reinforcement Learning (RL)to effectively train ANNs to solve robotics tasks.
Aside from ANNs, several other approaches have been proposed merging ML techniques with evolutionary optimization for robot control, as we will detail in Sect.  23.2.2. Among them, we can include behavior treesand more classical control theory approaches.
23.2.1 Neuroevolution
Neuroevolution (NE)is the sub-field of evolutionary computation that deals with ANNs. Here, NE has the objective of searching for a good robot controller, i.e., a good robot brain, meaning that it has to effectively search the space of ANNs, eventually finding the most suitable one for guiding an agent toward the accomplishment of its task. Several works exist in which NE has been applied to optimize robot controllers. Even though it is not feasible to analyze each of them in detail, we aim at providing an overview of the features of some relevant studies, together with a characterization along the following axes:
task to be solved by the robot
ANN model and architecture
EA used for the optimization
presence of another adaptation time-scale (e.g., learning)
Finally, we discuss the case where NE has been used for evolving one or more controllersthat are shared by many robots that interact, more or less tightly, to achieve a common goal.
23.2.1.1 Tasks Considered
For allowing NE to truly shine, the tasks which have been mostly taken into consideration in the examined literature are those which require advanced controllers, either because of the task difficulty itself or because of the complexity of the robot involved. Among the first ones, we can mention navigation[17, 18, 47, 59, 159, 160, 163] (see Fig.  23.1), predator-prey tasks  [30], or locomotiontasksin complex environments  [155], for which the sensor-actuator mappingbecomes non-trivial, even considering a simple agent with a restricted set of actions.
Fig. 23.1
A schematic view of a navigation task for a differential drive wheeled robot (left) and its realization with a few human mock-ups (right); both images are taken from  [163]. The task has been solved by [163] with NEAT
Among the latter ones, locomotionis usually the go-to task, as for legged robots (see Fig.  23.2), for instance, it already requires the control of several robotic parts  [2, 54, 150]. In the case of legged robots, the task of obtaining a policygoverning the movements of the legs is often referred to as gait generation
Fig. 23.2
Top view of two legged robots, each with four legs, for which the gait has been optimized by [150] using a few variants of Hyper-NEAT
Image taken from  [150]
Neuroevolutionhas been employed also for solving tasks related to industrial robots, among which the task of manipulating an object using a robotic arm with a suitable effector is a prominent example  [68, 179] (see Fig.  23.3).
Fig. 23.3
An overview of the task tackled with neuroevolution by [179]: the goal is to make the antropomorphic robotic arm (equipped with a human-like hand as effector) able to discriminate different kinds of objects using perception. The robot is controlled with an ANN with a single hidden layer of few nodes: the input layer is fed with readings coming from arm and hand proprio-sensors and tactile sensors; the output layer provides the arm and hand actuators values and the category of the object being manipulated as estimated by the robot.
Image taken from [179]
Last, when dealing with modular robots or swarm robotics, self-assembly[85] or reconfiguration  [194] tasks are also noteworthy test beds for evolutionary optimization: the use of ANNs for solving these tasks, and hence of NE for optimizing them, is still an open front.
23.2.1.2 ANN Model and Architecture
Focusing more on the NE aspect, the first features we concentrate on are the neuron model and the neural architecture employed. TheMcCulloch and Pitts neuron model (perceptron) is one of the most commonly used  [106] for computing neurons outputs, thanks to its simplicity and computational efficiency  [141, 174]. However, quite a few works have considered more biologically plausible models of neurons, such as the bio-inspired spiking neuron model  [72, 176] (see Fig.  23.4).
Fig. 23.4
A schematic view of a neuron model based on spikes, instead of continuous values; image taken from  [154]. [121] optimized a controllerfor modular soft robots based on this model
Concerning the architecture, again most of the works aim at simplicity and computational efficiency, choosing a fully-connected or a sparse feed-forward ANN. The latter ones, in particular, can either result from a process of NAS(as we will see in the following paragraph) or can be obtained by pruning  [119, 122], that is the removal of some neural structures (e.g., synapses) during the lifetime of the agent.
Some more sophisticated architectures of ANNs have also been used, namely, Recurrent Neural Networks (RNNs) and Long Short-Term Memory (LSTM), in order to endow the controllerwith a form of memory, which is particularly useful for the accomplishment of tasks that benefit from tracking previous perceptions and/or actions  [1, 196] such as navigation. In addition, recent advances in the field of deep learning (DL) have ignited the experimentation with deeper and more complex ANNs as robotic controllers, involving DL inspired elements such as self-attention modules  [143, 175] (see Fig.  23.5).
Fig. 23.5
A neural controllerincorporating an attention module, optimized by [143] for controlling a modular soft robot; image taken from  [143]. Self-attention is a mechanism that allows the ANN to enhance some of the inputs with respect to other ones, resulting, in practice, in a form of auto-adaptation. Thanks to the attention module, the authors were able to make modules (colored squares in the images on the right) not needing any form of communication among them, still obtaining a collective behavior highly effective for the considered task (locomotion): specifically, the ANN in each module becomes more attentive to sensor readings that are more useful locally, hence performing a sort of specialization
Last, a slightly separate role is played by Central Pattern Generators(CPGs), which are biological neural circuits that produce rhythmic outputs in the absence of rhythmic input  [15], that have been successfully employed in those tasks (e.g., locomotion)where a periodic behavior is useful for achieving stability and high performance  [78, 87, 102, 177] (see Fig.  23.6).
Fig. 23.6
The main component of a CPG (a differential oscillator, on left) and its usage within a specific morphology of a modular robot; both images are taken from  [78]. In brief, a CPG constitutes a dynamical system whose evolution over time (i.e., the dynamics) is determined by a few numerical parameters. [78] used CPGs and CPPNs (see Sect.  23.3.1.2) for evolving both the morphology and the controllerof simulated modular robots for the task of locomotion. Specifically, they used a compound of CPGs as an open-loop controller, i.e., one in which the CPGs are not fed with sensory feedback, since gait learning on a flat surface can be solved without such feedback. Moreover, the authors employed a Lamarckian approach, i.e., one in which some traits are developed after birth and are then inherited by the offspring
23.2.1.3 Evolutionary Algorithm
Another axis of categorization regards the evolutionary aspect, i.e., what is being evolved and what EA is being used to this extent. As seen in the previous paragraph, many works have relied on fixed neural architectures, be them feed-forward, recurrent, or more refined. In those cases, the most common approach to training involves evolving the parameters, i.e., the synaptic weights, of the ANN with EAs that are suitable for fixed-length numeric genotypes, such as evolutionary strategies (ES). Not only have ES been proved to achieve state-of-the-art performance on a wide set of tasks  [155], but, as opposed to gradient-based optimization methods (e.g., backpropagation) they need not enforce any particular constraint on the outputs of neurons. As such, ES have been profitably used to overcome training issues like the non-differentiability in Spiking Neural Networks (SNNs)  [31, 33, 47, 59, 121, 147, 159, 160]. In addition, within the domain of fixed architectures, quality-diversity (QD) approaches have also been explored, in order to avoid getting stuck in local optima  [27, 46] (see Fig.  23.7).
Fig. 23.7
Two examples of usage of QD EAs for optimizing the ANN-based controllerof robotic agents. aFour variants of ES, three of which employing a form of QD; image from [27]. [27] used ES for optimizing the neural network controlling a simulated humanoid for the task of locomotion. The plots show the behaviors of different agents in terms of position (top view): QD is clearly beneficial, as it allows the robot to overcome an obstacle being placed in front of it at its initial position (the star). bOverview of the outcomes of several optimizations performed with ES or Map-Elites (ME, a form of QD evolution  [118]) in terms of fitness (above) and evolvability (below), in the case of controlleroptimization for modular soft robots; image taken from [46]. xand yaxes of each plot represent two behavioral descriptors. [46] compared different representations and EAs in terms of their ability to favor evolvability and exploring the space of solutions
On the other hand, many studies have encompassed non-fixed neural architectures, relying on the evolutionary process for obtaining the most suitable ANN structure for the task at hand. In fact, as recently shown by [50], often the architecture of the ANN plays such an important role, that even random weights could be used, as long as the architecture stays untouched. Along this line, many have resorted to EC applied to NAS, mainly applying the NeuroEvolutionof Augmenting Topologies (NEAT) algorithm  [171, 172] for obtaining well performing and robust robot controllers[17, 18, 163, 188].
A step further has been taken with Hyper-NEAT[170], a generative encodingwhich evolves large scale ANNs with the principles of NEAT. This approach has led to outstanding results for the control of robots with high symmetry, such as legged ones  [2, 25, 30, 58, 150], since Hyper-NEATcan automatically identify and effectively exploit problem regularities (see Fig.  23.8), yielding to emergent controllermodularityand high coordination. As a side note, some older works have also focused on the importance of ANN modularityfor control tasks  [19], yet handcrafted solutions which tried to take advantage of symmetry and repetitions  [54, 181] were in general less fortunate than those obtained through a generative encoding
Fig. 23.8
Schematic view of the usage of Hyper-NEATfor optimizing the controllerof a robotic-like eight-legged character: on the right, the mappingbetween inputs and output that exploits the physical structure of the robot; on the left, a representation of the best ANN obtained for the task of locomotion
Both images are taken from [2]
23.2.1.4 Adaptation Time Scale
Another significant aspect to take into consideration when analyzing literature on ANNs for robot control regards the adaptation aspect. More precisely, most works focus on evolutionary adaptation, namely relying on the time-scale of different generations for improving the ANNs of the agents. However, some researchers have experimented with a shorter adaptation time-scale, that is life time learning, with the goal of fostering generalization to unforeseen circumstances, as for biological creatures  [48, 130, 131]. In this context, the most fruitful results have been achieved with unsupervised Darwinian learning, i.e., learned traits are not transferred to the offspring, in the form of neural plasticity, i.e., Hebbian learning. These ideas have been ignited by [123] which have proposed to evolve the synapse-specific Hebbian learningrules instead of the synaptic weights, and have been productively applied for controlling different types of robots to enhance their resilience to bodyalterations and/or environmental changes  [44, 138].
23.2.1.5 Modular Robots and Swarm Robotics
Thelast view-point considers the amount of “independent” modules or robots involved. In the first case, we consider modular robots, for which the modules may be able to detach or at least to control themselves independently of the others (see Fig.  23.9), whereas for the latter case we fall into the category of swarm robotics. In this context, we can put forward the concepts of monomorphic and polymorphic systems, where the first indicates modular agents or swarms were all components are alike, while the second refers to heterogeneous compounds. Some works have found monomorphism to be preferable  [136, 142] for both modular robots with truly embodied controllers[108] and robot swarms  [14, 85]. Intuitively, optimizing monomorphic components (i.e., modules or robots) is easier because the search space is smaller; moreover, the interaction between several independent components is facilitated when they are similar to each other. In addition, it has been observed that despite the independence of the agents, coordination can still emerge  [14, 58, 121], even in the absence of explicit inter-agent communication  [143].
Fig. 23.9
An overview of an evolutionary approach for optimizing the controllerof simple robotic modules that have the capacity of joining and hence forming complex morphologies; image taken from [136]. Single modules are governed by ANNs that determine the actions to be performed (i.e., how to control the actuator) and the messages to be sent to neighbor modules. Despite the approach is described by [136] as a form of co-evolution of morphology and controller, the optimization is actually a form of RL
23.2.2 Other Combinations of ML and EC for Controller Optimization
Besides ANNs, several other approaches to robot controlleroptimization have involved the application of EC. In this section, we try to cover some relevant works within this broad area
23.2.2.1 Combining Evolution with Reinforcement Learning
Reinforcement Learning (RL)is the field of ML in which an agent learns the desired behavior through a trial-and-error process of interaction with an environment  [173]. Due to its formulation, RL is naturally suited for the robotics context, in which it is applied to learn a controller, called policyinthe RL context, which maximizes a certain reward signalduring a given time span, e.g., during the life time of the agent. The core of RL is hence the agent policy, which can be represented in several manners, and needs to be optimized through a policy search process  [164] that can be conducted in various ways, depending mostly on the representation chosen and on the related search space.
For instance, it is possible to rely only on EC for policysearch with different EAs according to the specific representation chosen  [27, 145, 155, 171, 190]. In this context, the dependency between the policyand the effectiveness of the corresponding behavior is not explicitly modeled; instead of trying to estimate the model, the search process only aims at moving solutions toward points in the search space which maximize the final objective, i.e., yield to a higher cumulative reward. Even though EC for policysearch suffers from lower sample efficiency as compared to classical RL algorithms, there have been some attempts to tackle this issue  [89, 145]. In addition, it has been shown how EC can be a scalable alternative to classical RL  [155], achieving state-of-the-art performance on a benchmark of control tasks.
Other examples in which EC has been applied to RL aim at leveraging the advantages of the two approaches. In particular, two mainstream ideas suggest to make use of EC to improve the outcome of RL techniques or to allow EC and RL to act as two concurrent forms of adaptation occurring along different time-scales, as in  [60] (see Fig.  23.10). Concerning the former idea, EC has been used for reward shaping, to discover a reward signalthat allows the agent to efficiently learn  [128].
Fig. 23.10
The scheme principled by [60] for combining RL and evolutionary computation for ER; image taken from [60]. During the evolution, robotic agents are initially developed with an instinctive behavior dictated by a policy inherited from parents; their policy can change during their life based on their interactions with the environment, according to an RL approach; the selection phase of the evolutionary process takes finally into account the fitness resulting from the learned behavior, rather than just the instinctive one. The authors assessed their approach on a few (simulated) robotic tasks and found promising results
Some have tried to achieve explainable policies, which are of primary importance in critical settings, relying on tree-based representations and genetic programming (GP) in combination with RL  [29, 64, 193]. Last, EC and RL have often been combined, in and out of the robotic context for algorithm acceleration  [38, 103, 104, 116, 137], using EC as bootstrapping for RL, or for hyper-parameters tuning  [73].
23.2.2.2 Behavior Trees
Behavior Trees(BTs) describe the controllerof an agent by the means of a tree, which is meant to be traversed until reaching a terminal state (see Fig.  23.11). The leaves in BTs correspond to executable behaviors, whereas internal nodes represent control flows. This representation was first invented to enable modularityin artificial intelligence for computer games, but has recently received an increasing amount of attention in the robotics community  [70]. The main advantage of BTs lies in their interpretability, which comes in particularly handy when the considered task is critical or there is the need to adjust the learned behavior by hand to address reality gapissues  [161] (see Sect.  23.5). Given their tree-based structure, GP is naturally suited for optimizing BTs  [71], and it can also be boosted with grammatical evolution (GE) to enhance the final performance  [61, 79]. BTs have also been profitably applied in various multi-agent contexts, to ease the understanding and prediction of the emergent swarm behavior  [81, 125, 126].
Fig. 23.11
An example of a BT for a mobile manipulator, i.e., a robot composed of a manipulator mounted on a wheeled platform;
image taken from  [70]
23.2.2.3 Evolution in Control Systems
Despite not belonging fully to the ML context, several classical control theory approaches have benefited from EC techniques in robotics, so we briefly report some relevant works in the field here. For instance, EC has been used in combination with forward kinematics equations for trajectory planning in robotics  [186], in order to reduce the impact of noisy environments  [68]. In addition, GP has often been exploited to address the problem of inverse kinematics in manipulators (see Fig.  23.12), aiming at obtaining precise yet interpretable results  [20, 35, 86, 149]. Moreover, EC has been often applied in combination with control algorithms for parameter fine-tuning in robots like Unmanned Aerial Vehicles (UAVs)[10] or walkers  [66].
Fig. 23.12
An overview of the approach of [35] for performing the inverse static kinematic calibration of industrial robots using GP; image taken from  [35]. The authors used the capability of GP of optimizing both the structure and parameters of a regression model for building concurrently six models for the six joints of the robot
Last, we quickly mention some applications of EC on fuzzy control systems [42, 127, 146], that are those control systems based on fuzzy logic. In this context, EC has mostly been used for tuning the parameters of the fuzzy controller, in several robotic scenarios. In particular, autonomous vehicles have been a widely used test-bed for deploying fuzzy controllerswith feedback  [55]. Various works have encompassed wheeled robots in navigationtasks  [24, 82], and have used EC to achieve more interpretable results  [83, 84].
23.3 EML for Body and Body-and-Brain Optimization
The embodied cognitionparadigm  [140] posits that the intelligence of an agent (natural or artificial it may be) emerges from the complex interactions between the body, the brain, and the environment. Intelligence is not only rational, but is also embedded in the body. The paradigm stands in contrast with the traditional view inside the computer science community of intelligence being abstract, purely Platonic “reason”  [191]. Indeed, bodiesco-evolved with brains and shaped them, inasmuch as the bodydefines the actions that the brain can “afford”  [51]. As a matter of fact, there exist organisms capable of performing complex computations by the means of their bodiesonly. Individuals of the genus Planariaregrowamputated limbs by virtue of biological processes localized in the severed portion of their bodies  [98]. Individuals of the species Trichoplax adhaerensself-organize their cilia to locomote in the absence of any nervous system  [16]. As a notable instance in the artificial domain, passive walkers locomote by the means of bodydynamics only  [107] and are already a mature field of study within robotics  [26], to the point that artist Theo Jansen applied EC to design the Strandbeests(see Fig.  23.13), passive walkers that rely on wind energy only, to raise environmental awareness  [77].
Fig. 23.13
One of the Strandbeests realized by the dutch artist Theo Jansen, named by the creator Animaris Percipiere;
image taken from  [77]
