Time-delays between the action and the response;
Uncertainties including model uncertainties, parameter uncertainties, disturbances, and noise;
Multiple, sometimes conflicting control objectives;
Partial observation of the system state;
Distributed actuators and sensors;
Control theory based essentially on linear control has been successful for a large range of systems; methods such as Model Predictive Control (MPC) and Linear-Quadratic-Gaussian control (LQG) have been largely employed in many fields such as automotive, chemical processing, and power generation, to name a few examples. The stability of closed-loop behavior has been proven for optimal control. On the other hand, robustness against changes in operating conditions is not always guaranteed. We refer in particular to the famous paper by J. C. Doyle on the absence of guaranteed margins for LQG regulators [43]. For complex systems, reduced-order models can be built and linearization around a state of interest allow to employ linear control methods. Yet, those models may lack robustness as they have a limited domain of validityand generally don’t take into account the effect of control. Robust controllerscan be built with adaptive control methods, i.e., with online parameter estimation and control update. Nevertheless, the robustness of such methods is far from guaranteed.
Fig. 22.2
The two main control design frameworks. For simplicity, the reference signal and disturbances are not displayed. aModel-based control. A reduced-order model (M) of the plant (P) is built and used to optimize a controller(K) minimizing a cost function J. The controller is then used as such to control the plant. bMachine learning control. The controller is updated thanks to a machine learning algorithm, depicted by the yellow block (ML). The optimized controller is obtained by solving a regression problem
22.1.3 Why Evolutionary Machine Learning (EML)?
Machine learning opens a new avenue for the challenges of nonlinear dynamics. Machine learning may build “intelligent” controllersthat are able to leverage the nonlinearities of the system for improved performances and to perform well in a larger range of uncertainties than adaptive control [44]. In this framework, the control problem is reformulated as a regression problem for the control law. This optimization problem is in general non-convex and may feature many minima and plateaus. Machine-learned controllerscan be optimized directly in the plant in a model-free manner following an iterative and stochastic process. Figure  22.2summarizes the two main control design frameworks.
EML methods are particularly fitted to solve regression problems for control. The general idea of EML is the gradual optimization of candidate solutions to the regression problem (controllersin this context) following the Darwinian evolution. First of all, one or several candidate solutions also referred to as a populationof individuals, are randomly generated. All individuals are evaluated, or tested, to assess their performance or fitness. Based on their performance, the individuals are selected to generate the new generationof individuals. New individuals are generated by the stochastic modification of one individual (mutation) and/or recombination (crossover) of two individuals. Mutation and crossover are referred to as genetic operators. The choice of the genetic operator to generate a new individual is parameterized by the mutation and crossover probabilities. A new generation of individuals is produced until a stopping criterion is reached such as a performance threshold or total evaluation budget. Compared to other optimization solvers, evolutionary algorithms benefit from:
Noise insensitivity. These algorithms are gradient-free, i.e., they do not require the optimization problem to be differentiable;
A balance between exploration and exploitation enabling optimization in non-convex spaces including many minima, plateaus, and valleys;
Model-free optimization, the plant is considered as a black box and mappingsbetween the outputs and the inputs of the system are directly learned. No information on the dynamics of the plant is needed but can be included.
Interpretability. Symbolic regression solvers like genetic programming optimize function expressions that help human understanding of control mechanisms.
Easy to incorporate domain-specific knowledge. Constraints can be enforced in the structure of the solutions for example, and known solutions can be seeded in the population.
However, evolutionary methods may suffer from premature convergence, from slow learning, and come with no guarantee of the solution’s stability. EML parameters (population size, generations, selection method, mutation, and crossover probabilities) often require fine-tuning to achieve the best performances.
Fig. 22.3
Taxonomy of the main Evolutionary Machine Learning (EML) methods employed for control (right). The methods are divided into two groups: methods for optimization of parametrized controllers(top) and methods for optimization of general controllers (bottom). Classical methods of control theory and fuzzy control (left). Studies combining methods of evolutionary computation and control theory are reported in this review
22.1.4 Taxonomy of EML Methods for Control
Following [18], Machine learning (ML) is a sub-field of Artificial Intelligence (AI) regrouping that process and extract information from data. ML is subdivided into three categories:
Supervised: learning is achieved from labeled data with expert knowledge. The goal is to build a mappingbetween the data and the labels.
Semisupervised: learning is achieved from partially labeled data or by interaction with the environment. Semisupervised ML includes Reinforcement Learning (RL)and generative models.
Unsupervised: learning is achieved from unlabeled data. The goal is to derive the underlying structure of the data.
We locate evolutionary computation between supervised and unsupervised learning as the methods relies on the interaction with an environment/plant to solve a regression problem. This chapter focuses on the main methods of evolutionary computation employed for control, i.e., evolutionary algorithms, Learning Classifier Systems (LCS), Swarm Intelligence (SI), and Artificial Immune Systems (AIS). Figure  22.3gives the taxonomy of these EML methods in parallel with control theory methods that solve similar control problems.
Review papers such as [44, 48, 80, 131] detail many success stories of EML to solve control tasks. Thus, the automation control systems category is the most popular area of applications of EML with 10977 papers registered in the Web of Science database in 2020 [131]. Note that this number only includes studies using Genetic Algorithm (GA), Genetic Programming (GP), Differential Evolution (DE), Evolution Strategy (ES), evolutionary programming, and their variants. Interestingly, among the 10977 papers the vast majority of 10540, i.e., , employ algorithms to solve parametric optimization tasks.
In their review, [44] and [48] propose different classifications of EML methods based on the type of controlleror on the type of optimization. [44] makes the distinction between “pure” and “hybrid” controllers. Pure controllers are controllers learned from scratch with an evolutionary method, while hybrid controllersare known controllers (such as neural networks and fuzzy controllers) optimized or augmented with evolutionary methods. See Fig.  22.3for an overview of the main methods of control theory. On the other hand, [48] distinguish offline design of controllersand online optimization, i.e., the controlleris updated in real-time. Eventually, both reviews classify the methods into two groups:
Methods for parameterized control lawoptimization. A fixed structure of the control law is given and its parameters are tuned with EML.
Methods for general control law optimization. The control structure and parameters are learned simultaneously.
This study follows the latter classification.
22.1.5 Content of the Chapter
This book chapter is organized into six parts. First, the control problem is reformulated as a regression problem to be solved with EML (Sect.  22.2). Then, we review pioneering and among the most significant and recent applications of EML to parametric optimization (Sect.22.3) and control law optimization (Sect.  22.4) for control. In Sect.  22.5.1, EML for learning control-oriented reduced-order models is reviewed. Control stabilityand robustness of EML solutions are discussed in Sect.  22.5.2. Finally, Sect.  22.6concludes this chapter and envisions the future of EML for control.
22.2 Problem Formulation
In this section, we describe how the control problem can be reformulated as a regression problem to be solved with EML.
22.2.1 Control Objective and Cost Function
To employ evolutionary methods, the performance of a given controllerneeds to be translated into a fitness function to be maximized. In the following, we chose to employ the term costJfrom the control community. Contrary to fitness, the cost function is a quantity to minimize. In general, the control goal includes several non-commensurable and conflicting objectives, such as distance to the optimal solution, stability, convergence rate, noise sensitivity, robustness, actuation power, etc. A common way to design the cost function is then to take a weighted sum of several cost functions 
(22.1)
The choice of the weights () is strongly dependent on the applications.
There are also Pareto-based techniques that consider a cost vector and avoid choosing subjective weights. The goal is then to look for Pareto optimal solutions also referred to as non-dominated solutions, i.e., solutions in which one performance criterion cannot be improved except by degrading another one. Multi-objective genetic algorithms and multi-objective genetic programming methods have been developed to learn those Pareto fronts. However, the comprehensive explorationof the Pareto frontfor control problems with several objectives may require a large number of plant evaluations.
22.2.2 Control Optimization as Regression Problem
Once the cost function is defined, the control problem can be reformulated as a regression problem where the goal is to derive the best controllerthat minimizes the cost function J. In sensor feedback, the sensor signals are the control law input. Often, the sensor signals are lifted to a causal feature as control law input. The feature is causal, which means it is a function of the sensor signals and, potentially their history. The feature may be an estimated state from a dynamic observer or dynamical model [95], obtained by filtering into different components [88], or the time-delay coordinates [59].
We distinguish two categories of optimization problems.
Optimization of parameterized control laws. Here, the control law has a given structure with free parameters , e.g. the gain matrix of linear control. A general expression is: 
(22.2)
where is the actuation command, is the control law, are the free control parametersto be optimized, and is the sensor-based feature. The optimization problem is then to derive the optimal parameter vector that minimizes the cost function. The regression problem to solve is 
(22.3)
where Eis the space of admissible parameters. In control, EML has been largely employed for tuning PID (Proportional-Integral-Derivative) controllersor optimizing the weighting matrices in LQG or H-infinity controllers. Typical EML algorithms to solve such problems are Genetic Algorithms (GA), Evolutionary Strategies (ES, CMA-ES, etc.), Differential Evolution (DE), and Swarm Intelligence (SI) methods (PSO, ACO, etc.). The solutions are often represented by binary or n-ary strings [62], but there are also other representations closer to the problem such as gray coding and float point representation [106].
Optimization of general control laws. The structure of the control law is not imposed. In this case, a general expression is 
(22.4)
where is the actuation command, is the control law, and are the plant features. EML optimizes both the structure and its parameters. Thus, control laws can directly be learned with function optimization solvers such as Genetic Programming (GP) [79]. Functions can be represented by trees (like in gene expression programming) or matrices like in Linear Genetic Programming[16, LGP] for example. Functions in the frequency domain, or s-domain can also be learned with an optimization of block diagrams directly on Simulink [51]. The regression problem to solve is then: Find the optimal controllerthat minimizes the cost function J. 
(22.5)
with being the space of all admissible control laws.
One of the benefits of EML in controller optimizationis that the type of control is not imposed. Open-loop and closed-loop strategies can be learned indifferently as well as complex controllersleveraging noise to excite the plant. In practice, EML re-discovers open-loop and closed-loop control mechanisms and combines them to achieve better performances; This is especially true in fluid mechanics.
EML methods also benefit from a straightforward implementation of control constraints and domain-specific knowledge. Constraints can directly be enforced in the internal representation of the solutions to render some inaccessible. Pre-tests can also be done for each individual to avoid costly evaluations of unwanted solutions. The constraints can also be relaxed and included as a penalization term in the cost function.
Great care must be taken when designing the regression problem to ensure that it reflects the original control problem. The choice of the EML algorithm depends on the nature of the control laws, control inputs, control outputs, and genetic operators. Specific EML methods are often developed to take into account the specificities of the problems. In the following, we review pioneering recent applications of EML to control.
22.3 Optimization of Parameterized Control Laws
EML for parametric optimization hasbeen pioneered by [62] with GA. Since then many variants have been developed as well as other parameter optimization techniques based on evolutionary principles such as evolution strategies, differential evolution, and evolutionary programming. In this section, we review some of the most significant and recent control tasks achieved with parametric optimization using EML.
22.3.1 Genetic Algorithm, Differential Evolution, and Evolutionary Strategies
Flow control problems are notoriously hard to solve due to the inherent nonlinearities of the Navier-Stokes equations, the multi-scale feature of turbulence, and significant time delays between actuation and sensing. Nevertheless, [8] show that GA is able to learn the optimal parameters (voltage amplitude, burst frequency, and duty cycle) of a plasma actuator to control a turbulent separated flow in experiments. Two control tasks are achieved, the minimization of the reattaching distance and the maximization of the wall pressure fluctuations. After only a few generations of 120 individuals, GA learns the optimal solutions, i.e., forcing at the shear layer frequency and half the shear layer frequency, respectively. Thereafter, more challenging flow control problems have been tackled with EML and especially GP, see the next section for more details
Dracopoulos [44] reports various works combining GA and other artificial intelligence and control methods already since the ’90s. In particular, the author gives examples of combinations with neural networks, reinforcement learning, fuzzy logic, PID controllers, pole placement adaptive controller, and traditional optimization theory. Since then, [90] propose an improved genetic algorithm optimization fuzzy controller for the control of throttle valve in managed pressure drilling. The controller is developed to tackle the strong nonlinearities and time variability of the throttle valve control. The authors propose an improved initial population generation, an adaptive genetic operator probability selection, and genetic operators. The learned controller presents great advantages in terms of speed, stability, and robustness. More synergies between evolutionary algorithms and fuzzy controllers are described in [27–29, 61, 68, 134], and between GA and neuro-fuzzy controllersin various fields such as electrochemical systems [104], underwater vehicles [69], manipulation robots [122], and Unmanned Aerial Vehicle (UAV)control [14].
Other EML methods such as Differential Evolution (DE) and Evolutionary Strategies (ES) have also been employed for control. We refer to the review paper by [10] for a comprehensive review of DE over the two last decades. The authors report, in particular, the optimization of fuzzy controllers, multivariable PI controllers, and optimal controldesign. ES, CMA-ES, and other variants have been developed, such as the General Learning Evolutionary Algorithm and Method (GLEAM) for planning and control [13], and employed for the optimization of trajectory planning and a sliding mode tracking controller[31], the optimization of large-scale UAVcluster confrontation game in response to the shortcomings of optimal controland reinforcement learning[92], weight optimization of Artificial Neural Networks (ANNs) for reinforcement learningwith an ensemble of mutation strategies outperforming classical ES [2], automatic berthing with optimal controltheory [100, 101], low and high-dimensional control optimizationof reservoir management processes [4], and to optimize an adaptive predictive control of the hydrocracking process [89].
22.3.2 Swarm Intelligence Methods
Swarm Intelligence (SI) techniques are also well adapted for control design. PSO, in particular, has been able to solve large-scale nonlinear optimization problems when analytical methods failed to converge. Compared to GA, PSO benefits from an easier implementation with fewer parameters to tune, a more effective memory capability, and more efficient preservation of the population diversity[39]. In their review of PSO, [39] give examples of applications to power systems including PID controllertuning, weight optimization for neurocontrollers, and fuzzy logic-based controller optimization with hybrid GA-PSO methods. We refer to the review paper for more details on those applications. [132] reports that up to 38 SI methods have been developed between 1992 and 2017, and the two most popular SI methods being Particle Swarm Optimization (PSO) and Ant Colony Optimization (ACO). The authors also review SI applications to control design, such as PSO for control of renewable energy systems, or design of robust controllers, and ACO for optimal controlof pumping water distribution networks.
Lately, PSO has been employed for efficient coverage control for wireless sensor networks [139], disturbance rejection for position control of the magnetic levitation system [64], PID controlleroptimization for a quadrotor with a virtual sensor [112] and for efficient control of a nonlinear double-pendulum overhead crane with sensorless payload motion [67]. ACO has also been employed in several domains such as traffic control thanks to the Internet of Vehicles (IoV) [83], controlleroptimization for humanoid guidance in cluttered environments [127], optimization of a PID controller for vibration control of a wind turbine tower under different types of loads [120], and stability control for a magnetic levitation system [110].
22.3.3 Hybrid Variants
Efficiency of EML methods has been improved by merging them with other methods. [96] combine GA and the downhill simplex method to accelerate the learning of a chiller configuration for a heat source plant. GA acts as an explorationstep, its role is to find new basins of attractions in the search space and to avoid getting stuck in a local minimum. On the other hand, downhill simplex linearly combines the best individuals for a fast descent in the minimum; it acts as an exploitation step. The combination of the two methods is enabled by a straightforward conversion between the gene and vector representation. Such a method cures the poor exploitation of gradient information by GAs. The hybrid method requires 280 iterations to find the global optimum while classic GA needs 1320 iterations, i.e., a 79% evaluation cut.
In addition, PSO and AOC have been combined with several other methods in many applications, for example, [135] combine PSO and MPC for path tracking of four-wheel steering and four-wheel drive vehicles, [140] combine PSO and central pattern generatorfor control and optimization of a bionic robotic fish, [15] combine PSO and Gravitational Search Algorithm (GSA) to optimize a fuzzy sliding mode controllerfor Doubly Fed Induction Generators(DFIG) wind turbine, [12] combine ACO and Nelder-Mead’s downhill simplex to tune a PID controller for the automatic voltage regulator system, [84] combine ACO and sine-cosine algorithms for optimal path search and control of a mobile robot, and [26] combine ACO and GA to maximize the maximum power point of a photovoltaic module under varying temperatures and sunlight irradiance.
22.3.4 Multi-objective Optimization
As mentioned earlier, EML has also been employed to optimize parameters based on multiple and often conflicting objectives. Recently, multi-objective GA has been employed for hybrid electric vehicle parameter optimization [19] energy optimization in wireless sensor networks [70], response mitigation of a wind-excited tall building [77], PID controllerdesign [53], optimal distributions of actuators and sensors in structures [24, 25], energy optimization and thermal comfort in building design [144], solving complex multi-UAVmission planning problems [121], greenhouse environment control [94], and optimization of HVAC system energy consumption [111].
Multiple-objective problems have been solved with PSO, such as optimization of power management in AC/DC micro-grid [66], congestion control in wireless sensor networks with a hybrid multi-objective PSO-GSA algorithm [130], optimization of a robust fuzzy controllerfor a four-degree-of-freedom quadrotor [97], global path planning and path control for unmanned surface vehicle [54], path planning in rough terrain for rotary unmanned aerial vehicles [145], and PI control of DFIG wind turbine under electrical fault conditions [7]. We refer to the detailed review by [126] for more information on multi-objective meta-heuristics for controllertuning. ES has also been employed for multi-objective control of the IFAC 1993 benchmark control problem [11].
22.4 Optimization of General Control Laws
Inthis section, we review recent applications of EML methods to derive general control laws, i.e., problems where the structure of the control law is not imposed.
22.4.1 Tree-based Genetic Programming and Linear Genetic Programming
Thefirst applications of Genetic Programming (GP) to control are the examples provided by [79]. Control laws are represented by tree expressions. GP is employed to solve the broom balancing problem where the goal is to balance a broom on a translating cart in minimal time. To assure the robustness of the solution, 10 initial conditions have been tested for each individual. For the learning process, 500 individuals evolved through 50 generations. The learned solution performed slightly better than a “pseudo-optimal” strategy derived from a linearization of the problem. Another example is the truck backer upper problem where the goal is to steer a tractor-trailer truck so as to back it up to a loading dock. For robustness, 8 initial conditions have been tested and 1000 individuals evolved through 50 generations. After 25 generations, GP derives a solution for the eight cases. It is worth noting that for the examples, there are no known mathematically exact solutions.
In [114], the authors propose the first implementation of online GP for real robot control. A compiled GP system is employed, i.e., a binary code is generated without any interpreting steps. The genetic operators are then directly applied to the binary code. Such a method improved the performance by a factor of 2000 compared to an interpreted-language method. GP is able to learn an efficient control law with noise input and real-time constraints for obstacle avoidance. Interestingly, the performance of the control is retained for different environments.
In [80], the author presents 77 human-competitive results produced by GP. We note in particular the synthesis of a time-optimal robot controllerand the design of an analog electrical circuit that implements a near-optimal strategy [81]. 72 test cases were evaluated for each individual. A population of 40000 individuals evolves through 70 generations for the robot controllerand a population of 640000 individuals evolves through 31 generations for the electrical circuit. Among the human-competitive results related to control, we note the reproduction of patented results such as the PID controller (1939 U.S. patent 2,175,985 by Albert Callender and Allan Stevenson), the PID-D2 (second derivative)typeof controller(1942 U.S. patent 2,282,726 by Harry Jones) and also the production of innovative results that have been patented. We refer to the review paper for more information on those successes.
Recently, [42] developed a new GP method (complete binary genetic programming, CBGP)to synthesize an optimal controlfor a group of robots. The advantage of CBGP is that all functions are represented by a complete binary tree of the same depth. Moreover, the arguments and functions with one and two arguments are at specific positions in the tree. The definition of genetic operators becomes then easier to define. We refer to [41] for a review of machine learning techniques, especially GP methods, employed for control synthesis, optimal control, and model identification
Regarding multi-objective GP algorithms for control, they have been employed in autonomous controllerdesign for UAVs[6, 115], crowd control [63], and performance optimization of home energy-management systems [147].
Fluid mechanics presents a particular challenge to control as unsteady flows include complex, nonlinear, and multi-scale dynamics that are not easy to address even for popular machine learning algorithms [18]. Moreover, contrary to other engineering fields, fluids often require expensive tests. Experiments are often difficult to repeat and automate and have generally a limited evaluation budget to reduce the impact of drifts. Numerical simulations need also supercomputers to do parallel computations. In addition, flow properties require long evaluations (often more than 10 times the natural period of the phenomenon to control) to have converged statistics. The definition of the flow control problem is not straightforward as decisions on the type and number of sensors and actuators, their position, the cost function definition, and flow features are required. There is no general method to define the flow control problem and they often rely on engineering wisdom. Hence, classical control methods have been replaced with model-free machine learning methods such as GP. Recently, Machine Learning Control (MLC) based on GP [45] and LGP [34] has been successful in dozens of flow control tasks in numerical simulations and experiments often exploiting nonlinear mechanisms [113, 123]. Due to the stochastic nature of EML, the learned control laws are rarely reproducible. However, in most of the flow control applications, the same control mechanisms are consistently rediscovered in different runs.
The straightforward implementation of GP in experiments enables an easy interface with complex measurement methods. Thus, in [49], GP is employed to mitigate the flow separation over a backward-facing step in a water experiment with real-time feedback of the velocity field for the control.
GP is also able to learn robust control laws. The vortex-induced vibration of a cylinder has been reduced by 94.2% with GP and the learned control law is robust and efficient for different conditions (Reynolds number ranging from Re=100 to 400) [124]. In order to enforce robustness, [5] and [129] learn control laws in varying conditions for stall suppression with increasing angle of attack and drag reduction of a truck model with varying yaw angle, respectively.
Recently, GP has been benchmarked against reinforcement learningfor the drag reduction problem of cylinder flow. [23] show that for this particular problem, deep reinforcement learningshows higher robustness with respect to the initial conditions and noise, while GP identifies compact and interpretable control laws. Moreover, in this study, like in many others in fluid mechanics, GP acts as a sensor optimizer, i.e., only a few sensors are selected to achieve effective control. On the other hand, in [119], GP achieved the best performance but needed a larger number of evaluations and had the highest learning variance.
LGP and downhill simplex are also combined to exploit the local gradient information in a subspace spanned by the most performing individuals. Better performances than GP and with fewer evaluations (1/5) are achieved for the stabilization of the fluidic pinball in numerical simulations [33] and the open cavity experiment [36] with gradient-enriched Machine Learning Control (gMLC). The learning rate is estimated to be improved by a factor of 10. Contrary to [96] (see Sect.  22.3), an additional reconstruction step is required to build back a matrix representation for the linear-combined control laws. The reconstruction is in essence a function-fitting problem that the authors solve with LGP.
22.4.2 Cartesian Genetic Programming and Grammatical Evolution
Cartesian GP (CGP)employsgraph representation to encode the control laws. The fixed two-dimensional grid of nodes prevents bloating, i.e., the control law does not become larger with each new generation without any performance improvement; Moreover, CGP can represent solutions with an arbitrary number of outputs [58, 107] Applications of CGPto control include the optimization of a controllerto guide a robot out of a maze [58] the optimization of ANN for nonlinear control exemplified on the simple and double pole stabilization problem [98, 99], and the design of continuous-time controllersfor hydraulic turbine power control [71].
In Grammar-based GP or Grammatical Evolution (GE), the control laws are evolved according to a specified grammar. Thus, GE benefits from flexible means of search space restriction, and homologous operators [102] which are valuable to encode control constraints and reduce the complexity of the control laws. In [22], the authors highlight the flexibilityof GE by being able to learn controllersin different paradigms (behavioral or connectionist), different structures (rule sets, machine code, or complex polynomials), and different coordination scales (within a single robot or multiple robots). We refer to their publication for references on each application. [136] employs a grammar to define a nonrandom mutation operator for the gait control of a snake-like robot. The authors achieved better results than classical GP for unconstrained, constrained environments and with partially damaged snakes. Interestingly, when trained in an environment with obstacles, the snake robot learns solutions adopted by real snakes such as bodyelevation or compact side winding. In [21, 22], the authors reformulate the control problem as a vector-valued function to be estimated with GE for autonomous robot control. [143] employ GE for designing a control program for a vehicle robot.
Table  22.1summarizes the key successes of GP for flow control tasks. Note that in most of those applications, convergence has been reached in approximately half of the total number of evaluations.
Table 22.1
Summary of recent control problems in flow control solved with GP
22.4.3 Artificial Immune Systems and LCS
Followingthe words of [137]: “The immune system is highly distributed, highly adaptive, self-organizing in nature, maintains a memory of past encounters and has the ability to continually learn about new encounters”. Hence, immune systems have inspired methods based on Artificial Immune Systems (AIS) to solve different types of problems. In [137], the authors report applications of AIS to the field of robotics, e.g., the control of large populations of robots to generate self-organization [85, 108], and the design of an autonomous robot—the immunoid—that mimics the immune system for displacement and collecting tasks and where the antibodies act as potential behaviors [78, 141]. [9] proposes an adaptive control methodology based on the immune network, the resulting methodology is close to Q-learning. [76] tune a PID controllerfor nonlinear processes with an immune network algorithm based on fuzzy sets. [72] take inspiration from the immune system to build increasing levels of intelligent control (robust feedback control, adaptive control, optimal control, planning control) for autonomous aircraft control problems. More recently, AIS has been employed for the coordination and control of a wheeled mobile manipulator [38].
AIS is part of rule-based machine learning alongside Learning Classifier Systems (LCS). LCS are methods that combine discovery, usually based on genetic algorithm, and learning, based on supervised learning, or reinforcement learning[138]. The main idea is to break down the complex solution space into simpler smaller parts. Such an approach is well-suited to derive adaptive controllers. Thus, [65] introduces a variant of LCS, Temporal Classifier System (TCS), to learn a real robot controller. [20] develop a distributed adaptive control for road traffic junction signals with LCS. [125] employs LCSto improve the design of Organic Computing, a field aiming to design and control complex systems that are able to adapt. [74] employs a variant of LCS, eXtended Classifier System (XCS), to evolve a set of control rules to control networks that model real-world systems. [133] employ XCS for self-adaptive and self-organizing agents; compared to (deep) reinforcement learningthe advantages of XCS are the interpretabilityand continual evolutionof knowledge.
22.5 More Control Problems22.5.1 Model Identification for Control
EML isalso employed to build models of the plant and subsequently apply methods from classical control theory. One of the first studies on this approach is [82], where the authors use GA to identify both discrete and continuous-time systems. The poles and zeros are then estimated with GA and used to build an adaptive controller. Note that the control design method performs well even in the presence of unmodeled dynamics. Nonetheless, the authors report a huge computational load and an increase of the CPU time by a factor of 50 compared to a classical model identificationtechnique. [51] employ GP for the modeling of a laboratory scale process involving a coupled water tank system and for the identification of a helicopter rotor speed controllerand engine from flight test data.
Recently, [109] use a multi-objective GA to estimate the parameters of a nonlinear dynamic model for a hydraulic robot manipulator. The authors show in particular that the multi-objective GA achieves better performance than single-objective GA. [1] and [52] employ a GA and a multi-objective GA, respectively, to tune the parameters of a fuzzy modeling method and thus identify a nonlinear system. In [40], multi-objective GA is employed to optimize the parameters of an automatic control system (a Simulink model). Multi-objective GP is employed in combination with a NARMAX method to build a model for a chaotic system [56].
The development ofcontrol-oriented digital twins that mirror the plant is key to accelerating the learning of controllers. Expensive evaluations can be replaced by fast and cheap simulations. Moreover, digital twinscan be learned during the controller optimization process by leveraging the data generated by the evaluation of each individual. Lately, evolutionary methods have also been employed to build digital twins. In [91], the authors developed Evolutionary Digital Twin(EDT) to address the lack of flexibilityand adaptability in traditional methods. [55] propose a GA approach to building digital twinsfor photovoltaic power simulation. [93] give an example of trajectory optimizationwith GA based on a digital twin
22.5.2 Control Stability and Robustness
Following [44], EML controllerscan be classified into two categories: pure controllers and hybrid controllers. Hybrid controllers can benefit from stability and robustness results from control theory. Thus, [30] propose a multi-objective evolutionary algorithm approach combined with loop-shaping design procedure for robust design of control systems. On the other hand, the stability of pure controllersneeds a case-by-case analysis usually from scratch. Moreover, as EML solutions are rarely identically reproducible, the stability and robustness analysis of the controllersneed to be repeated for each new run.
For linear systems, the stability depends on the value of the transfer function’s poles. The poles need to reside in the stability region of the complex plane, i.e., in the open left plane for continuous time and inside the unit circle for discrete times. Such analysis can be automated for most EML solutions. For control laws learned with GP, a simplification step is generally required. However, the simplification is not straightforward as some operators are protected (, , etc.), i.e., their definition is extended to all real numbers. The protection allows to close the space of controllersbut calls for special treatment for singularity points. For nonlinear systems, stability is studied through the Input-To-State Stability (ISS)paradigm, and the EML solutions need to be integrated into the control system.
The robustness of EML solutions to out-of-design conditions and uncertainties still need to be addressed in future work. A study on the robustness of evolutionary fuzzy control systems is given in [103]. [50] discuss the robustness of GA-tuned controllerswhen applied in the real world. Here are a few rules of thumb for robustness from past experience with flow control experiments. First, if the control mechanisms rely on large-scale dynamics, a controller learned in one condition is likely to perform well for a range of conditions. Second, learning control laws based on features that scale with the operating conditions may be more relevant. This is for example the case in [49] where the Strouhal number, i.e., the non-dimensionalized frequency, is chosen instead of the frequency in Hertz unrelated to the velocity change. Third, robustness to different operating conditions can be included in the learning process. For instance, each individual can be evaluated in all operating conditions sequentially or with a transient [5, 129]. The downside is, of course, an increased evaluation time for each individual. [57] present a method for handling uncertainties in the Covariance Matrix Adaptation Evolution Strategy (CMA-ES)for the reduction of thermoacoustic instabilities of gas turbine combustors. Due to the stochastic nature of the combustion process, the operating conditions are modified and the uncertainties are introduced in the cost function. The method allows online optimization of robust feedback controllersand achieves good performances even under highly unsteady operating conditions, The algorithm requires only a few additional evaluations per generation and is well-suited for online optimization. Robustness to hardware faults needs also to be addressed as the control may significantly vary if an actuator or sensor is damaged. An example of fault tolerant control based on CGPis proposed by [60].
