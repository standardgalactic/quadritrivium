33.
Mei, Y., Chen, Q., Lensen, A., Xue, B., Zhang, M.: Explainable artificial intelligence by genetic programming: a survey. IEEE Trans. Evol. Comput. 1–1 (2022)
34.
Molnar, C.: Interpretable Machine Learning: A Guide for Making Black Box Models Explainable (2022). leanpub.com
35.
Mothilal, R.K., Sharma, A., Tan, C.: Explaining machine learning classifiers through diverse counterfactual examples. In: ACM Conference on Fairness, Accountability, and Transparency (Jan 2020)
36.
Muharram, M., Smith, G.D.: Evolutionary constructive induction. IEEE Trans. Knowl. Data Eng. 17(11), 1518–1528 (2005). NovemberCrossref
37.
Murdoch, W.J., Singh, C., Kumbier, K., Abbasi-Asl, R., Yu, B.: Interpretable machine learning: definitions, methods, and applications. Proc. Natl. Acad. Sci. 116(44), 22071–22080 (2019)
38.
Nguyen, B.H., Xue, B., Zhang, M.: A survey on swarm intelligence approaches to feature selection in data mining. Swarm Evol. Comput. 54, 100663 (2020)
39.
Pearson, K.: LIII. On lines and planes of closest fit to systems of points in space. Lond. Edinb. Dublin Philos. Mag. J. Sci. 2(11), 559–572 (1901)
40.
Ribeiro, M.T., Singh, S., Guestrin, C.: “Why should I trust you?”: Explaining the predictions of any classifier. In: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1135–1144 (2016)
41.
Rudin, C.: Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nat. Mach. Intell. 1, 206–215 (2019)Crossref
42.
Saeed, W., Omlin, C.: Explainable AI (XAI): a systematic meta-survey of current challenges and future opportunities. Knowl. Based Syst. 263, 110273 (2023). MarchCrossref
43.
Samek, W., Montavon, G., Lapuschkin, S., Anders, C.J., Müller, K.-R.: Explaining deep neural networks and beyond: a review of methods and applications. Proc. IEEE 109(3), 247–278 (2021)Crossref
44.
Sayed, S., Nassef, M., Badr, A., Farag, I.: A Nested Genetic Algorithm for feature selection in high-dimensional cancer Microarray datasets. Expert Syst. Appl. 121, 233–243 (2019). MayCrossref
45.
Schleich, M., Geng, Z., Zhang, Y., Suciu, D.: GeCo: quality counterfactual explanations in real time. Proc. VLDB Endow. 14(9), 1681–1693 (2021). MayCrossref
46.
Schofield, F., Lensen, A.: Using genetic programming to find functional mappings for UMAP embeddings. In: 2021 IEEE Congress on Evolutionary Computation (CEC), pp. 704–711 (June 2021)
47.
Seyyed-Kalantari, L., Zhang, H., McDermott, M.B.A., Chen, I.Y., Ghassemi, M.: Underdiagnosis bias of artificial intelligence algorithms applied to chest radiographs in under-served patient populations. Nat. Med. 27(12), 2176–2182 (2021)
48.
Sha, C., Cuperlovic-Culf, M., Ting, H.: SMILE: systems metabolomics using interpretable learning and evolution. BMC Bioinform. 22, 284 (2021)Crossref
49.
Sha, Z., Hu, T., Chen, Y.: Feature selection for polygenic risk scores using genetic algorithm and network science. In: 2021 IEEE Congress on Evolutionary Computation (CEC), pp. 802–808 (June 2021)
50.
Sharma, S., Henderson, J., Ghosh, J.: CERTIFAI: a common framework to provide explanations and analyse the fairness and robustness of black-box models. In: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, pp. 166–172, New York NY USA, Feb 2020. ACM (2020)
51.
Su, J., Vargas, D.V., Sakurai, K.: One pixel attack for fooling deep neural networks. IEEE Trans. Evol. Comput. 23(5), 828–841 (2019)
52.
Tamam, S.V., Lapid, R., Sipper, M.: Foiling explanations in deep neural networks (2022). CoRR, arXiv:​2211.​14860
53.
Uriot, T., Virgolin, M., Alderliesten, T., Bosman, P.A.N.: On genetic programming representations and fitness functions for interpretable dimensionality reduction. In: Proceedings of the Genetic and Evolutionary Computation Conference, GECCO’22, pp. 458–466, New York, NY, USA, July 2022. Association for Computing Machinery (2022)
54.
van der Maaten, L., Hinton, G.: Visualizing Data using t-SNE. J. Mach. Learn. Res. 9(86), 2579–2605 (2008)zbMATH
55.
Virgolin, M., Alderliesten, T., Bosman, P.A.N.: On explaining machine learning models by evolving crucial and compact features. Swarm Evol. Comput. 53, 100640 (2020)
56.
Wachter, S., Mittelstadt, B., Russell, C.: Counterfactual explanations without opening the black box: automated decisions and the GDPR. Harv. JL & Tech. 31, 841 (2017)
57.
Wang, B., Pei, W., Xue, B., Zhang, M.: A multi-objective genetic algorithm to evolving local interpretable model-agnostic explanations for deep neural networks in image classification. IEEE Trans. Evol. Comput. 1–1 (2022)
58.
Xue, B., Zhang, M., Browne, W.N., Yao, X.: A survey on evolutionary computation approaches to feature selection. IEEE Trans. Evol. Comput. 20(4), 606–626 (2016). AugustCrossref
59.
Xue, Yu., Tang, Y., Xin, X., Liang, J., Neri, F.: Multi-objective feature selection with missing data in classification. IEEE Trans. Emerg. Top. Comput. Intell. 6(2), 355–364 (2022). AprilCrossref
60.
Xue, Y., Xue, B., Zhang, M.: Self-adaptive particle swarm optimization for large-scale feature selection in classification. ACM Trans. Knowl. Discov. Data 13(5), 50:1–50:27 (2019)©  The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.  2024
W. Banzhaf et al.(eds.)Handbook of Evolutionary Machine LearningGenetic and Evolutionary Computationhttps://doi.org/10.1007/978-981-99-3814-8_17
17.  Evolutionary Algorithms for Fair Machine Learning
Alex  Freitas1and James  Brookhouse1
(1)
School of Computing, University of Kent, Canterbury, UK
Alex  Freitas
Email: A.A.Freitas@kent.ac.uk
Abstract
At present, supervised machine learning algorithms are ubiquitously used to learn predictive models that have a major impact on people’s lives. However, the vast majority of such algorithms were developed to optimise predictive accuracyonly, ignoring the issue of fairnessin the predictions of the learned models. This often leads to unfair predictive models, since real-world data usually contains biasor prejudices against certain groups of individuals (e.g. some gender or race). Hence, an increasingly important research area involves fairness-aware machine learning algorithms, i.e. algorithms that optimise both the predictive accuracyand the fairnessof their learned predictive models, from a multi-objective optimisation perspective. In this chapter, we review fairness-aware Evolutionary Algorithms (EAs) for supervised machine learning. We first briefly provide some background concepts on fairness measuresand multi-objective optimisation approaches. Then, we review six EAs for fairness-aware machine learning, which are in general based on multi-objective optimisation principles. The reviewed EAs address a variety of supervised machine learning tasks, namely: three EAs address a data pre-processing task for classification (one addressing feature constructionand two addressing feature selection); one EA optimises the hyper-parameters of a base classification algorithm; one EA evolves an ensemble of artificial neural network models; and one EA finds fair counterfactuals. We conclude with a summary of the main findings of this review and some suggested future research directions.
17.1 Introduction
At present, many important decisions affecting people’s lives are made by automated processes. Examples are the decisions about whether or not someone is hired for a job, or whether or not their application for a loan or mortgage is approved. The automated processes that make such decisions, or at least recommend such decisions (which could be later approved by humans) are often based on supervised machine learning methods [41]. We focus here on classification, a major type of supervised learning task where the goal is to learn, from data, a classification model that predicts the class of an object (e.g. an individual) based on the values of a set of features (attributes or variables) describing the characteristics of each object.
Many types of supervised learning methods have been developed over decades of research [14, 41, 44], and there are now many methods that can achieve high predictive performancein general. However, the vast majority of such methods was developed to maximise predictive performanceonly, ignoring the issue of fairnessin their predictions. Hence, when learning from data about people, they often learn unfair classification models, in the sense that the learned models often reflect biases or prejudices in the data [1, 31, 35]. In addition, algorithms can also introduce biases or prejudices into the learned model, e.g. when choosing the wrong loss function. Some precise definitions of fairness will be given in the next Section, discussing fairness measures, but for now, by unfair predictions, we informally mean the case where a desirable outcome or class (e.g. a loan approval) is predicted more often for one group of people than another, due to an unfair discrimination against the unfavoured group—e.g. discrimination against females or non-white individuals.
Hence, in recent years, there has been an increasing interest in developing classification methods that learn fair predictive models from data. In this chapter, we focus on Evolutionary Algorithms (EAs) for fair classification, which is a new and promising research area. The motivation for developing such EAs for fair classification is twofold. First, EAs are robust search and optimisation methods that are less likely to get trapped into local optima than (often greedy) local search methods [15, 37]. Second, the population-based nature of EAs facilitates multi-objective optimisation [11, 38]. Multi-objective optimisation problems arise naturally in the area of fair classification, since in general we would like to optimise at least two objectives: the predictive accuracyand the fairnessof the learned classification models. Indeed, the EAs for fair classification discussed in this chapter perform, in general, multi-objective optimisation, as will be discussed later.
The remainder of this chapter is divided into four sections. The next two sections discuss some background on fairnessmeasuresand multi-objective optimisationapproaches. These sections are followed by a section that briefly reviews six EAs for fair classification, in six separate subsections. One of these subsections will be substantially longer than the others because it will also report computational results comparing two versions of a GA for fair feature selection. Finally, the last section presents a general discussion and conclusions.
17.2 Background on Fairness Measures
Inthe context of the well-known machine learning classification task, what do fairness and biasmean? The classification task involves assigning an individual a class label using its predictive features. However, some of the predictive features could be considered sensitive (or protected) features—e.g. gender, race and age. These sensitive features can be used to partition individuals (instances in a dataset) into two groups: the protected group contains the individuals who are considered to be subject to unfair biasand are more likely to obtain a negative outcome (class label)—e.g. being denied a loan, whilst the unprotected group contains the individuals who are seen to be in a privileged position and are more likely to obtain a positive class label.
Fairnessis not a single simple concept, and the literature contains a large number of measures that capture some notion of fairness [31, 40]. We can categorise these fairness metrics into two main types: group-level and individual-level measures of fairness.
An example of a group-level fairnessmetrics is thediscrimination score [6], which measures the difference between the prediction rates of the positive class across the protected and unprotected groups. Group-level measures can also capture differences in the different types of errors in a classification model, such as the differences in the false positive or false negative error rates across the protected and unprotected groups [7].
One limitation of group-level fairnessmeasuresis that they do not consider fairness at the individual level. That is, two very similar individuals within the same group might unfairly receive different outcomes (class labels), but a group-level fairnessmetric would not pick on this behaviour.
Individual-level fairnessmetrics avoid this limitation, as they concentrate on the similarities between individuals and their corresponding class labels. Individual-level metrics often consider all non-sensitive features which are often ignored by the group-level metrics.
One well-known individual-level fairnessmetric is consistency. Consistency uses a k-nearest neighbours approach to fairness, where an individual is compared to the neighbours, and if they are all assigned the same class the test is considered maximally satisfied. This test can then be performed for all individuals and an average is taken [43]. However, one disadvantage of considering all the non-sensitive features is that the neighbourhood becomes increasingly meaningless as the distances between individuals will tend to increase as the number of features increases. This leads to the comparisons of increasingly different individuals.
In reality, it is not possible to find a single measure of fairnessthat can be declared always the best. It has also been shown that some fairness measurescannot be simultaneously optimised and some sort of trade-off is required [7, 23, 30, 32].
We have mentioned a number of fairness measuresin the last few paragraphs and will define them more precisely below, but first, we need to define some nomenclature:
S: Protected/sensitive feature (attribute): unprotected group, protected group.
: the predicted class; Y: the actual class; both and Ycan take class labels 1 (positive outcome) or 0 (negative outcome).
TP, FP, TN, FN: Number of true positives, false positives, true negatives and false negatives, respectively.
The first fairness measurewe mentioned was the discrimination score(DS) [6], sometimes referred to as statistical parity, which is defined as
(17.1)
The discrimination scoreis a group-level fairnessmeasurethat takes the optimal value of 0 if both protected and unprotected groups have an equal probability of being assigned to the positive class by the classifier.
The next two measures are related in that they are group-level measures that concentrate on opposite types of misclassification errors. The first is the False Positive Error Rate Balance Score (FPERBS) [7, 8]:
(17.2)
FPERBS measures the absolute value (ignoring the sign) of the difference in the probability that a truly negative instance is incorrectly assigned to the positive class between the protected and unprotected groups.
Analogously, the False Negative Error Rate Balance Score (FNERBS) [7, 18, 25] measures the absolute value of the difference in the probability that a truly positive instance is incorrectly assigned to the negative class between protected and unprotected groups. This score is calculated as
(17.3)
Note that a score of 0 indicates an optimally fair result for both FPERBS and FNERBS.
Finally, we will look at an individual-level measure, consistency [43], which is defined as
(17.4)
Consistency is an individual-level similaritymetric that compares the predicted class of each instance in the dataset to the class prediction of that individual’s k-nearest neighbours in the dataset. If all of an individual’s kneighbours have the same predicted class, then that individual is considered maximally consistent. We then repeat this for all Nindividuals in the dataset. A completely consistent model would have a consistency of 1 and a maximally inconsistent model would have a value of 0. As mentioned earlier, one downside of this measure is the validityof an individual’s neighbours being similar to it—i.e. as the number of features increases the dataset becomes sparser in terms of the search area that contains individuals, which breaks down the concept of neighbourhood. In addition, the consistency measure is sensitive to variations in feature scaling, value ranges and distance metrics.
Note that, for group-based fairness measuresin general, when the dataset has multiple sensitive features, trying to optimise a fairness measurewith respect to the values of each sensitive feature separately can lead to the problem of ‘fairness gerrymandering’ [20]. In essence, this problem occurs when, although the model is fair with respect to the values of one sensitive feature (specifying coarse-grained groups of individuals), the model is unfair with respect to a logical conjunction of the values of multiple sensitive features (specifying finer-grained subgroups of individuals). For example, in a study evaluating commercial face-recognition systems, it was observed that the logical conjunction of gender and skin-type features led to a much more unfair distribution of error rates of facial recognition than the distribution for each of those two features separately [4].
It should also be noted that there is no guarantee that maximising any of the above-described fairness measureswould lead to really fair predictions from a user’s perspective, since machine learning algorithms usually ignore political philosophy aspects of fairness [2]—which are out of the scope of this book chapter.
17.3 Background on Multi-objective Optimisation Approaches
In general, when analysing data about people or otherwise sensitive data, ideally a classification (supervised learning) algorithm should maximise both predictive accuracyand fairness, so this naturally leads to a multi-objective optimisation problem. Hence, not surprisingly, EAs for fair classification usually cope with multi-objective optimisationin some way. Therefore, before discussing such EAs, let us briefly review here the topic of multi-objective optimisation.
In general, methods for coping with multi-objective optimisation problems can be divided into three broad approaches [16]. The first and simplest approach is to convert a multi-objective problem into a single-objective one by using some kind of weighted-sum formula, where each objective is assigned a (user-specified) weight. In this case any standard, single-objective optimisation method can be used to optimise the weighted sum. This approach has the drawback that the weights are usually arbitrary and ad hoc. In practice, we can run the optimisation method many times with different assignments of weights to the objectives and compare the results, but this is time-consuming, tedious and inefficient, since each run using a specific weight assignment will ignore the results of other runs with different weight assignments.
The other two approaches, Pareto optimisationand lexicographic optimisation, avoid the drawback of having to specify ad hoc numerical weights for the objectives, as follows.
