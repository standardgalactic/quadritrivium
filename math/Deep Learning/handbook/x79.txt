0.00
0.26
0.01
0.35
BIM-100
0.09
21.76
0.00
0.12
0.00
0.23
PGD-50-10
0.08
18.10
0.00
0.11
0.00
0.21
Under the single-step FGM attack, the DENSER model remains the most susceptible, while the NSGA-Net models from the micro search space remain the most robust. Nevertheless, the strongest attacks in our analysis (BIM-100 and PGD-50-10) still bring the accuracy of most models to below 1% and, in the case of NSGA-M and NSGA-mB, to zero. Surprisingly, especially given the observations with distortions, this does not hold true for DENSER, whose accuracy just drops to around 20% under this threatmodel.
Overall, we consider this to be a promising result and a possible indicator that more robust models may be found in the search space used by DENSER. Moreover, the discrepancies in the relative robustness of the models between the two distance metrics demand further analysis. We are especially interested in understanding what characteristics of the DENSER model make it more-robust and why those do not seem to help the model against attacks. This is the focus of the work we are currently developing.
A comparison between the three NSGA-Net models from the micro search space also reveals that NSGA-mB is marginally less robust than the other two, even though it is more complex (i.e., it has a higher number of parameters) than NSGA-mA. This is observed under both distance metrics, but the susceptibility of NSGA-mB is particularly higher than that of NSGA-mA and NSGA-mC under the BIM-10 attack with perturbations. A distinguishing factor between the three networks is that, unlike NSGA-mA and NSGA-mC, NSGA-mB does not use Squeeze-and-Excitation blocks  [26]. Thus, it seems that Squeeze-and-Excitation may help improve the robustness of an ANN, a hypothesis worth investigating in future work.
15.4.4 Impact of Data Pre-processing
InSect.  15.4.2.1, we described the different pre-processing steps applied to the images before they reach the first layer of each model. Contrary to WRN-28-10 and DENSER, the pre-processing for the NSGA-Net models changes the scale of the data (i.e., the difference between the maximum and minimum values of a feature is larger than 1). Therefore, when the attacks operate in as in the scenario considered thus far, the NSGA-Net models perceive the adversarial perturbations as approximately 4 times larger than the perturbation budget of the threat model
To show this effect, we considered a scenario where the perturbation budget remains the same but refers to the distance in the space in which the first layer of a network expects the data to be. This is accomplished by crafting the adversarial examplesafter all the pre-processing steps are applied to the data, instead of performing the attacks in the range. We are aware that it is misleading to claim that the perturbation budget is the same, but our goal is to show that failing to specify this simple detail may jeopardize the validityof the conclusions taken from this type of study. This issue becomes even more relevant when conducting a study with pre-trained models from different sources, since there is a high chance the pre-processing procedures are not consistent across them.
The results of this experiment are shown in Table  15.3for both the and the attacks. The baseline model is excluded from this analysis since it only requires the data to be in , without centering or standardizing it.
Table 15.3
Accuracy on the CIFAR-10test set when all model-specific pre-processing is applied to the original inputs before performing the attacks. The highest reported accuracy for each attack is in bold
DENSER (%)
NSGA-M (%)
NSGA-mA (%)
NSGA-mB (%)
NSGA-mC (%)
FGSM
16.37
46.65
60.61
61.57
63.64
FGSM-10
6.16
40.82
58.41
58.88
60.97
BIM-10
0.00
2.70
12.22
8.36
12.70
BIM-50
0.00
0.81
6.87
3.86
6.96
FGM
44.75
67.96
78.11
77.70
80.12
BIM-10
30.77
25.98
48.60
44.80
49.61
BIM-50
24.13
17.57
41.63
37.43
42.04
BIM-100
21.76
16.86
40.72
36.46
41.09
PGD-50-10
18.10
15.99
40.10
35.38
40.05
As expected, for the DENSER model there is no significant difference between these results and those from Table  15.2. In contrast, the robustness of the NSGA-Net models appears to be much higher now, especially in the case of -bounded perturbations.
On the one hand, this experiment shows that the choice of data pre-processing may have a negative impact on adversarial robustness. Therefore, this decision should not be overlooked while designing networks under scenarios where adversarial examplesmay be a concern. In the particular case of NE approaches, one might even consider including this design choice in the evolutionary process, even though there is not always a straightforward way to do so (e.g., NSGA-Net). On the other hand, this experiment highlights the importance of clearly specifying the conditions under which the attacks were generated and the target models were trained (including any model-specific pre-processing step) to avoid overestimating robustness when performing these evaluations.
15.4.5 Impact of Discretization Post-processing
Wealso evaluated the impact of converting the pixel values of the perturbed images back to integers between 0 and 255. We just considered the case in which the attacks operated in the range and any additional pre-processing was applied after the perturbations were added to the images. Therefore, we multiplied each pixel value by 255 and rounded to the nearest integer (nearest even integer for values exactly halfway between two integers). We then re-applied all pre-processing steps required by the model and reported the accuracy on the post-processed examples. Table  15.4shows the results for the attacks under both distance metrics.
For the attacks, differences are mainly detected when random restarts are incorporated (FGSM-10). The attack success rate slightly deteriorates, but the differences are of less than 0.25% and seem negligible. As far as the attacks are concerned, the largest differences occur with FGM but also seem negligible (always of less than 0.25%). The success of the attacks is mostly affected by this post-processing procedure when their target is an NSGA-Net model from the NASNet search space.
As pointed out in Carlini et al. [9], this scenario is particularly relevant when dealing with attacks that can produce considerably smaller perturbations, as may be the case with attacks that find the minimal perturbations that fool a model.
Table 15.4
CIFAR-10test set accuracy when the attacks operate in , but the generated images are post-processed. The highest reported accuracy for each attack is in bold
WRN-28-10 (%)
DENSER (%)
NSGA-M (%)
NSGA-mA (%)
NSGA-mB (%)
NSGA-mC (%)
