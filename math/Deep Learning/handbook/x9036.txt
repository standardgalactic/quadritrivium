Fig. 26.4
Installation view of Edenat the Australian Centre for the Moving Image in 2004
As a software system Edenis an evolutionary simulation that operates over a cellular lattice, inhabited by agents (‘creatures’) who have, among other capabilities, the ability to make and hear sound.4Creatures use an internal, evolving classifier system to control their behaviour in the world. The virtual environment that the creatures inhabit develops in response to the presence and movement of people experiencing the system as an artwork. The work is conceptualised and designed as a symbiotic artificial ecosystemwhere virtual species interact with the human audience. It is designed as an experiential environment, whereby viewer’s participation and activity within the physical space have important consequences over the development of the virtual environment.
26.3.1 Related Work and Inspiration
Conceptually, the work was inspired by a period of time I spend in one of Australia’s most remote national parks (Litchfield National Park in the Northern Territory of Australia). Observing such a diverse and exotic variety of flora and fauna in a remote wilderness area over a period of several weeks, I drew inspiration from the collective intelligence of insects and how they fitted into the landscape. So one might think of this work as a contemporary form of landscape painting, but with the significant difference that the medium I am ‘painting’ with is processrather than physical paint on a canvas. The work is not meant to be a visual representation of a real landscape or ecosystem, rather it aims to capture the phenomenological sense of process in natural systems and environments.
Eden draws its technical inspiration from John Holland’s Echo[15], particularly in the use of classifier systems for the internal decision-making system of agents. Others have used evolutionary systems as a basis for musical composition, but in the main for composition simulation[35, 40], rather than as a new form of creative tool for the artist and audience.
The Living Melodiessystem[12] used a genetic programming framework to evolve an ecosystem of musical creatures that communicate using sound. Living Melodiesassumesthat all agents have an innate ‘listening pleasure’ that encourages them to make noise to increase their survival chances. Eden,contains no such inducement, beyond the fact that some sonic communication strategies that creatures discover should offer a survival or mating advantage. This results in the observation that only some instances of evolution in Edenresult in the use of sonic communication, whereas, in Living Melodies, everyinstance does. Living Melodiesrestricts its focus to music composition, whereas Edenis both a sound and visual experience.
26.3.2 Physical Installation
The artwork is exhibited as an installation, around 6  m 6  m, that can be experienced by multiple people simultaneously. It consists of two semi-transparent projection screens arranged in an ‘X’ shape, upon which the virtual environment is projected (via two video projectors). The ambient lighting is minimal—making the screens and the light they reflect and transmit the predominant source of visual interest in the space. A fog machine helps emphasise the three-dimensional nature of the projections, extending the simulated virtual world deeply into the installation space.
Multiple speakers are arranged around the screens to spatialise the sound generated from the simulation according to the source location. So, for example, when a specific creature makes a noise, it will appear to the human audience to emanate from the creature’s current location in physical space.
In addition to this audio-visual infrastructure, an infrared digital video camera is placed above the screens, looking down at the space surrounding the projection screens. This area is illuminated by an infrared lighting system, which is invisible to the human eye and so does not affect the perceptual properties of the work. The camera system identifies and tracks the position and movement of people in the immediate vicinity of the artwork. It is not necessary that the audience has any direct knowledge of this sensing. The purpose of identifying the location and tracking of the work’s human audience is an environmental stimulus for the virtual world, which ultimately contributes to selection pressures that encourage a symbiotic relationship between people experiencing the work and the creatures populating the virtual world.
26.3.3 Technical Implementation
I will now discuss the main technical details of the system, with particular emphasis on the EML mechanisms that facilitate behaviour and learning of virtual creatures. Further details, particularly the payoffand biddingprocesses for rule selection, may be found in  [21, 22].
The environment projected onto the screens is the Eden world, made of a two-dimensional toroidal cellular lattice that develops using a global, discrete time-step model—a popular model based on the theory of cellular automata  [9, 38]. Each cell in the lattice may be populated by one of the following entities:
Rock:inert matter that is impervious to other entities and opaque to sound and light. Rock is placed in cells at initialisation time using a variation of the diffusion limited aggregation(DLA) model [42]. Rocksprovide refuge and contribute to more interesting spatial environmental behaviour of the agents.
Biomass: a food source for creatures. Biomass grows in yearly5cycles based on a simple feedback model, similar to that of Daisyworld[39]. Radiant energy (in ‘infinite’ supply) drives the growth of biomass. The amount of radiant energy falling on a particular cell is dependent on a number of factors, including the local absorption rate of the biomass and global seasonal variation. Probabilistic parameters can be specified at initialisation time to control these rates and variations. The efficiency at which the biomass converts radiant energy into more biomass is also dependent on the presence of people in the real space of the artwork. This dependency is detailed in Sect.  26.3.5
Creatures:mobile agents with an internal, evolvable classifier system, based on XCS [41]. Agents obtain energy by eating biomass or by killing and eating other agents. More than one agent may occupy a single cell.
Each creature consists of a set of sensors, an evolving learning classifier system, and a set of actuators. This configuration is illustrated in Fig.  26.5. Sensors provide measurement of the environment and internal introspection of an individual agent’s status. The XCS relates input messages from the sensors to desired actions. A creature learns through environmental reward, which causes changes in the actions that are performed in response to sensory input. The actuators are used to carry out actions in the world. The success or failure of an intended action will be dependent on the physical constraints in operation at the time and place the intent is instigated.
Fig. 26.5
A section of the Edencellular lattice in visual form (left). To emphasise the lattice structure, grid lines have been layered over the image. The image shows rocks (solid), biomass (outline) and an agent (thick circle). The diagram (right) shows the agent’s internal schematic structure, consisting of a number of sensors, a performance system that evolves and a set of actuators
At initialisation of the world, a number of creatures are seeded into the population. Each agent maintains internal state, which includes the following:
Current age, an integer measured in time steps since birth. Agents live up to 100 years and cannot mate in their first year of life.
Health index:an integer value indicating the overall health of the agent. A value of 100 indicates perfect health; if the health index falls to 0, the agent dies. An agent can lose health via a sustained negative energy leveldifferential (explained next) by bumping into solid objects, such as rocks, or being hit by other agents. In addition, the loss in health from being hit by another agent depends on both its mass and health index.
Energy level:a measure of the amount of energy the agent currently has. Agents gain energy by eating biomass or other agents. Energy is expended attempting to perform actions (regardless of their success); a small quantity of energy is expended even if no action is performed at a given time step. If an agent’s energy level falls to zero, the agent dies and its bodyis converted to new biomass in the cell in which it died.
Mass: an agent’s mass is linearly proportional to its energy level, plus an initial ‘birth mass’ that is normally distributed over the population.
26.3.3.1 Sensors
Sensorsprovide a way for a creature to introspect on itself and its environment  [29]. A creature can use a range of sensor types, but the sensors themselves do not undergo any evolution and are fixed in function, sensitivity and morphology. It is up to an individual creature’s learning system to make use of a sensor, so data from a particular sensor will only be used in the long term if it provides useful information that assists the agent’s survival or mating prospects. Sensor use does not incur any cost to the agent. Sensor information available to an agent consists of:
A simple vision system that detects the ‘colour’ of objects on facing and neighbouring cells (the range is limited to a single cell). Rocks, biomass and agents all have different ‘colours’, which enables an agent to distinguish between them.
A sensor to detect the local cell nutritional value. Cells that contain biomass or dead agents have a high nutritional value, rocks and empty cells do not.
A sound sensor that detects sound pressure levels over a range of frequency bands. Sound can be detected over a much larger spatial range than vision and also with greater fidelity.
An introspection of pain.Pain corresponds to a negative health index differential and would usually indicate attack by another agent or that the agent is bumping into rocks.
An introspection of the current energy level.
26.3.3.2 Actuators
Actuators are used to signal a creature’s intent to carry out an action in the world. The physical laws of the world will determine whether the intended action can be carried out or not. For example, the creature may intend to ‘walk forward one cell’, but if that cell contains a rock, the action will not be possible. Furthermore, all actions cost energy, the amount dependent on the type of action and its context (e.g. attempting to walk into a rock will cost more energy than walking into an empty cell).
As with the sensors, the number and function of actuators are fixed and do not change as the system evolves. Actions will only be used in the long term if they provide benefit. Analysis of actions used by creatures who are successful in surviving shows that not all make use of the full set of actuators.
At each timestep, creatures may perform any of the following actions:
Moveforward in the current direction.
Turnleft or right.
Hitwhatever else is in the cell occupied by the agent. Hitting another agent reduces that agent’s health level using a non-linear combination of the mass, health and energy level of the agent performing the hit. Hitting other objects or being hit will cause pain and a loss of health.
Matewith whatever is currently occupying the current cell. Obviously, this is only useful if another agent is in the same cell. Mating is only possible if the age of both agents is greater than 1 year.
Eatwhatever is occupying the current cell. Creatures can only eat biomass or dead creatures (which turn into biomass shortly after death).
Sing: make a sound that can be heard by other agents.
Performing an action costs energy, so creatures quickly learn not to perform actions without benefit. For example, attempting to eat when your nutritional sensor is not activated has a cost but no benefit. Attempting to move into a rock has a cost greater than moving into an empty cell.
A creature may also choose not to perform any action at a given time step (a ‘do nothing’ action), but even this costs energy, although less than any other action.
26.3.4 XCS Classifier System
Each creature learns via a variant of Wilson’s eXtended Classifier System (XCS) [41]. Classifier systems are rule-based systems that translate input sensory data into actions based on a set of competing rules. Rules have varying degrees of generality, allowing a creature to deal with highly specific or general situations effectively. New rules enter the system when no current rule matches an input stimulus, or via an evolutionary process where existing rules are subject to the standard genetic operators of mutation and crossover. Over time a creature learns how to live in its environment using a combination of the XCS learning system and a simulation of natural selection—only those creatures that have learnt to survive in the virtual world will end up being successful and having offspring.
26.3.4.1 Evolution
An environmental reward system allows rules that have contributed to the agent’s successful survival to be used more often and for learning to occur over an agent’s lifetime.
Evolutionary algorithms generally follow a Darwinian metaphor in that phenotypes pass their genetic information onto offspring, via the processes of crossover and mutation. An implicit or explicit fitness evaluation determines which members of a population survive and hence, which genes (rules) become dominant in the population over many generations.
Rather than adopting this standard evolutionary metaphor, Edenuses a form of Lamarkian evolution[6], where a creature passes on the rules it has learnt during its lifetime to its offspring at birth. Rules are selected proportional to their previous success, so only the most successful rules end up staying in the population over generations.
This design decision was used to allow more rapid adaptation to changing environmental conditions: a necessary feature if the creatures in the artificial ecosystem are to adapt to the behaviour of people experiencing the work in real time. Another way to consider this approach is that parents teach their offspring all the good things they have learnt so far in their lifetime—a kind of social learning—but in Edenthis happens instantly at the moment of birth.
26.3.5 Interaction
The Edensystem has a unique relationship between the physical and virtual components of the system. As shown in Fig.  26.3, an infrared video cameraand machine vision system monitors the space of the installation environment, recognising the presence and movement of people in the space. A video digitisation sub-system performs basic image processing and analysis, such as background removal and feature detection. This data is converted into a stream of vectors indicating the location and movement of individuals in the exhibition area, and used to drive environmental parameters in the virtual simulation.
Edenhas no explicit fitness function. Agents continue to be part of the system based on how well they can survive and mate in the current environment. If certain selection pressures are applied, such as food becoming scarce, only those agents who can adapt and find food will prosper. By driving environmental conditions from the presence and movement of people in the exhibition space, agents must implicitly adapt to an environment that includes aspects of the world outside of the simulation.
To achieve this, two mappingswere used:
Presence in the real environment maps to biomass growth rates. The presence of people around the screen area affects the rate of biomass growth in that local area of the simulated world. Areas with no people correspond to a barren environment: little biomass will grow without the presence of people in the gallery environment.
Movement in the real environment maps to genotype mutation rates. The greater the movement of people in the space, the higher the mutation rate for rule evolution.
These mappingswere based on some assumptions. First, people will generally spend time experiencing something only if it interests them. In the context of experiencing an artwork, people generally may spend a short time evaluating their interest in an artwork, but, after a short time, if it no longer interests them, they will leave. There may be other reasons for leaving, but in general, the duration of stay will have some relation to how ‘interesting’ the experience is.
Agents require food to survive. If people are in the real environment, then food will grow at a more rapid rate. An agent who is making ‘interesting’ noises, for instance, would have a better chance of keeping a person’s attention than one who is not. Moreover, an agent making a progression of sounds, rather than a just a single, repeating sound, is likely to hold a person’s attention even longer. Agents who encourage and hold a person’s attention in the space implicitly give the environment around that agent a more plentiful food supply.
The mappingof people’s movement in the space to mutation rates is based on the assumption that people will move over an area looking for something that interests them and, when they find it, will stay relatively still and observe it. Hence, the movement of people within the real space serves to inject ‘noise’ into the genome of agents who are close to the source of movement. Higher mutation rates result in more variation of rules.6If an agent or group of agents are holding the viewer’s attention, then less rule discovery is needed in the current environment, whereas, if people are continually moving, looking for something ‘interesting’, this will aid in the generation of new rules.
26.3.6 Observations
As mentioned at the beginning of this section, the work has been exhibited extensively, which has allowed the collection of much anecdotal evidence about the system and how people interact with it. Early on I realised that certain factors have a marked effect on people’s behaviour and require specific compensations in the software. For example, when the gallery is closed, there will be no people in the space, which diminishes the food supply for the creatures. Without compensation for gallery opening hours, the entire population dies out each night! The general flow and number of visitors varies according to location and time of day, so the system tries to compensate for this through careful tuning of the system parameters.
Analysis of the rules agents use shows that sound is often used to assist in mating, as would be expected, and with the influence of people, sound is used in other ways as well. Once the environmental pressures from audience behaviour are incorporated into the system, the generation of sound shows a marked increase and analysis of the rules discovered shows that making sound is not only used for mating purposes.
The resulting sounds produced by the system doappear to interest and engage the audience, and combined with the visual experience, Edendoes incite, from visitors, an interest and curiosity in what it is doing. In many cases, people are not aware of the learning system, camera sensing or even the fact that what they are experiencing is a complex virtual ecosystem. This does not necessarily hinder the experience, as there is no correct or incorrect way to behave except to appreciate the experience. Anecdotal accounts from people who have experienced the work describe it as ‘having a sense that it is somehow alive’, or ‘like being in a strange forest at night’. In a number of exhibitions, people returned to the work over a period of several days, to see how the qualitative behaviour of the virtual environment had changed. In one exhibition, a local businessman visited the work during his lunch hour every day for 3 weeks, describing the experience as ‘fascinating...one that made me more sensitive to my own environment’. While these are, of course, subjective evaluations, it does appear that Edenis able to adapt and evolve to create an ongoing interest for its audience.
Having now gone into this detailed example of EML in an artistic project, the next section looks at the current fascination with generative AI and how evolutionary systems might expand the possibilities for generative AI in the arts.
26.4 The Rapid Rise of Generative AI Art
In recent years, generative AI has increasingly become a central concern for art that is created with technology. AI methods have been used for artistic applications at least since the 1960s [7]. For example, one of the most prominent and dedicated artists working with AI was Harold Cohen (1928–2016), who devoted the majority of his professional career to trying to uncover the creative possibilities of AI, primarily using software he referred to as AARON. The AI methods used by Cohen were very simple and primitive by today’s standards, but his skills and experience as a conventional painter played an important role in the software’s development and the works he developed. While Cohen’s work was certainly well known amongst people working in the computer arts, it went largely unrecognised by the broader art world.
Things changed dramatically in October 2018. AI-generated art made headlines around the world when a ‘work of art created by an algorithm’ was sold at auction by Christie’s for US$432,500—more than 40 times the value estimated before auction [8]. The work, titled Portrait of Edmond Belamywas one of ‘a group of portraits of the fictional Belamy family’7created by the Paris-based collective Obvious
The three members of Obvious had backgrounds in Machine Learning, Business and Economics. They had no established or serious history as artists. Their reasoning for producing the works was to create artworks ‘in a very accessible way (portraits framed that look like something you can find in a museum)’ with the expectation of giving ‘a view of what is possible with these algorithms’.[27]
The works’ production involved the use of Generative Adversarial Networks(GANs), a technique developed by Ian Goodfellow and colleagues at the University of Montreal in 2014 [14]. GANs became a technology enthusiastically adopted by a number of artists, heralding a new era of ‘AI Art’. Such was the hyped enthusiasm for ‘GAN Art’ that machine learning researcher, Francois Chollet proposed the term GANism,8indicating that the method’s distinctive aesthetics were significant enough to make ‘GAN Art’ worthy of being compared alongside major movements in the history of art, speculating that it might ‘become a significant modern art trend’.
In her book on AI Art, Joanna Zylinska took a more critical view of the GAN Art ‘movement’:
Through both their art and their discourse on art, Kogan, Tyka, Akten and Klingemann adopt a worryingly uncritical instrumentalism, where seemingly child-like curiosity is underpinned by the progressivist model of technological expansion towards some kind of improvement - of accuracy, data sets and, ultimately, art as we know it. They are thus poster boys for AI art as underwritten by Google in the way they incarnate the very ideas of progress, innovation and upward trajectory that drive the gung-ho progressivism of the AI 2.0 era.
Zylinska picked up on what she referred to as the ‘gen-art boys’ discourse’, arguing that ‘dreamy neural network art of this kind thus ultimately has a pacifying effect, anaesthetising us into the perception of banal sameness that creates an illusion of diversification without being able to account for differences that matter—for why and where they matter, when and to whom. It thus ends up enforcing a mode of existence that philosopher Franco ‘Bifo’ Berardi hascalled a ‘neurototalitarianism’  [43].
Fig. 26.6
The progress of Generative AI. Left: an image generated using a Generative Adversarial Network (GAN) similar to the ‘Portrait of Edmund Belamy’ which sold for US$432,500 in 2018; and Right: an image created in less than 20 seconds using MidJourneysoftware in 2022 with the text prompt ‘19th century portrait of a French nobleman’
With the rise of text-to-image systems such as DALL-E 2, MidJourney and Stable Diffusion, which allow the generation of detailed and complex imagery from short text descriptions, ‘GAN Art’ rapidly faded into technological and creative oblivion, superseded by newer generative AI techniques trained on a vast corpus of human visual culture. These Text-to-Image (TTI) systems allow anyone to write a brief English prompt and have the system respond with a series of images that depict the scene described in the prompt.
As shown in Fig.  26.6, in the space of a few short years, generative AI has progressed from blurry and vague visual depictions defined by numerous technical artefacts to images with a quality that rivals highly competent human artists and illustrators. And as the technology evangelists keep reminding us, ‘AI is currently the worst it will ever be’.
This example is illustrative for several reasons. Firstly, it demonstrates the rapid pace of technological development of generative AI systems—in just a few years the quality and capability of generative AI systems has increased significantly. Secondly, from the perspective of Art, it demonstrates how a reliance exclusively on a technology does not make for a significant creative practice or define an ‘art movement’. The price paid for Portrait of Edmond Belamynow seems even more excessive than it did at the time,9given how outdated the technology has so quickly become.
In the earliest days of using computers for art, artists were also programmers, necessarily so because anyone using a computer had to program itto get it to do anything. As software developed, both technically and culturally, there was a shift to using software, written by others, with software evolving to become more like an artist’s tool rather than something of their own creation. With movements such as generative art, algorithmic art and software art, the emphasis on creative coding as an artistic practice remains high, but much of the current efforts in ‘AI Art’ require the use of software written by others, over which the artist may have minimal understanding or direct control.
It has long been discussed how the tools used shape both the limitations and outcomes of a creative practice. Tools themselves influence how one thinks about making with that tool and therefore, what one makes and howit gets made. A tool as simple as a pencil has enormous and varied expressive power, even though anything drawn with a pencil looks like a ‘pencil drawing’ as a class of image making. With generative AI, there are important differences because the level of direct control is removed and important creative features, such as intent, embodiment and expressive capability are either non-existent or severely diminished. Moreover, generative AI systems are trained on a vast corpus of human art and can only ever generate statistical mixups of prior human art.
So while TTI systems, for example, can generate ‘new’ images (in the sense that the exact specific image has not existed before), they are all statistical amalgamations of pre-existing images, precluding the possibility of stylistic innovation, or any conceptual innovation. In the terminology of Boden [4], they support only combinatorial creativity(the combination of existing elements), not transformational creativity (where elements are transformed into something new). In simple terms, TTI systems could not ‘invent’ Cubism if they were only trained on data prior to 1908, yet this was possible for human artists.
Fig. 26.7
An example of MidJourney’s image-to-prompt AI: At left an original image (an evolved virtual sunflower, a still frame from the evolutionary artwork Turbulenceby the author [20]). Right, the four text prompts The –ar 4:3at the end of each prompt refers to the image aspect ratio
So while many people are now using generative AI to make ‘art’ using TTI software like Stable Diffusion and MidJourney, I would argue that the majority of the creativityin this process rests with the data on which the AI was trained, i.e.  from prior human creativity. MidJourney, for example, allows you to input an image and get an AI to describe it for you as a ‘prompt’ (Fig.  26.7). Notice that the generated text prompt often includes the phrase ‘in the style of’. While some styles are general across a number of individual artists (‘postmodern photomontage’), others are specific to an individual (‘larry carlson’), what I have dubbed style theft, whereyou can request the system generates an image that mimics the style of a specific artist, without that artist’s permission or blessing.
Prompts generated in this can then be fed into MidJourney’s text-to-image system, synthesising images from the machine-generated prompt. Figure 26.8shows an example image generated by each of the four generated text prompts shown in Fig.  26.7(right).
Fig. 26.8
Example images generated by the four text prompts shown in Fig.  26.7
As can be observed, while the images capture something of the original, they are stylistically different10and conceptually unrelated. The complex and somewhat cryptic nature of the generated prompt text makes it difficult to anticipate how changing the prompt will affect the resultant image, so for now, working with prompt-based systems remains a challenge. More importantly, the derivativenature of generative AI challenges human artists to find creatively interesting, non-derivative ways of working with these systems.
26.4.1 Evolution and Generative AI
Evolutionary techniques have been combined with generative AI systems, for example, in the evolution of prompts in text-to-image systems, evolving, for example, according to aesthetic criteria. However the fundamental problem of a machine understanding human subjective criteria remains an open problem [23].
In a summary of the view of aesthetic appreciation from the perspective of neuroimaging, researchers highlighted that the neuroscientific evidence currently suggests that ‘aesthetic appreciation is not a distinct neurobiological process assessing certain objects, but a general system, centred on the mesolimbic reward circuit, for assessing the hedonic valueof any sensory object’ [33]. Another important finding was that hedonic valuesare not solely determined by object properties. They are subject to numerous factors extrinsic to the object itself. Similar claims have come from psychological models [18]. These findings suggest that any algorithmic measure of aesthetics that only considers an object’s visual appearance ignores many other extrinsic factors that humans use to form an aesthetic judgement (including context, prior knowledge and experience, emotional state and affect). Hence, they are unlikely to correlate strongly with specific human judgements.
Nonetheless, what generative image systems (and the internet in general) tell us that there are certain properties of images, for example, that have wide appeal, so it is not that everyone’s taste is so different as to be entirely unique. Some of the reasons for this are well known, including evolutionary adaptations [11, 13, 26]. But in specific creative contexts, such as Western art, where originality and uniqueness are highly valued, generalisations rarely work (if they did it would be easy to formulate and therefore teach ‘creativity’).
Deep learning methods can be used to learn an individual’s style or aesthetic preference [25]; however, such systems require training on large datasets which can be tedious and time-consuming for the artist and still do not do as well as the trained artist’s eye in resolving aesthetic decisions.
26.5 Conclusion
What can evolutionary machine learning contribute to art and creativity?This very much comes down to the individual approach. Technological art is, by definition, a technology-driven approach, but a focus exclusively on technological achievement or efficiency has become increasingly untenable over the last decade when considering the results and their impact on art, culture and environment (at least in the opinion of this author). Technology is increasingly inseparable from broader concerns, including how it affects culture, cultural preservation and diversity; social and environmental impacts. AI has questionable practices regarding the acquisition of training data, the exploitation of cheap labour in developing countries and the dominance of large technology companies focused on profit-driving development.
This does not mean EML cannot be valuable for the arts. The 2023 European STARTS prize for integrating art, science and technology was awarded to Alexandra Daisy Ginsberg from the UK for the work ‘Pollinator Pathmaker’. This work used machine learning methods to design gardens from the perspective of pollinators, allowing people to design and plant gardens that are ‘pollinator friendly’, using art to create agency in this era of ecological crisis. This is just one example of many artworks that are making use of machine learning methods to effect positive change—for both us and our environment – a topic that is becoming increasingly vital globally.
