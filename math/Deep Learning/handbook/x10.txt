78.
Stanley, K.O.: Efficient Evolution of Neural Networks Through Complexification. PhD thesis, Department of Computer Sciences, The University of Texas at Austin (2004)
79.
Stanley, K.O., D’Ambrosio, D.B., Gauci, J.: A hypercube-based encoding for evolving large-scale neural networks. Artif. Life 15, 185–212 (2009)Crossref
80.
Stanley, K.O., Miikkulainen, R.: Evolving neural networks through augmenting topologies. Evolut. Comput. 10, 99–127 (2002)Crossref
81.
Stanley, K.O., Miikkulainen, R.: Competitive coevolution through evolutionary complexification. J. Artif. Intell. Res. 21, 63–100 (2004)Crossref
82.
Such, F.P., Madhavan, V., Conti, E., Lehman, J., Stanley, K.O. and Clune, J.: Deep neuroevolution: genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning (2017). arXiv:​1712.​06567
83.
Suganuma, M., Kobayashi, M., Shirakawa, S., Nagao, T.: Evolution of deep convolutional neural networks using cartesian genetic programming. Evol. Comput. 28, 141–163 (2020)Crossref
84.
Sun, Y., Xue, B., Zhang, M., Yen, G.G.: Evolving deep convolutional neural networks for image classification. IEEE Trans. Evol. Comput. 24, 394–407 (2020)Crossref
85.
Sun, Y., Xue, B., Zhang, M., Yen, G.G., Lv, J.: Automatically designing cnn architectures using the genetic algorithm for image classification. IEEE Trans. Cybern. 50, 3840–3854 (2020)Crossref
86.
Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., Rabinovich, A.: Going deeper with convolutions. In: 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1–9 (2015)
87.
Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z.: Rethinking the inception architecture for computer vision. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2818–2826 (2016)
88.
Urbanowicz, R., Moore, J.: Learning classifier systems: a complete introduction, review, and roadmap. J. Artif. Evolut. Appl. 2009, 736398 (2009)
89.
Urbanowicz, R.J., Bertasius, G. and Moore, J.H.: An extended michigan-style learning classifier system for flexible supervised learning, classification, and data mining. In: International Conference on Parallel Problem Solving from Nature, pp. 211–221. Springer (2014)
90.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I.: Attention is all you need. In:  Guyon, I.,  Von Luxburg, U.,  Bengio, S.,  Wallach, H.,  Fergus, R.,  Vishwanathan, S.,  Garnett, R., (eds.), Advances in Neural Information Processing Systems, vol.  30, pp. 6000–6010 (2017)
91.
Vinyals, O., Toshev, A., Bengio, S. and Erhan, D.: Show and tell: a neural image caption generator. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3156–3164 (2015)
92.
Wu, X., Jia, L., Zhang, X., Chen, L., Liang, Y., Zhou, Y., Wu, C.: Neural architecture search based on cartesian genetic programming coding method (2021). arXiv:​2103.​07173©  The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.  2024
W. Banzhaf et al.(eds.)Handbook of Evolutionary Machine LearningGenetic and Evolutionary Computationhttps://doi.org/10.1007/978-981-99-3814-8_3
3.  EML for Unsupervised Learning
Roberto  Santana1
(1)
University of the Basque Country (UPV/EHU), San Sebastian, Gipuzkoa, Spain
Roberto  Santana
Email: roberto.santana@ehu.eus
Abstract
This chapter introduces the use of Evolutionary Machine Learning (EML) techniques for unsupervised machine learning tasks. First, a brief introduction to the main concepts related to unsupervised Machine Learning (ML) is presented. Then, an overview of the main EML approaches to these tasks is given together with a discussion of the main achievements and current challenges in addressing these tasks. We focus on four commonly found unsupervised learning tasks: Data preparation, Outlier detection, Dimensionality reduction, and Association rule mining. Finally, we present a number of findings from the review. These findings could guide the reader at the time of applying EML techniques to unsupervised ML tasks or when developing new EML approaches.
3.1 Introduction
While supervised Machine Learning (ML) algorithms require human interventionto label the examples that are used for learning, unsupervised ML methods do not need tagged examples as a guidance. These algorithms can be used to group instances that share common patterns, to reduce the problem dimensionality or identify regularities in the data, to detect outliers, and for other multiple tasks that are relevant in real-world applications.
Since the availability of labeled data is scarce in many practical problems, unsupervised ML tasks are frequent in a variety of application domains such as computer vision [33, 61], natural language processing [10, 89], physics [6, 57], and other fields [12, 53, 80]. In many areas, unsupervised learning is important also from a conceptual or theoretical point of view since unsupervised approaches can provide clues about the underlying structure of the problem or reveal the existence of anomalies or unusual behaviors in a system.
As in other areas of ML, evolutionary algorithms have been applied to solve unsupervised ML tasks, and have been also hybridized with classical unsupervised ML algorithms. In this chapter, we discuss the main achievements of Evolutionary Machine Learning (EML) in unsupervised learning tasks. We also present some of the current challenges in EML for this type of problem.
The best known unsupervised learning task is perhaps clustering. EML for clustering is covered in Chap.    5. Therefore, the focus of this chapter is on a set of other four commonly found unsupervised learning tasks: Data preparation, Outlier detection, Dimensionality reduction, and Association rule mining. Certainly, there are other unsupervised ML tasks that can be found in real-world applications. However, we consider that the ones selected are among the most relevant in the field and are indeed the tasks where the application of EML algorithms has produced the biggest impact. When reviewing EML methods for the imputationand dimensionality reductiontasks, we consider their application together with both, supervised and unsupervised algorithms, since in the general case these two tasks do not required any type of supervisory signal for their implementation, and can be considered as unsupervised problems on their own.
The chapter begins with a brief presentation of the unsupervised ML tasks, and some of the main concepts involved. Then, in the central part of the chapter, work on the use of EML for each of the four tasks is reviewed. The paper concludes with a discussion on the main trends and challenges in EML research for unsupervised ML tasks.
3.2 Main Concepts
In this section, we cover some of the main concepts of unsupervised ML tasks.
3.2.1 Data Preparation
Data preparation comprises the steps required before an ML algorithm can be applied to solve the task. These steps may include the application of several techniques that are usually organized in a pipeline. The choice of the particular methods to include in the ML pipeline depends, both, on the characteristics of the ML task to be addressed, and on the requirements of the ML algorithm that will be used to solve the task. Some of the most common tasks in data preparation are instance selection, feature discretization, and data imputation
3.2.1.1 Instance Selection
Instance selection consists of selecting a subset of the original instances while preserving as much as possible the information relevant to the task. Instance selection methods can remove instances considered as redundant and reduce the time required for learning the ML model. A straightforward approach to instance selection consists of associating a binary variable to each instance and interpreting that the instance is selected if the corresponding variable has value 1, and not selected if the value is 0. For real-world problems in which the dataset can be very large, the complexity of the problem significantly increases and efficiency is essential for feasible instance selection algorithms [63].
3.2.1.2 Feature Discretization
Feature discretizationconsists of transforming the original (continuous) features of the problem to discrete ones. This preprocessing step can be necessary for ML algorithms that require discrete inputs. Feature discretizationcan be also applied to problems with a discrete representation when a reduction in the cardinality of the variables can lead to gains in terms of the efficiency for the ML method. Feature discretizationis challenging since interactions among the variables should be taken into account for the discretization process. A rich variety of methods have been proposed for solving this problem [42].
3.2.1.3 Data Imputation
Missing datais a common problem in real-world applications. Most of ML techniques cannot deal with missing values in datasets. One typical strategy is deleting the incomplete instances from the dataset, but this approach is not effective for scenarios where the amount of data is limited. Another approach is data imputationwhich aims to replace the missing observations in the original dataset with a likely value or “educated guess”.
A variety of imputationmethods exist and their effectiveness depends on different factors such as the characteristics of the dataset (e.g., tabular data, spatial data, and time series), the particular type of missing data, and the ML algorithm being applied [26]. For instance, in prediction problems associated with spatial data, the putative imputed values should take into account the localization of the missing measurements. Similarly, in time seriesdatasets, there is usually a temporal dependence between the elements of the data that should be taken into consideration forimputation
There are many challenges in the design of imputationalgorithms, which include the generation of realistic and diverse imputationvalues and the need for computationally efficient methods that can be deployed in real-world scenarios.
3.2.2 Outlier or Anomaly Detection
The outlier detectionor anomaly detection task consists of the identification of instances that are abnormal or depart significantly from the distribution of the majority of instances in the dataset. Outlier detection isimportant because outliers may indicate corrupted or noisy data, or can correspond to anomalous behavior of particular interest for the practitioner.
The outlier detectionproblem is complicated by the maskingand smearing effects [73]. Maskingoccurs when an outlier masks another preventing its detection. Smearing occurs when one outlier makes another, non-outlier observation, to appear as an outlier. Maskingand smearing determine that the approaches that detect outliers one by one can be inaccurate.
Different variants of the anomaly detection task exist, associated with the particular type of data or the domain of application. For example, in time seriesmodels, the change-point detection problem [32] consists of identifying abrupt changes in the time seriesdata. In intrusion detection systems [81], the goal is to detect malicious or anomalous behaviors in the system.
3.2.3 Dimensionality Reduction
Dimensionality reduction [77] consistsof reducing the original number of features to a lower dimensionality while keeping the most relevant information for the problem. This task is critical for problems that involve a large number of features. It can also be part of different ML pipelines, and be used for various purposes such as information visualization, or improving the performance of supervised ML methods. Among the most common approaches to dimensionality reductionare feature selection, feature construction, and manifold learning
3.2.3.1 Feature Selection and Feature Extraction
Feature selectioninvolves the selection of a subset of the original problem features that contain the most relevant information for the tasks addressed and are more suitable for the application of the ML algorithm. The typical approaches to feature selectioncan be divided into three groups: i) Filter methods, which use information measures to evaluate each set of features; ii) Wrapper-based strategies, which use some performance metric of a supervised learning algorithm as the evaluation criterion for the features; and iii) Hybrid approaches, which combine characteristics of the two previous methods.
Feature constructionis the process of deriving new features which condense information of multiple original features.
3.2.3.2 Manifold Learning
The goal of manifold learning (MaL) algorithmsis to create a mappingfrom the original problem representationto a lower embedded representation in such a way that it preserves some meaningful properties of the original dataset, e.g., the local neighborhoods are preserved at every point of the underlying manifold. MaL algorithms propose different nonlinear transformations of the original data. There are many aspects that influence the difficulty of manifold learningfor a particular problem, which include the original dimensionality of the data and their distribution, as well as the number of instances.
3.2.4 Association Rule Mining
Association rules [2, 88] allow to extract patterns that characterize relationships between items in a dataset. An association rule has an if-thencondition form in which the antecedent (ifpart) expresses the condition for the occurrence of the consequent (thenpart). The antecedent is a combination of items found in the dataset, and the consequent is an item that frequently co-occurs with the antecedent. The degree of uncertainty of an association rule is measured using a support, also called coverage, and the confidence of the rule. The support is the number of transactions that include all items in the antecedent and consequent parts of the rule, i.e., how often the rule appears in the dataset. The confidence is the number of instances that the rule predicts correctly, expressed as a proportion of all instances to which it applies.
Association rule mining (ARM) [31] is one of the most used and investigated methods of data mining. One of the reasons for its popularity is that association rules are usually considered interpretable by humans. However, the complexity of mining association rules rapidly increases with the number of instances in the dataset and the number of items. Therefore, many of the developments in the area have been oriented to increase the computational efficiency of these algorithms.
3.3 EML for Data Preparation
There are a number of ways in which EML algorithms can be used for data preparation. In this section, we organize the review of EML applications according to the particular tasks involved in data preparation.
3.3.1 EML for Instance Selection
One of the limitations in the application of evolutionary algorithms (EAs) to instance selection is the computational complexity of the problem as the number of instances grows. Some authors have proposed methods to deal with this question. In [40], a strategy that divides the original dataset using a fuzzy clustering algorithm is proposed. By splitting the dataset into different clusters and applying a genetic algorithm (GA) to each of the clusters, the time required for instance selection reduces significantly. The divide and conquer technique also improves the predictive model performance.
In some scenarios, instance selection can be used as an initial step for data imputation. For example, in [3], a genetic programming (GP) method is proposed that performs instance selection on datasets with incomplete dataand uses the selected instances as the source for imputing missing data
Instance selection has been also tackled as a previous step to regression. In [41], the Non-Dominated Sorting Genetic Algorithm II (NSGA-II)multi-objective EA [18] is used to search for the optimal subset of instances in the training dataset. For this purpose, NSGA-IIis combined with the k-Nearest Neighbors(KNN) algorithm which is used for evaluating the solutions during the instance selection process. The multi-objective approach allows to obtain a pool of solutions with a different balance between the number of selected instances and the accuracy of the regression.
In [11], an analysis of two evolutionary instance selection algorithms is presented. The results of the algorithms are compared against classical instance selection algorithms on two applications. For the selected evaluation problems considered, the EAs outperform the classical algorithms.
In [34], a GA and two other instance selection methods are used to determine whether a combination of instance selection from the observed data andmissing value imputationperforms better than applying the imputationmethod alone. The authors conclude that for most numerical datasets, performing instance selection to filter out some noisy data, can improve the imputation’sresults.
The interested readers can consult [19] for a survey on EML applied to instance selection and generation.
3.3.2 EML for Feature Discretization
An early work on the application of EML methods for multivariate discretizationwas presented by Kwedlo and Kretowski [46], which investigates the discretization problem in the context of decision rule induction. The introduced EA uses six different search operators including crossover and a mutation-like operator. The algorithm is tested on eight datasets, and the results indicate that both classification accuracy and complexity of the discovered rules are comparable with the results of other classifiers.
Kim and Han [38] propose a GA to search the optimal or near-optimal thresholds for feature discretization. The discretized values are passed as inputs of a neural network and used for the prediction of stock price index. The authors conclude that the combination of GA-based discretization and ANN prediction is able to reduce the dimensionality of the feature space, enhancing the generalizability of the predictive model.
In [24], a GA approach is presented for finding a subset of cut-points that defines the best possible discretization scheme of a dataset. The discretization method is validated by using it together with the C4.5and Naive Bayes (NB) classifiers. This work is extended in [67] to investigate feature discretizationin problems where there are high interactions among multiple attributes. The introduced evolutionary multivariate discretizer selects the best combination of boundary cut-points to create discrete intervals. The experimental results show that the algorithm is competitive with other discretization schemes in terms of accuracy and the obtained solutions require a lower number of cut-points than the other discretizers.
Zhu et al. [91] research discretization problems with class-attribute interdependence of the discretization scheme. They analyze information measures of the interdependence between two discrete variables, and propose the use of an improved information distance criterion to evaluate this interdependence. An Artificial Colony Optimization (ACO) algorithm is proposed to detect the optimal discretization scheme, and its evaluation on a real industrial database verifies the effectiveness of the algorithm.
Flores et al. [23] propose the simultaneous discretization of multiple variables using an Estimation of Distribution Algorithm (EDA) [47, 48] approach within a wrapper scheme. EDAs construct a probabilistic model of the best solutions which is then sampled to guide the search. In [23], the solutions sampled by the EDA were evaluated using the Naive Bayes classifier. The algorithm was tested using artificial and real datasets, and the experimental results suggest that the method is effective for feature discretization
Chen et al. [13] introduce an adaptive GA for discretizing high-resolution remote sensingbig data. The GA uses a binary representation and a fuzzy rough model for the definition of the fitness function. The algorithm is compared to other four EML methods and to seven supervised discretization algorithms. The comparison criteria include the running time, search efficiency, interval quality, and classification accuracy. The experimental results show that the new algorithm can effectively solve the problem of feature discretization
3.3.3 EML for Imputation
Al-Helali et al. [4] use a combination of GP and weighted KNN for imputing incomplete observations in symbolic regression. The idea of this hybrid approach is to use both the instance-based similarityof KNN clustering and the feature-based predictability of GP regression. They conduct an extensive evaluation of the algorithm on 10 datasets comparing the results with state-of-the-art algorithms. The results show that the proposed algorithm is significantly better than some of the other imputationmethods.
Garciarena et al. [25, 27] add imputationmethods as a component of ML pipelines generated using the Tree-based Pipeline Optimization Tool (TPOT)[62], a tool for automatic ML. They use GP to evolve pipelines that include the appropriate imputationmethod for a given problem. A different approach to the combination of EAs and imputationmethods is presented in [74], where the authors combine multiple imputationwith a GP approach that learns a classifier for each pattern of missing values. The algorithm is compared to other three imputationmethods on a number of datasets from UCI, showing better results in terms of accuracy and computational time.
Lobato et al. [55] propose a multi-objective GA for data imputationwhich is based on NSGA-II. They evaluate the new algorithm on 30 datasets with induced missing values and show that the algorithm presents a good trade-off between the different evaluation measures investigated.
EML has also been used for imputationproblems defined on spatial data and time series. In [21], a GA was proposed for conditional spatial simulation. The random key genetic algorithm (RKGA) solution encoding was used. The proposed algorithm is applied to impute missing datain optical measurements of gas turbines, and it was much faster than the simulated annealing approach applied to the same problem. In [22], a GA approach was proposed for time seriesimputationand tested on four weather time seriesdatasets. The proposed algorithm was shown to outperform the expectation-maximization (EM) method commonly used for this problem.
While GAs have been the class of EA most extensively applied to data imputation, other types of EAs have been also applied. Krishna and Ravi [44] introduce an imputationalgorithm based on Particle Swarm Optimization (PSO). They use PSO to optimize different fitness functions that evaluate the quality of the imputationprocess. The authors report that the results are competitive to a hybrid data imputationmethod based on the K-means clustering algorithm and on a multi-layer perceptron.
Another question analyzed in the literature is how the quality of the imputationprocess changes as the number of missing dataincreases. In [1], a hybrid approach combining GA and two types of neural networks is proposed and evaluated in the scenario in which the number of missing cases within a single record increases. The accuracy of the method was shown not to reduce significantly.
3.3.4 EML for Dimensionality Reduction
Dimensionality reduction canbe implemented following different strategies; among them are feature selection, feature construction, and manifold learning. In this section we review the applications of EML to these tasks.
3.3.4.1 EML for Feature Selection
Raymer et al. [68] proposea hybrid GA-KNN method to simultaneously solve the problems of feature selection, feature extraction, and classifier training. They evaluated the algorithm on two medical datasets and on the identification of favorable water-binding sites on protein surfaces. The derived GA feature extractor was compared to the sequential floating forward selection algorithm (SFFS) [66] for feature selectionfor each dataset showing competitive results.
Neshatian and Zhang [60] propose a GP-based approach for feature selectionin an image-based face classification problem. To evaluate the quality of the solutions, the fitness functions incorporates a variation of the NB classification algorithm. The authors evaluate their proposal on a face detection dataset and show that according to the performance metrics considered, the classification results improve while the dimensionality of the problem is significantly reduced.
In addition to GA and GP approaches, other EAs have been applied to dimensionality reduction. Yin et al. [85] introduce the immune clonal strategy, a variant of artificial immune systems (AIS), for dimensionality reduction of hyperspectral images. The algorithm selects a subset of bands in the hyperspectral images which are used to solve the classification problem. The hyperspectral band selection problem has been also tackled using a multi-objective immune algorithm in [87]. The authors model the problem considering the objectives of information preservation and redundancy removal. The MOEA approach is tested on three real hyperspectral image datasets and is shown to efficiently explore the solutions with optimal trade-offs between these objectives.
