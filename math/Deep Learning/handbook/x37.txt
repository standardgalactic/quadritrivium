Modification: modifying the encoded information of the unit at the selected position.
It is worth noting that operating the additionand removalmutation strategies can change the length of the individuals. Algorithms  2and 3show the offspring generation and environmental selection process, respectively. Figures  9.7and  9.8give examples of mutation and recombination operations considering one-point crossover and three types of mutation.
Fig. 9.7
Example of the “one-point” crossover operation
Fig. 9.8
Example of the a“Addition”, b“Removal”, and c“Modification” mutation
9.2.2.4 Fitness Evaluation
Generally, fitness is calculated based on the encoding information and the task at hand. For example, in the image classificationtask, an individual’s fitness is the classification accuracy on the corresponding fitness evaluation dataset. For evaluating individual fitness, the individuals in each generation need to be decoded into a corresponding neural network architecture and then trained like a common neural network. In the case of searching CNNfor the image classificationtask, the individual is decoded into a CNN and then added to a classifier to be trained. The decoded CNN is trained on training data, and fitness is the highest classification accuracy on the fitness evaluation data after the CNNtraining.
Algorithm  4details the fitness evaluation of individuals in the population . First, each individual pin population should be decoded as a CNNarchitecture from the encoding information (line 4), which is an inverse of the encoding strategy introduced in the population initialization. Second, the CNNis initialized with weights (line 4) like a hand-crafted CNN. Generally, the weight initialization method is the Xavier initializer. Third, each CNNis asynchronously placed on an available GPUfor training on the training dataset (line 4). Generally, the training method is stochastic gradient descent (SGD) with momentum, which is commonly used in the deep learning community. Finally, once the training phase is finished, the trained DNN is evaluated on the fitness evaluation dataset (line 4), and the evaluated performance accrepresents the fitness of the individual (line 4). It is important to note that both and here are training sets: is for training the weights, while is for training the architectures, and both architecture and the weights are part of the DNN model.
9.3 Discussions of  State-of-the-art ENAS
Nowadays, many of the neural architectures generated by ENAS have outperformed hand-crafted architectures across various domains. This breakthrough benefits from large quantities of works studying the three core components of ENAS, i.e., search space, search strategy, and performance evaluation. In this section, some key state-of-the-art designs of these three components are discussed.
9.3.1 Search Space
Commonly, the search space can be divided into three categories according to the basic units adopted: layer-based search space, block-based search space, and cell-based search space[35]. But there is one exception: some ENAS methods only take care of the connections between units instead of the basic units themselves and cannot be classified in any of the above categories  [58]. Therefore, an extra category named topology-based search spaceis introduced to represent this type of search space.
9.3.1.1 Layer-Based Search Space
For thiscategory, the basic units of the search space are primitive layers, such as convolution layers, pooling layers, and fully connected layers. The search space used in the step-by-step design of the ENAS algorithm in Sect.  9.2.2is exactly the simplest case of the layer-based search space, which only adopts the convolutional layerand the pooling layer. Three more complex examples of neural architectures in the layer-based search spaceare shown in Fig.  9.9. As can be seen in the figure, different combinations of the layers correspond to different candidate architectures in the layer-based search space, and different architectures may have a different number of layers. There may also be some limitations, e.g., pooling layers must come after convolution layers and fully connected layers must come after pooling layers.
Fig. 9.9
Three examples of neural architecture of the layer-based search space
This type of search space is a feasible solution commonly adopted by early works  [19, 51] because of its simplicity. However, there are two significant limitations in this design. First, the layer-based search spaceoften leads to low search efficiency. Specifically, well-performing DNNs are commonly composed of hundreds of primitive layers. As a result, if we want to search for well-performing DNNs in a layer-based search space, the search space should also include candidate neural architectures containing hundreds of primitive layers. However, the number of candidate neural architectures grows exponentially as the size of neural architectures increases. Second, the quality of the evolved solution may be underwhelming when compared with hand-crafted ones, such as ResNet  [21] and DenseNet  [23]. This is because the skip connections, recognized as the most important component of these hand-crafted neural architectures for performance improvement, cannot be represented by primitive layers.
9.3.1.2 Block-Based Search Space
Theblock-based search spaceimplies that the basic units of the search space are blocks composed of primitive layers and skip connections. The blocks are predefined by experts, with fixed primitive layers and topological relationships, with the hope that the designed blocks have promising performance. Traditional blocks include ResBlock  [21], DenseBlock  [23], ConvBlock (Conv2d + Batch-Normalization + Activation)  [13], and InceptionBlock  [55], which are shown in Fig.  9.10
Fig. 9.10
Illustrations of ResBlock, DenseBlock, ConvBlock, and InceptionBlock
It is easier for the block-based search spaceto find good neural architectures than the layer-based search spacefor two reasons. First, the block-based search spacecontains better neural architectures than those contained in the layer-based search space. This is because the block-based search space can make full use of the effectiveness of skip connections. Some excellent hand-crafted architectures, such as ResNet and DenseNet, are part of this space. Second, the search efficiency of a block-based search space is significantly higher than the layer-based search space, especially when searching for deep neural architectures. This is because the block-based search spaceregards complex modules (i.e., blocks) as basic units, which leads to significantly fewer candidate neural architectures when the depth of the candidate architectures is fixed.
Obviously, block design is a key task of block-based search. In practice, some ENAS methods employed the conventional blocks directly  [50], while others designed different blocks according to specific scenarios. For example, Chen  et al.  [6] proposed eight blocks, including ResBlock and InceptionBlock encoded in a 3-bit string, and used Hamming distance to determine similar blocks. Song  et al.  [45] proposed three residual dense mixed blocks to reduce the computational costcaused by the convolution operation of image super-resolution tasks.
9.3.1.3 Cell-Based Search Space
The cell-based search spaceis similar to the block-based one. Commonly, the search space can be divided into a micro part and a macro part. The micro part focuses on how to construct the basic units, and the macro part defines the topological relationship between basic units. For example, the basic units of the block-based search spaceare predefined blocks, and only the connections between different blocks remain to be determined. In this situation, the block-based search spaceonly supports the search process on the macro part but not on the micro part.
Generally speaking, the search part of the cell-based search spaceis exactly opposite to the block-based search space, i.e., the cell-based search spacefocuses on the micro rather than the macro part. The basic units of the cell-based search spaceare cells whose structures are determined during the search instead of being predefined. These cells are repeatedly stacked according to a predefined topological relationship, combining their structures to form the final architecture. The commonly used cell-based search spacesinclude NAS-Bench-101 search space  [63], NAS-Bench-201 search space  [12], NASNet search space  [69], and DARTS search space  [32]. For better understanding, we take the NAS-Bench-101 search space as an example, and we explain how to form a candidate architecture in detail. As shown in Fig.  9.11, the overall architecture starts at the convolutional stem and ends with a global average pooling layer and a dense softmax layer. Its bodycomprises three stacks, and every stack is obtained by stackingthree repeated cells. In addition, a downsampling layer is added between every two stacks. Please note that all the cells in one candidate architecture have the same structure and a cell example is shown in Fig.  9.11
Fig. 9.11
Overall architecture and cell example of NAS-Bench-101
The cell-based search spacehas several advantages. Firstly, it significantly reduces the size of the search space. This is because the cell-based search space only searches for a few types of cells (typically one to two types) and constructs the final architectures by repeatedly stackingthese cells. Secondly, neural architectures in the cell-based search space have the potential to surpass hand-crafted neural architectures. This is because cell-based search spaceconcentrates on the micro part and can find cells that include but are not limited to conventional structures such as ResBlock and DenseBlock. Thirdly, neural architectures searched from the cell-based search spacedemonstrate excellent transferabilityin practice. Specifically, the cells can be stacked to form neural architectures with varying depths (i.e., number of cells) and widths (i.e., number of filters), which can be applied to different tasks. For example, in the NASNet search space, the final architectures are commonly obtained by stacking20 cells with an initial channel number of 36 for small-scale datasets such as CIFAR-10, while the settings change to 14 cells with an initial channel number of 48 for large-scale datasets such as ImageNet
9.3.1.4 Topology-Based Search Space
Topology-based searchdoes not consider the configuration of the structural units (layer or block) and is only concerned with the connections between units. This type of search space is usually associated with specific practices. Two examples of the topology-based search spaceare considered next.
Fig. 9.12
An example of combining the topology-based search space with pruning
One typical case is to combine the topology-based search spacewith pruning. In particular, Wu  et al.  [58] construct a topology-based search spacebased on the shallow VGGNet  [43], where the candidate neural architectures can be obtained by pruning unimportant weight connections from the VGGNet. An example of the neural architecture in this topology-based search spaceis shown in Fig.  9.12. In the figure, the circles denote neurons in the network, the red solid lines denote the weight connections that still remain, and the black dashed lines denote the pruned connections. Only the solid lines and the nodes that are connected by the solid lines together represent a candidate neural architecture. Please note that the figure only shows one specific example of the candidate neural architecture, and all possible combinations of nodes and solid lines denote candidate architectures, which together form this topology-based search space
Another classical application is in the one-shot NASmethod, in which all candidate architectures can be represented by subgraphs of a supergraph  [14]. Yang  et al.  [61] proposed CARS to search for architectures by choosing different connections in the supergraph, and then the architecture was built by the chosen subgraph. Consider the simplest case, as shown in Fig.  9.13, a supergraph composed of one input node, three different operations (including a 3  3 convolution, a 5  5 convolution, and max-pooling), and one output node summing all its inputs. Then a subgraph can be obtained by only activating one operation (i.e., the max-pooling in the figure), which represents the architecture that passes input through the max-pooling layer to derive its output. Similarly, the other two candidate architectures can be obtained by activating the 3  3 convolution and 5  5 convolution, respectively. As a result, the simple search space contains three candidate architectures.
Fig. 9.13
An example of combining the topology-based search space with the one-shot method
9.3.2 Search Strategy
There are two methods to perform the population initialization: fixed-length encoding and variable-length encoding.
9.3.2.1 Fixed-Length Encoding
As the name states, the individuals will all have the same length in the search process under the fixed-length encoding strategy. For example, Xie et al.  [59] encode each architecture layer as a fixed-length binary string, and then these binary strings for each layer are put together to form the encoding of the whole architecture. Loni et al.  [36] define the chromosome of individuals with a fixed number of parts. In each chromosome, each part has its own specific meaning, such as the type of activation functionsand the kernel size of convolution layers. In addition, Broni et al.  [5] represent individuals as chromosomes consisting of fixed-length strings. Each gene is encoded through cellular encoding, which is a generative encodingbased on simple local graph transformations.
Here we take DeepMaker  [36] as an example to show how to encode the neural architectures into a series of fixed-length encodings in detail. Since DeepMaker is proposed to search for both accurate and lightweight architectures, the encoding of each individual contains five parts: the number of condensed layers which makes the resulting architecture light, the number of convolution layers which makes the resulting architecture accurate, the type of activation functionand optimizer, and the kernel size of the convolution layers. With this encoding strategy, each individual can be denoted by a series of encodings of a fixed length. In Fig.  9.14, the general genotype and two encoded individuals are shown. The general genotype is a list that contains information on the five aspects introduced above. Moreover, in the individuals, the number of the condense layers and convolution layers are determined, which are shown as 2, 3 and 3, 1 in this example, and then the type of the activation function, the optimizer and the kernel size are selected from the candidates.
Fig. 9.14
An example of the fixed-length encoding in DeepMaker  [36]
Discussion:Because the individuals all have the same length of encoding, the standard crossover and mutation operators can be directly used in the search process. However, though fixed-length encoding has such advantage, there is also a significant drawback. The length of the individuals needs to be determined in advance under the fixed-length constraint. Because the optimal length of the individuals is not known in advance, determining the optimal depth of the networks (i.e., the length of the individuals) highly relies on expertise in designing neural architectures. ENAS methods that utilize the fixed-length encoding strategy do not have the ability to search globally for optimal network depths, which prevents them from being completely automated. For instance, assume that the optimal depth of the network is eight. However, if the predetermined length of the individuals is six, the NASalgorithm will never search the optimal neural architecture. Variable-length encoding is designed to address the problems above, and this encoding strategy is introduced in the following subsection.
9.3.2.2 Variable-Length Encoding
When it comes to variable-length encoding, the length of different individuals is allowed to be different. For instance, Sun et al.  [51] designed a layer-based variable-length encoding strategy to achieve the completely automatic design of CNNarchitectures. Wang et al.  [56] and Sun  et al.  [53] borrow the idea of the IP address to encode architectures. In this encoding strategy, each layer in the architecture is determined based on the range of their IP addresses, and a layer called the “disabled layer” is designed to achieve the goal of “variable length”. For example, there are two architectures with a depth of five. The first architecture consists of five convolution layers, and the second architecture consists of four convolution layers and one disabled layer. Actually, the depth of the second architecture is considered four because of the presence of the disabled layer. Individuals within the population exhibit different lengths based on this type of design.
Fig. 9.15
Examples of the variable-length individuals in EvoCNN  [51]
The EvoCNN method  [51] proposed by Sun et al. is taken as an example and introduced as follows. There are three basic units designed and utilized to construct the individuals, the convolution layer, the pooling layer, and the fully connected layer, as shown in Fig.  9.15. Furthermore, these three kinds of layers are stacked to construct a single chromosome which denotes an individual in the population, and there are three constructed individuals with different lengths shown as examples in Fig.  9.15
Fig. 9.16
An example of the crossover operation designed in EvoCNN  [51]
Under the variable-length condition, classical evolutionary operator design often cannot achieve the desired results. Special operator design needs to be proposed for variable-length individuals, as introduced in the following paragraphs.
Crossover:In this case, we take the design of the crossover operator of EvoCNNsssssssssss [51] proposed by Sun et al. as an example to help the readers understand the overall flow of the crossover operation under the variable-length condition (see Fig.  9.16). The first individual presented is of length eight and contains three convolution layers, two pooling layers, and three fully connected layers, and the second individual presented is of length nine and includes two convolution layers, three pooling layers, and four fully connected layers. With the initialization, the first step of crossover is to stack the layers of the same type by the order they appear in the chromosome of the individuals, which is shown in part (a). After that, the lists which contain the same type of layers are aligned at the top, the layers are paired, and the crossover operation is performed, as shown in part (b). Finally, as shown in part (c), the layers in the lists are restored based on their initial order in the chromosome.
Mutation:The mutation operator employed in EvoCNN includes adding, removing, or modifying the layer at the selected mutation point. The mutation operation can occur at each point of the chromosome of a specific individual with a pre-determined probability. In addition, for the modifying operation, when replacing a layer with another layer, the information encoded will have to be changed accordingly, such as filter width of the convolution layers and the number of neurons in fully connected layers.
Discussion:Although there are several significant advantages of the variable-length encoding described in Sect.  9.2.1.1, there are also some disadvantages. First, the general genetic operators, especially the crossover operator, cannot be used under variable-length encoding because the general crossover operator specifies that the two individuals crossed over must be the same length. Second, the variable-length encoding may lead to excessively large individuals (i.e., over-depth architectures). These over-length individuals can increase the time and resource budget for the fitness evaluation process, thereby reducing the overall efficiency of the ENAS algorithm.
9.3.3 Fitness Evaluation
Performance evaluation techniques can be divided into four categories: training from scratch, low fidelity, fitness caching, and weight inheritance
9.3.3.1 Training from  Scratch
Training from scratchis the earliest and most accurate way to evaluate architecture performance. If we want to know the architecture performance, we typically make a full training on the training set, evaluate the architecture with its trained weights on the validation set, and finally, the real performance is obtained on the unseen test set.
This type of performance evaluation is inefficient since training complex networks is costly and ENAS algorithms require the evaluation of many individuals. For instance, Real  et al.  [40] employed 250 computers over 11 days to complete the LargeEvo algorithm. Given its resource-intensive nature and inefficiency, this technique is unaffordable for most users, making it less attractive nowadays.
9.3.3.2 Low Fidelity
Low fidelityevaluation is a performance evaluation technique that assesses neural architectures through proxy tasks. This technique is based on the assumption that the architecture performance on the proxy task is close to the real performance or that their performance rankings are consistent with those on the original task. Low fidelityhas long been a popular performance evaluation technique for its efficiency and has produced many specific methods. Next, we will introduce some popular low fidelitymethods.
Fig. 9.17
Two learning curves of two different individuals
Early Stopping:Early stoppingpolicyis a widely used low fidelitymethod, acting on the training settings. Its simplest form consists of training the neural architectures for a small and fixed number of epochs, on the basis that the performance after a small number of training epochs can reflect the rankings of the real performance. This method is practiced by the work of  [49], where training individuals with a small number of epochs is enough. Assunção  et al.  [1] improve the method by shortening the training time in each epoch instead of the number of training epochs, which allows the neural architectures to undergo all the training epochs under a more precise evaluation. So  et al.  [44] proposed to set hurdles after a fixed number of epochs so that weak individuals stop training early to save time, while well-performing individuals can continue training to get precise performance.
Sometimes, early stoppingdoes not reveal the real performance rankings of the individuals (especially when large and complicated neural architectures are involved) and may result in poor individuals being selected. As shown in Fig.  9.17, the individual that performs better in the early training stage is not necessarily better in the final training stage. Specifically, individual2 appears to be better than individual1 before epoch t2, but the opposite is true after epoch t2. As a result, individual1 is a better neural architecture that should be chosen according to their real performance. However, if the early stoppingpolicyis applied, and the stopping epoch is set to be smaller than t2, then individual2 is chosen by the algorithm, which is not consistent with our expectations. Therefore, it is crucial to determine the point at which the training process should stop. Commonly, neural networks can be trained to convergence, after which the training is meaningless and no longer leads to performance improvement. As can be seen in Fig.  9.17, individual1 converges at epoch t3, and individual2 converges at epoch t1. It is reasonable to choose epoch t3 as the stopping point of individual1 and epoch t1 as the stopping point of individual2, respectively. Therefore, some methods  [19] stop training when observing there is no significant performance improvement over a certain number of epochs.
Reduced Training Set:In this method, a subset of the original or a smaller dataset is used as the proxy dataset. In the search phases, the individuals are evaluated on the proxy dataset and compared according to the proxy results. Once evolution is over, the evolved neural architecture is transferred to the target task for final use. In practice, thanks to so many benchmark datasets in the image classificationfield, it is convenient to evaluate and compare the architectures on a smaller dataset such as CIFAR-10first and then apply the better architectures on a large dataset such as CIFAR-100 and ImageNet. An illustration of the method is shown in Fig.  9.18. As we know, the CIFAR-10dataset is significantly simpler than the ImageNet dataset, so the search process can be a hundred times faster than searching directly on the ImageNetdataset.
Fig. 9.18
An illustration of the reduced training set method
In addition, using a subset of the original dataset as a proxy task is also feasible. For example, Liu  et al.  [34] searched for promising architectures by training on a subset of data. After that, transfer learningcan be used on the large original dataset.
Reduced Population:Reducing the population size is also a low fidelitymethod, unique to EC algorithms because non-EC algorithms have no concept of population.
Fig. 9.19
An illustration of the ENAS algorithm with the reduced population size
The simplest way is to reduce the population size directly to speed up the evolution. However, since the population size is an important parameter, which may limit the global search ability of the ENAS algorithms, simply reducing population size may lead to significant performance degradation. An improved method was proposed to reduce the population size dynamically, with a balance between efficiency and performance. Fan  et al.  [17] used the evolution strategy and divided the evolution into three stages with population reduction, as illustrated in Fig.  9.19. At the first stage, the population is set to be 3n, where nequals two in the example. As a result, the search process is performed with a population size of 6 at the first stage. Once the first stage is completed, population size is reduced to 2n, which is equal to four in the example, and the best four individuals circled in the figure at the first stage are kept in the second stage for continued evolution. Similarly, after the second stage of evolution, the population size is reduced to n, which is equal to two in the example, and the best two individuals in the first stage are retained for the final stage of evolution. In this method, the population is kept large at the first stage, promoting the global search ability of the evolution process. At the next stages, the population size is reduced to shorten the evolution time.
In addition to reducing population size, reducing the complexity of individuals in the population is also feasible. For example, Liu  et al.  [33] began the evolutionary process with small-sized architectures for time-saving. Similarly, Wang  et al.  [57] searched for well-performing blocks instead of the whole architectures at the early stage. After that, the evolutionary process was performed among the architectures composed of the searched blocks.
9.3.3.3 Fitness Caching
Fitness Cachingrefers to another category of the performance evaluation technique, which saves architectural information for reuse. This category is especially popular in ENAS algorithms, because in the population-based methods, it is natural to always keep the best few individuals in each generation for the next generation. When the performance of the same architecture is repeatedly required, performance is directly obtained by the cache mechanism, which is significantly cheaper and more efficient than evaluating the architecture again.
Fig. 9.20
An example of fitness caching
There are many different approaches in practice. For instance, the simplest way is to directly store the fitness of individuals in memory. Before evaluating an architecture, if the information can be found in memory, it can be retrieved from memory instead of reevaluating the architecture. An example is shown in Fig.  9.20. When the offspring Kis generated, we first check the cache that can be a lookup table. If Individual Kexists in the lookup table, we directly assign to offspring Kinstead of reevaluating it.
Similarly, Sun  et al.  [52] saved information pairs of architectures and their corresponding fitness values, but used a hashing method to accelerate the query process. Johner and Wassner  [25] utilized the caching mechanism in another form, prohibiting the appearance of architectures that existed before offspring generation. This approach did succeed in reducing time, but with the limitation that the best individuals were forbidden to survive in the population, possibly misleading the search.
9.3.3.4 Weight Inheritance
Weight inheritanceis a category of performance evaluation technique that uses the weight information of previously trained neural networks based on the assumption that similar architectures may have similar weights. This technique is also applied in ENAS algorithms because offspring generated by crossover and mutation during the evolutionary process usually have parts as their parents, and the weights of the same parts can be inherited. One specific example is shown in Fig.  9.21. In the figure, the architecture on the left is an individual in the current population. It has three nodes, numbered 1 to 3 in sequence, connected one by one. The architecture on the right is a newly generated individual with four nodes, numbered 1 to 4. Among them, in addition to node number 4, the other three nodes have the same topological relationship as Architecture1. As a result, these three nodes can directly inherit the weights from the corresponding nodes of Architecture1, and only the weights of node number 4 are trained from scratch. With the weight inheritancetechnique, the neural networks only need to be fine-tuned for a few epochs, vastly reducing the search costs.
Fig. 9.21
An example of weight inheritance
9.4 Advanced Topics
In this section, we introduce five advanced topics in ENAS: benchmarks of ENAS, efficient evaluation, resource-constraint ENAS, interpretabilityof ENAS, and theoretical analysis of ENAS.
9.4.1 Benchmarks of  ENAS
As research into ENAS continues, an increasing number of ENAS algorithms are being proposed. To verify their effectiveness, extensive comparison with similar methods is required. However, since different methods use different search spaces and training methods, a completely fair comparison is generally difficult to achieve. Addressing this challenge, a popular benchmark platform called BenchENAS[60] has been implemented recently, serving as a valuable tool for ENAS researchers to compare different ENAS algorithms relatively fairly. The overall framework of BenchENAS, depicted in Fig.  9.22, consists of five main components: the ENAS algorithms (top part), the data settings (left part), the trainer settings (right part), the runner (middle part), and the comparer (bottom part).
Fig. 9.22
The overall framework of the popular benchmark platform BenchENAS[60]
In the BenchENASframework, users begin by selecting an ENAS algorithm to run with the data settings and trainer settings. Subsequently, through the runner and the comparer components, relatively fair comparison results are obtained. In the component of ENAS algorithms, users can choose one ENAS algorithm provided by BenchENASor implement their own algorithm. The component of data settings allows users to select candidate datasets such as MNIST, CIFAR-10, CIFAR-100, and ImageNet, which are popular benchmark datasets for image classification. In addition, users can configure the scheduler of the learning rate, the settings of the batch size and the number of epochs, and the type of optimizer in the trainer settings component. Once all settings are determined, the runner component executes the selected ENAS algorithm. In particular, in the runner component, the whole algorithm is launched and run by the center node. Then a well-designed evaluator is used to evaluate the fitness of each individual in the population. At the end of the algorithm execution, the comparer component obtains the indicators, including the accuracy, the number of flops, the parameter size, and the GPUdays. After obtaining these indicators, the users can compare different ENAS algorithms fairly. Despite BenchENAS takinga significant step forward in enabling fair comparisons between ENAS algorithms, several challenges still need to be properly addressed. Firstly, the current inclusion of ENAS algorithms in BenchENAS is limited, and future platforms should aim to incorporate a more diverse and representative set of well-implemented ENAS algorithms. Secondly, though the efficient evaluator in BenchENAShas significantly reduced the computational resource budget already, it does not fundamentally address the problem of high computational resource consumption in ENAS algorithms. To address this problem, it is crucial to incorporate more efficient evaluationmethods, which is also a challenge and needs to be investigated in the future.
9.4.2 Efficient Evaluation
Although the evaluationtechniques introduced in Sect.  9.3.3have demonstrated their effectiveness, there are still challenges to be addressed regarding the accuracy and efficiency of the evaluation. In order to alleviate these issues, further research is proposed in two key areas.
In particular, the first category of works is similar to those introduced in Sect.  9.3.3. These approaches still require obtaining the fitness of individuals on the fitness evaluation dataset directly. For example, Bender et al.  [3] designed a supernetto reduce the time for training thousands of individuals to obtain their fitness. The supernetis a set that contains all possible individuals in the population, and each individual is a subset of the supernet. The evaluation cost can be reduced because the individuals are evaluated by inheriting the trained weights after the supernetis trained. However, because the weights in the supernetare heavily coupled, the performance evaluated with inherited weights may be inaccurate sometimes. In addition, another category of work uses proxy models, which are small-scale versions of the architectures to be evaluated. For instance, Lu et al.  [38] construct proxy modelsof individuals by reducing the number of layers and channels in architectures. The efficiency is improved because training these proxy modelscosts very little time. However, the prediction accuracy of the performance values is compromised in this case, as the proxy modelsoften do not represent the actual performance of the original architecture.
On the other hand, another category of methods, called performance predictor, utilizes an indirect approach to estimate the performance values of individuals. In particular, the performance predictor is often formulated as a regression model, which can predict the performance of the individuals without training them on the training dataset. The performance predictors can be divided into two categories: performance predictors based on the learning curveand end-to-end performance predictors. In the first category, the final performance is predicted based on the performance of the first several training epochs  [39]. In the second category, the end-to-end performance predictor can predict the performance of an architecture without any prior knowledge about the performance of this architecture. To obtain this kind of performance predictor, a certain number of architectures with their corresponding performance values are needed to train the predictor in advance  [8]. This process is time-consuming and resource-intensive, defeating the original purpose of a performance predictor to reduce time and resource consumption.
In summary, efficient evaluationtechniques are still facing bottlenecks in terms of accuracy and efficiency, and more approaches need to be proposed to address these issues in the future.
9.4.3 Resource-Constraint ENAS
