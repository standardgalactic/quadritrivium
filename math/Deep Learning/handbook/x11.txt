ACO is used in [37] for feature selectionin an approach in which features are represented as nodes of a graph. The algorithm is compared to other swarm optimization approaches on 12 datasets and the authors conclude that the proposed algorithm achieves a better feature set in terms of classification accuracy and number of selected features. Xue et al. [82] introduce two bi-objective PSO algorithms with the goal of simultaneously minimizing the classification error rate while minimizing the number of features selected. The two MO-PSO algorithms were compared to single-objective PSO. The results indicate that the algorithms are able to obtain solutions with a small number of features and achieve higher classification performance than methods that use all features.
In [90], three different variants of multi-objective PSO algorithms are introduced for discretization-based feature selection. The algorithms can select an arbitrary number of cut-points for discretization helping to better explore the relevant features. Experiments are conducted on ten benchmark microarray gene datasets comparing the algorithm to other three PSO approaches and two traditional feature selectionalgorithms. The results show that the multi-objective PSO algorithms significantly outperforms the other methods in terms of test classification accuracy.
EDAs have also been applied to feature selectionin a number of works. In [36], two variants of EDAs that respectively use univariate and bivariate marginal distribution models are applied to feature selectionin the problem of predicting the survival of cirrhotic patients that have received a treatment. The bivariate model assumes the existence of some kind of dependencies among the attributes of the database and is shown in the experiments to produce the best average accuracy results for each classifier.
An EDA using Bayesian network models for feature selectionis proposed in [35]. The algorithm uses a wrapper approach, over NB and ID3 learning algorithms, to evaluate each candidate solution. The experiments conducted on eight datasets show that the algorithm can significantly reduce the number of features used by the classifiers while keeping a similar predictive accuracy. Saeys et al. [69] use a univariate marginal distribution model to represent the distribution of the best solutions. They propose to use the univariate probabilities to rank features according to relevance. The algorithm is applied to the splice site prediction problem.
Said et al. [70] study a common limitation of EML for discretization-based feature selection, i.e., the fact that encoding the problem solution as a sequence of cut-points can lead to the deletion of possibly important features that have a single or a low number of cut-points. They propose to attack the problem as a bi-level optimization problem and solve it using a co-evolutionary algorithm. The introduced algorithm performs feature selectionat the upper level while discretization is done at the lower level. The results of the experiments show that the algorithm outperforms state-of-the-art methods in terms of classification accuracy, generalization ability, and feature selectionbias
While most of feature selectionand feature constructionmethods are applied to supervised ML tasks, there are some situations where no supervision is available. In these cases dimensionality reduction can be required and EML has been also applied in these scenarios. In [39], Kim et al. introduce a bi-objective evolutionary local selection algorithm that simultaneously solves feature selectionand clustering. The evolutionary search is hybridized with the application of two clustering algorithms (K-means and EM). The experimental results show that the algorithm can find an appropriate clustering model and also identifies relevant features.
There are also EML proposals that try to solve more than one unsupervised ML task simultaneously. In [5], a GA is applied as the building block of a reduction algorithm that can perform feature selectionand instance selection simultaneously. The algorithm is tested on nine real-world datasets with varying difficulties. The experimental results show that the proposed algorithm can reduce both the number of features and the number of instances. The characteristics of the fitness landscape of GP-generated ML pipelines that comprise a feature selectioncomponent have been investigated in [28].
3.3.4.2 EML for Feature Construction
One of the first proposals on EML for feature constructionwas presented in [76], where the authors advanced the idea of using GAs to form combinations of existing features via a well-chosen set of operators. The suggested crossover operator for the GA representation was similar to those used to combine tree-structured programs in GP. The approach was tested on the problem of recognizing visual concepts in texture data, and feature constructionwas applied after an initial feature selectionstep. The results indicated a small but significant improvement in the classification and recognition of real-world images.
The Evolution-COnstructed (ECO) feature constructionmethod is introduced in [54]. It uses a GA to discover highly discriminative series of transforms. The approach focuses on image feature extraction, and the GA representation encodes a sequence of transforms, their operating parameters, and the regions of the image the transforms will operate on. The features generated by the algorithm were used by an AdaBoostclassifier and evaluated on several image datasets. The results show that the features generated by the algorithm are highly generalizable and also have high discriminative properties. The ECO method is further enhanced in [86] by reducing the dimensionality of the ECO features.
Otero et al. [65] introduce a GP-based single feature constructionmethod in which each program is encoded using the standard tree-structure representation. To measure the quality of the features, they use the information gainratio as a fitness function. The algorithm was tested using four datasets and the C4.5classifiers. For two of the datasets, the error rate of C4.5with the constructed attribute was smaller than the error rate of C4.5without it. In the other two cases, there were no statistical differences in the results.
In [59], another GP-based single feature constructionmethod is presented. The authors focus on the analysis of decision tree classification algorithms that used the constructed features. GP uses information gainor the Gini index informationmeasures as the fitness function. The algorithms were tested using four classifiers and five datasets, and one of the conclusions from the experiments was that all classifiers benefit from the inclusion of the evolved feature.
A GP-based algorithm for multiple feature constructionis proposed in [43] where the tree-structure representation is used, but each individual’s genotype is split into two disjoint parts, evolving featuresand hidden features, and each individual decides on its own which features should and should not be visible to the genetic operators. The algorithm was tested using six datasets. The results indicate that the GP-based approach to feature constructionprovides remarkable gains in terms the predictive accuracyof the ML classifiers.
Ma and Gao [56] propose the use of GP within a filter-based multiple feature constructionapproach. Each GP program encodes the transformation that creates a single feature. The top individuals are stored in a storage pool as a way to represent multiple features. The algorithm is compared to other three state-of-the-art feature constructionalgorithms on nine datasets. The experimental results show that the algorithm can obtain better performance than classifiers that use the original features and outperform the other state-of-the-art algorithms in most cases.
Most EML contributions to feature constructionare based on the use of GAs and GP algorithms. However, other evolutionary algorithms have been also applied to this problem.
Xue et al. [83] introduce the use of PSO to feature construction. The algorithm uses a binary encoding to represent the low-level features that are selected, and for each solution perform a local search among a set of arithmetic operators to select those to construct a new high-level feature. The PSO method is evaluated using seven datasets and three classification algorithms. The results show that using the single constructed feature achieved competitive results than using all the original features, and when the constructed feature is added to the original set, it significantly improves their classification performance in most of the cases. Two other solution representations for PSO algorithms are proposed in [16]. The rationale behind the pair representation and the array representation is to allow PSO to directly evolve function operators. Experiments are conducted on seven datasets and from the results, it is concluded that the two PSO algorithms can increase the classification performance of the original PSO representation for feature constructionpresented in [83].
In addition to PSO, other EML approaches that have been used for feature constructioninclude Gene Expression Programming [20], Kaizen Programming [17], and EDAs [71]. Several other works apply EML techniques for feature selectionand feature construction. The interested readers can consult [82] for a detailed survey on this area.
3.3.4.3 EML for Manifold Learning
Theapplication of EML algorithms to dimensionality reductiontasks other than feature selectionand feature constructionhas not received the same attention in the literature. Nevertheless, there are some recent works that propose the use of EML for MaL.
Orzechowski et al. [64] propose two different GP-based MaL methods and compared them to other eight classical MaL algorithms over a large collection of 155 datasets. One of the proposed GP-based methods, ManiGP, showed far more separable results than any other MaL included in the comparison. However, the high computational time required by this method makes its practical use very limited.
Lensen et al. [50] propose the use of GP for solving MaL tasks where the goal is the preservation of local neighborhoods. This work is extended in [52] by learning manifolds with different number of dimensions using a bi-objective approach. In [51], an approach similar to the one introduced in [50] is compared to other five baseline methods on 12 datasets. The results show that the algorithm is competitive with all baselines for higher dimensionality () and often outperforms them at lower dimensionality.
Uriot et al. [75] evaluate different fitness functions to guide GP in MaL tasks. They compare the GP algorithm to other three classical algorithms for dimensionality reductionand report that GP is a competitive approach to obtain interpretable mappingsfor this task.
3.3.5 EML for Outlier or Anomaly Detection
Outlier detectionwas one of the ML problems earliest addressed using EML. In [49], Leardi presented a GA-based algorithm that, while solving a feature selectionproblem within a cross-validationframework, was able to detect outliers. The idea was to identify deletion groups (folds) for which the results of the classifier were not as accurate as in the majority of the other folds, suggesting the presence of one or more outliers. The GA was tested using six different datasets, and authors reported that the algorithm was able to easily identify anomalous examples unidentified by classical methods.
Crawford and Wainwright [14] presented one of the initial EML approaches to outlier detection. They researched the question of outlier diagnostics, i.e., rather than minimizing the effect of outlying data, outlier diagnostics try to assess the influence of each point in the dataset. They used a GA with a variant of two-parent uniform order-based crossover and accomplished the identification of potential outlier subsets. Experiments were conducted using five different datasets and the authors concluded that the GA approach was effective for generating subsets for multiple-case outlier diagnostics.
Bandyopadhyay and Santra [8] also investigated the question of estimating the “outlierness value” of an outlier group as a relative measure of how strong an outlier group is. They propose a GA that projects the original points to a lower dimension and identifies outliers in this lower dimensional space as those with inconsistent behavior (defined in terms of their sparsity values). The algorithm was evaluated on three artificial and three real-life datasets showing the effectiveness of the new algorithm.
Tolvi [73] introduces a GA which searches among different possible groupings of the data into outlier and non-outlier observations. The analysis is focused on outlier detectionlinear regression models. The algorithm is validated on two small datasets, and its scalability is investigated when the number of instances is increased.
In [29], a hybrid GA that includes a multi-start metaheuristicis proposed for anomaly detection. The rationale behind the use of the multi-start mechanism is to generate a suitable number of detectors with high anomaly detection accuracy. It also incorporates the K-means clustering algorithm as a way to reduce the size of the training dataset while keeping the diversity. The new proposal is applied to the evaluation of intrusion detection and compared to other six ML techniques. The experimental results suggest that the algorithm outperforms the other methods in terms of test accuracy.
Cucina et al. [15] introduce a GA approach to detect outliers in multivariate time series. The rationale behind the use of GAs in this context is that it can process multiple outliers simultaneously, being less vulnerable to the maskingand smearing effects. To test the algorithm, authors use four simulated time seriesmodels. The experimental results show that the GA is able to provide a better performance than other methods for the case of consecutive outliers, while the results were similar for the case of well-separated outliers.
In [58], a PSO algorithm is proposed for the outlier detectionproblem and compared to a Local Outlier Factor (LOF)algorithm on five real datasets. In most of the datasets, the PSO approach outperforms LOF.
3.3.6 EML for Association Rule Mining
One straightforward representation of association rules is to associate two bits with each possible item to indicate whether it belongs to the antecedent part, the consequent part, or to none of them. This is the approach used in [79] where if these two bits are 00 then the corresponding attribute appears in the antecedent part, and if it is 11 then the attribute appears in the consequent part. The other two combinations, 01 and 10, will indicate the absence of the attribute in either of these parts. The problem with this representation is that the length of the chromosome grows significantly for problems with a large number of items. A different type of representation is proposed in [84] where each item has associated one binary variable, and there is an additional integer variable that indicates the position in the chromosome that separates the antecedent from the consequent.
Kuo et al. [45] advocate the use of PSO for the generation of association rules from a database. The algorithm is applied to the analysis of investors’ stock purchase behavior. The authors state that while the commonly used Apriori algorithm requires the minimal support and confidence to be determined subjectively, the PSO approach can determine these two parameters quickly and objectively.
Ghosh and Nath [30] propose a multi-objective GA for ARM in which, together with the predictive accuracy, the comprehensibility and the interestingness measures computed for each rule were considered as objectives. The paper introduces some strategies to diminish the computational cost of the algorithm. Instead of using the entire dataset, the algorithm works on a sample of the original database, and the sample may not truly reflect the actual database. Another multi-objective approach is presented in [79], where the use of an external archive with the non-dominated solutions is proposed as a way not to lose rules that were generated at intermediate generations.
In [9], a multi-objective PSO algorithm for ARM is presented. The objectives to optimize include confidence, comprehensibility, and interestingness. The algorithm is compared to other four evolutionary algorithms in three datasets. The authors conclude that the new algorithm shows a robust behavior for association rule mining with a simple particle design and without requiring a priori discretization.
For comprehensive surveys on the use of evolutionary algorithms for ARM, the interested reader is referred to [7, 72, 78].
3.4 Conclusions
In this chapter, we have presented an overview of the use of EML to unsupervised ML tasks. Our analysis shows that most of the EML approaches are based on GAs and GP although other evolutionary algorithms such as PSO and ACO are also applied. As a summary of this chapter, we have extracted a number of observations that can guide the reader at the time of applying EML techniques to unsupervised ML tasks or when developing new EML approaches.
Dependence between Different ML Tasks and Holistic EML Approaches
EML is usually applied to particular ML tasks (e.g., dimensionality reduction, and imputation). However, it is clear that the solutions found for some ML task of the ML pipeline can affect the solution of the next stages of the pipeline. Therefore, when applied to solve real-world problems, holistic EML approaches are required that take into consideration the dependencies between tasks.
There are works in the field of EML that simultaneously confront more than one ML task. For example, discretization-based feature selectionEML algorithms can find an appropriate discretization of the features that allow to select relevant features more accurately [70, 90]. There is also an interaction between instance selection and imputation, and some EML methods have been proposed that exploit this relationship [3]. As ML approaches become more sophisticated and involve more steps, EML algorithms should pay more attention to these dependencies.
The Potential of Multi-objective EML
Multi-objective EML strategies are involved in many scenarios where EML outperforms classical ML methods for unsupervised ML problems. As in other domains, it is common in ML problems the necessity to obtain solutions that satisfy multiple performance criteria, which are often conflicting. Multi-objective EML methods are natural candidates for the solution of these problems.
New Ways of Hybridization Between EML and Classical ML Techniques
In several of the EML applications, the evolutionary algorithm is combined with supervised and unsupervised classical ML methods. Traditional ML metrics, and clustering and classification algorithms are frequently incorporated as part of the fitness function. Novel ways of integrating the ML techniques as part of the EML could lead to more powerful EML strategies.
Computational Efficiency as a Barrier to Scalability
Increasing the population size of population-based EML algorithms in which the evaluation of a single individual implies processing the complete dataset can become impractical for very large datasets. EML algorithms that can work with small samples of the dataset have been proposed to deal with the scalability challenge. New strategies are required in order to make EML more competitive with other ML methods.
References
1.
Abdella, M., Marwala, T.: The use of genetic algorithms and neural networks to approximate missing data in database. In: IEEE 3rd International Conference on Computational Cybernetics, 2005, pp. 207–212. IEEE (2005)
2.
Agrawal, R., Imieliński, T., Swami, A.: Mining association rules between sets of items in large databases. In: Proceedings of the 1993 ACM SIGMOD International Conference on Management of Data, pp. 207–216 (1993)
3.
Al-Helali, B.,  Chen, Q., Xue, B., Zhang, M.: Gp with a hybrid tree-vector representation for instance selection and symbolic regression on incomplete data. In: 2021 IEEE Congress on Evolutionary Computation (CEC), pp. 604–611. IEEE (2021)
4.
Al-Helali, B., Chen, Q., Xue, B., Zhang, M.: A new imputation method based on genetic programming and weighted KNN for symbolic regression with incomplete data. Soft. Comput. 25(8), 5993–6012 (2021)
5.
Albuquerque, I.M.R., Nguyen, B.H., Xue, B., Zhang, M.: A novel genetic algorithm approach to simultaneous feature selection and instance selection. In: 2020 IEEE Symposium Series on Computational Intelligence (SSCI), pp. 616–623. IEEE (2020)
6.
Andreassen, A., Feige, I., Frye, C., Schwartz, M.D.: JUNIPR: a framework for unsupervised machine learning in particle physics. Eur. Phys. J. C 79, 1–24 (2019)
7.
Badhon, B., Jahangir, M.M., Kabir, S.X., Kabir, M.: A survey on association rule mining based on evolutionary algorithms. Int. J. Comput. Appl. 43(8), 775–785 (2021)
8.
Bandyopadhyay, S., Santra, S.: A genetic approach for efficient outlier detection in projected space. Pattern Recogn. 41(4), 1338–1349 (2008)zbMATH
9.
Beiranvand, V., Mobasher-Kashani, M., Bakar, A.A.: Multi-objective PSO algorithm for mining numerical association rules without a priori discretization. Expert Syst. Appl. 41(9), 4259–4273 (2014)
10.
Berg-Kirkpatrick, T., Bouchard-Côté, A., DeNero, J., Klein, D.: Painless unsupervised learning with features. In: Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pp. 582–590 (2010)
11.
Cano, J.R., Herrera, F., Lozano, M.: Instance selection using evolutionary algorithms: an experimental study. In: Advanced Techniques in Knowledge Discovery and Data Mining, pp. 127–152 (2005)
12.
Casolla, G., Cuomo, S., Cola, V.S.D., Piccialli, F.: Exploring unsupervised learning techniques for the internet of things. IEEE Trans. Industr. Inf. 16(4), 2621–2628 (2019)
13.
Chen, Q., Huang, M., Wang, H., Guangquan, X.: A feature discretization method based on fuzzy rough sets for high-resolution remote sensing big data under linear spectral model. IEEE Trans. Fuzzy Syst. 30(5), 1328–1342 (2021)
14.
Crawford, K.D., Wainwright, R.L.: Applying genetic algorithms to outlier detection. In: Proceedings of The Sixth International Conference on Genetic Algorithms (ICGA-1995), pp. 546–550 (1995)
15.
Cucina, D., Di Salvatore, A., Protopapas, M.K.: Outliers detection in multivariate time series using genetic algorithms. Chemometr. Intell. Labor. Syst. 132, 103–110 (2014)
16.
Dai, Y., Xue, B., Zhang, M.: New representations in PSO for feature construction in classification. In: Applications of Evolutionary Computation: 17th European Conference, EvoApplications 2014, Granada, Spain, April 23–25, 2014, Revised Selected Papers 17, pp. 476–488. Springer (2014)
17.
de  Melo, V.V., Banzhaf, W.: Kaizen programming for feature construction for classification. In: Genetic Programming Theory and Practice XIII, pp. 39–57 (2016)
18.
Deb, K., Pratap, A., Agarwal, S., Meyarivan, T.: A fast and elitist multiobjective genetic algorithm: NSGA-II. IEEE Trans. Evol. Comput. 6(2), 182–197 (2002)
19.
Derrac, J., García, S., Herrera, F.: A survey on evolutionary instance selection and generation. In: Modeling, Analysis, and Applications in Metaheuristic Computing: Advancements and Trends, pp. 233–266. IGI Global (2012)
20.
Drozdz, K., Kwasnicka, H.: Feature set reduction by evolutionary selection and construction. In: In Proceedings of the 4th KES International Symposium on Agent and Multi-Agent Systems: Technologies and Applications, KES-AMSTA-2010, Part II, pp. 140–149. Springer (2010)
21.
Eklund, N.H.W.: Using genetic algorithms to estimate confidence intervals for missing spatial data. IEEE Trans. Syst., Man, Cybern., Part C (Appl. Rev.) 36(4), 519–523 (2006)
22.
García, J.C.F., Kalenatic, D.,  Bello, C.A.L.: Missing data imputation in time series by evolutionary algorithms. In: Advanced Intelligent Computing Theories and Applications. With Aspects of Artificial Intelligence: Proceedings of the 4th International Conference on Intelligent Computing, ICIC-2008, pp. 275–283. Springer (2008)
23.
Flores, J.L., Inza, I., Larrañaga, P.: Wrapper discretization by means of estimation of distribution algorithms. Intell. Data Anal. 11(5), 525–545 (2007)
24.
García, S., López, V., Luengo, J., Carmona, C.J., Herrera, F.: A preliminary study on selecting the optimal cut points in discretization by evolutionary algorithms. In: International Conference on Pattern Recognition Applications and Methods (ICPRAM-2012), pp. 211–216 (2012)
25.
Garciarena, U., Mendiburu, A., Santana, R.: Towards a more efficient representation of imputation operators in TPOT (2018). CoRR, arXiv:​1801.​04407
26.
Garciarena, U., Santana, R.: An extensive analysis of the interaction between missing data types, imputation methods, and supervised classifiers. Expert Syst. Appl. 89, 52–65 (2017)
