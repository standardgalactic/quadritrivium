Fig. 8.7
Symbiotic cooperative coevolutionwith bid based agents [126]. Participants from the Team population (LHS) are defined as pointers to participants of the Agent population (RHS). For illustration the Agent population is considered to consist of three types of action (e.g. class 1, 2, 3), represented by the star, triangle and circle. Valid teams should consist of Agents representing at least two different types of action. The same Agent can appear in multiple Teams, but the team complement should be unique
In order to extend the two population model into a variable length representation (thus hybrid homogeneous/ heterogeneous compositions), agents need to distinguish between context and action [125]. Thus, agent execution9is used to identify the bid (or confidence) given the environment’s state. All agents from the same teamprovide a bid; however, only the agent with the highest bid ‘wins’ the right to suggest its action [126, 127, 223]. This means that multiple agents might appear in the same team with the same action, but with different contexts, adding another degree of flexibilityto the process of divide-and-conquer.10Moreover, teams need not start with the full complement of agent types, but instead incrementally develop the relevant type complexity.
In the simplest case the action, a, is just a discrete scalar value.11Agent actions are chosen from the set of task-specific actions , e.g. class labels. We now have an agent representation that can evolve teams consisting of any number of teamparticipant types and different sizes. Moreover, the parent pool is identified at the level of teams. Any team participants not associated with a surviving team are deleted, i.e. task specific fitness need only be defined at the level of the teampopulation. Hitchhiking is still an issue but can be addressed by periodically dropping team participants that never win a round of bidding. The resulting symbiotic model of coevolutionwas demonstrated to be superior to evolution without ensembles [127] and competitive with learning classifiers and support vector machines under multi-class classification tasks [126, 223]. Further developments included scaling to high-dimensional () classification tasks [56, 127] and operation under non-stationary streaming dataclassification tasks (Sect.  8.6).
The approach has also been extended to produce hierarchical teams for reinforcement evolutionary ensemble learning (rEEL) [55, 105]. This is distinct from maEEL as rEEL solutions describe a single agent policybut explicitly decompose the task/representation. One implication of this is that strategies for solving one aspect of a task can be reused for solving other aspects of a task [103]. The resulting tree structure represents teams as tree nodes and agents as arcs. Leaves represent atomic actions. The tree is constructed bottom-up (but evaluated top-down from a single root team), with successive layers describing their actions in terms of pointers to previously evolved teams [55, 105].
In order to ensure that different teams represent different strategies, then diversitymaintenance (during evolution) represents a re-occurring theme (Sect.  8.4.1). In particular, different tasks could be interleaved with the development of the hierarchical team, thus a natural approach for task transfer[101, 103]. Alternatively, competitive coevolutionhas been used to develop an ‘arms race’ between tasks and solution strategies resulting in the organization of thousands ofteamparticipants for optimally solving (tens of millions of) Rubik’s Cube configurations [186, 190]. As an additional benefit, unlike monolithic solutions (a single large agent), only one team per tree level is evaluated to determine the ultimate action, making for extremely efficient solutions when compared to neural networks [103, 187].
8.5.2 Tangled Program Graphs
Thesymbiotic framework was also generalized to organizing teams into graphs of teams, leading to results that are competitive with deep learning solutions on (visual) reinforcement learning problems[102, 187]. Indeed, the resulting ‘tangled program graphs’could learn policies for playing multiple game titles simultaneously under the ALE benchmark, i.e multitask learning[104]. The graph representation gives direct insightsinto the nature of the decomposition of agents to decision-making under different game titles, i.e. interpretable machine learning. Later developments demonstrated that the approach also scales to multiple types of partially observable task12such as Dota 2 [188] and ViZDoom navigation[108]. Support for real-valued actions under tangled program graphsenabled problems in recursive forecasting [108], multitask control [109], andbiped locomotion [14] to be addressed.
One of the unique benefits of the graph-based ensemble is that they are exceptionally efficient to train and deploy. The composition of agents, teams, and graphs is incremental and entirely emergent. This results in solutions whose complexity reflects the properties of the task, not the initial dimensionality of the state space or a priori assumptions about suitable solution topologies. Thus, under visual reinforcement tasks, less than 20% of the pixels are used to reach a decision [102, 104, 187].13The complexity of the entire solution is typically three or more orders of magnitude lower than deep learning approaches to the same task [102, 104, 187]. Specifically, in order to make a decision, only a fraction of the graph is evaluated, which this changing as a function of the state. Insightsare then possible about the functionality of participants in the evolved team[107]. In addition, this has led to the use of graph ensembles in ultra-low power signal processing applications such as real-time intrusion detection on IoT devices [196]. Indeed, solutions to the ALE visual reinforcement learning benchmark[137] have been demonstrated using nothing other than Raspberry PI embedded controllers[48].
Table 8.1
Summary of EEL research with specific application contexts
Application area
Publication
Control systems
Direct control
[55, 76, 161]
Dynamical systems modelling
[1, 47]
Path planning/obstacle avoidance
[54, 105, 144, 145]
Robot locomotion
[14]
Data analysis
Boolean expression learning
[13, 91, 92]
Cancerprediction
[7, 11, 85, 113, 222]
Instance selection
[71]
Interpretable solutions
[35]
Multi-label classification
[146]
Multi-objective
[21, 40, 42, 63, 123, 129, 139, 176]
Outlier reduction
[50]
Software fault utilization
[191]
Feature Construction
Application specific
[7, 21, 85]
High-dimensional (input)
[56, 127, 148]
Image data
[9, 27, 28, 36, 37, 89, 171, 180, 203]
Inter-task feature transfer
[8, 28, 149]
Low cardinality (training)
[9]
Multi-feature construction
[56, 89, 206]
Multi-task or transfer learning
Supervised
[8, 28, 149, 178, 180]
Reinforcement learningagents (memory)
[106, 108, 109, 188, 189]
Reinforcement learning agents (reactive)
[22, 48, 101–104, 112, 186, 187, 190]
Multi-agent Reinforcement learning
Air traffic control
[5]
Five aside soccer
[75]
Keepaway soccer
[101, 152, 213, 219]
Half-field offence
[101, 103]
Multi-agent communication
[52, 140, 225]
Multi-rover
[6, 51, 74, 75, 110, 175, 224]
Preditor–prey
[74, 225]
RoboCup
[15, 133]
Sensor networks
[4]
Scalable training
Active learning
[69, 72, 126, 127, 139, 185, 192]
Algorithmic efficiency
[214, 228]
Cloud or cluster computing
[18, 68, 212]
GPU platform
[17]
Surrogate fitness
[36]
Scheduling
Capacitated arc routing
[216]
Dispatching rules
[58–60, 80, 163]
Trip planning
[98]
Streaming
Benchmarks
[19, 64, 65, 67, 83, 210]
Churn detection
