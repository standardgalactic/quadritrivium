5.2.2 Genetic Programming for Symbolic Regression
Symbolic regression(SR) isa special type of regression without the need for a predefined model structure or the assumption on the underlying data distribution. This is a sharp difference from traditional regression. During the modelling procedure, SR finds the model structure in a symbolic representation and the coefficients of the model simultaneously. This is particularly attractive when there is a lack of domain knowledge and/or when the data is too complicated to derive any single or obvious distribution.
Genetic Programming[43] (GP), an EC approach, is an attractive and dominating technique for SR due to its flexible representation and symbolic nature. GP follows the Darwinian natural selection principle and automatically generates computer programs to solve various problems in many different fields [43]. Given a set of data, GP for SR (GPSR) learns the models iteratively via combining a number of features and essential mathematical expressions. The modelling process in GPSR typically begins with a population of randomly generated solutions/SR models. Utilising breeding mechanisms and fitness-based selection, this population simulates natural evolution by stochastically generating a new population in the next generation. At the end of the evolutionary process, the SR model with the best fitness will be the final solution.
In GP, candidate solutions are typically represented as parse/expression trees, an example of which is shown in Fig. 5.2. The nodes in the parse tree come from a function set Fand a terminal set T, both of which are user predefined. For GPSR, the function set Fusually consists of some basic arithmetical functions including Addition(“+”), Subtraction(“-”), Multiplication(“”) and Division(protected “/”). It can also include more advanced mathematical functions, e.g. transcendental functions and trigonometric functions. Each of these functions has a fixed number of arguments. For example, there need to be two arguments for function “” while only one argument for “Sin” (Sine) function. The terminal set Tusually consists of independent/input variablesand some random constants. The definition of the function set and the terminal set needs to meet the properties of sufficiencyand closure[14]. Sufficiency indicates the terminal set and the function set should be expressive enough to contain potential solutions, while closure requires that every function is able to take any possible output(s) of any other nodes in the tree. A typical example of satisfying closure is extending standard division to the protected “/”. If there is any division by 0 happens, protected “/” will return a predefined value of either 0 or  1 or some other soft-map function.
There are also many other forms of representations for GP including linear genetic programming[16], gene expression programming[88], Cartesian GPwith a graphicstructure [54] and Grammar-based genetic programming[53]. Among these GP variants, GP using tree representation is still most common for SR [15].
Fig. 5.2
A GP tree representing the regression model 
There are a few commonly used initialisation methods for GP to create the initial population. Growis probably the simplest initial method. In Grow, a node can be selected from either the terminal set or the function set randomly before reaching the maximal tree size/depth. The current branch will stop building when a node from the terminal set is used. Consequently, Grow usually generates trees of different sizes and shapes. Another common initialisation method for GP is the Fullmethod. This method generates GP trees with a size/depth as the predefined maximal values. Before the maximum size/depth is reached, the tree nodes consist of the operators from the function set only. A combination of the two aforementioned methods is referred to as ramped-half-and-half. Using ramped-half-and-half for initialisation, the Full and Grow methods will generate half of the initial population for GP, respectively.
The evolutionary process of GP seeks fitter individuals where a fitness function evaluated on the training set is needed to measure the performance of GP individuals. The design of the fitness measure should provide a fine-grained differentiation between candidate solutions for GP. Generally, various error measures, e.g. sum of absolute error, mean/root mean squared error, normalised mean squared errorand relative squared errorare commonly cast as the fitness function in GPSR.
Evolution in GP proceeds by transforming parents into offspring. After obtaining the performance of GP individuals, GP selects individuals as parents for breeding with a biasto solutions above average performance. Selection operatorsand genetic operatorsare needed during this procedure.
Tournament selectionis perhaps the most common selection operator in GP. tindividuals are firstly sampled, then the fittest one among these tindividuals will be selected as a parent for generating child/offspring programs. The tournament size timplements selection pressure. Typically, a larger tournament size corresponds to a higher selection pressure as more fitted individuals are likely to be selected as parents.
Crossover, mutationand reproductionare the three most common genetic operators for GPSR. With two parent individuals, crossovergenerates two new offspring individuals via swapping the genetic materials from the parents. While mutation performs on one parent only to generate a new child. It mutates a subprogram/submodel of the parent with a randomly generated one. Reproduction works by directly copying the parent.
A summary of the key points when tackling a GPSR problem is as follows:
Choose a representationscheme for GPSR and specify the elements in the terminal set and the function set. The scheme will form the regression models without any predefined model structure.
Choose the evaluationmechanism for measuring the performance of GPSR solutions. Typically, various error functions can be used, e.g. mean squared error is a commonly used one.
Determine the selectionoperator to select parents for breeding.
Determine the genetic operatorsfor breeding. Two basic genetic operators for GPSR are subtree crossoverand subtree mutation
5.2.3 Learning Classifier Systems for Regression and  Function Approximation
Learning classifier systems (LCSs)are an evolutionary machine learning technique where learning is considered an adaption process that interacts with a previously unknown environment and keeps on receiving numerical rewards in solving a given task [66]. The core of an LCSis a set of rules/classifiers which typically take the form of “IF conditionTHEN action” plus adapted parameters such as rule fitness. The goal of LCSsis to have these classifiers collectively model the decision-making process. The learning process in LCSsis often guided by EC techniques, e.g. genetic algorithms (GAs).
The XCS classifier system [84] is a Michigan-style LCS, where individual rules form the population so must cooperate to form the overall solution in a piecewise manner. XCS is designed to learn the most accurate predictions for a given input and available actions. As an enhanced variant of XCS, XCSF[85] is designed for function approximation that evolves overlapping piecewise-linear functions. Given an input vector , XCSF iteratively evolves a population of classifiers, each of which makes a linear prediction. XCSFrelies on two major learning components. The evolutionary component is designed to evolve classifiers that generate linear approximations of the function. The gradient descent-based component generates linear approximation and estimates the accuracy of the classifiers. The interaction between these two components is important for the success of XCSF
In XCSF, a classifier consists of a condition Cand a prediction Pre. The condition Cdefines a hyper rectangle with an interval of , while the prediction Prespecifies a prediction of the input vector, which is usually linear and determined by the inner product where is the input vector augmented by a constant and the weight vector . While the condition parts are evolved by a GA, the other components are iteratively approximated by a gradient-based technique. In each iteration, a match set [M], containing classifiers whose conditions match the input vector, is generated to predict the function value. More specifically, given an input vector , each classifier in [M] generates a prediction . The system prediction of the function value is given by the fitness-weighted average of the predictions of all matching classifiers. Each classifier in [M] is updated according to its error signal using the delta update rule, i.e. , where is the learning rate.
Many different variants of XCSFhave been proposed for improving its capabilities, performance and efficiency. Butz et al. [17] develop a new approach to partitioning the problem space. To this aim, they first modify the geometric interpretation toward hyper-spherical shapes, then introduce hyper-ellipsoidal conditions where a distinction in stretching for each dimension is possible. Lanzi et al. [48] extend XCSFby using a polynomial approximation for prediction modelling. They find that the quadratic and cubic approximations can increase the accuracy of XCSFand generate a set of more compact rules. Loiacono et al. [52] introduce support vector regression into XCSF. The enhanced XCSFis shown to have a more accurate prediction for higher dimensional functions and converge much faster than the original XCSFwith linear approximation. Stein et al. [69] augment XCSF with the Interpolation Component to take a set of sampling points and carry out an interpolation. The authors later [70] extend this method by utilising a Radial Basis Function to calculate the interpolated function value.
5.3 Genetic Programming for Symbolic Regression: Generalisation
Generalisationisa critical performance metric for any machine learning algorithm that measures the prediction accuracy of learned models on future unseen data. With a good generalisation capability, a learnt model not only has a good learning performance on the training data but also obtains a good prediction performance on the test data. An important goal of machine learning techniques is to learn a model that can minimise the expected error [36], where L(Y,  f(X)) is the loss functionbetween the target outputs Yand the model predictions f(X). A set of input Xand the corresponding output Yare typically sampled from an underlying distribution , where P(X) is the distribution of the input Xand the conditional distribution P(Y|X) is based on the input-output relation. To measure the expected error Err, we need the joint distribution P(X,  Y). However, in most learning tasks and real-world applications, the joint distribution is typically unknown. Thus, learning algorithms mainly rely on the empirical riskminimisation principle, which relies on choosing the candidate model with the lowest training error [36]. In many cases, the training error/empirical risk could be a good indicator of the expected test error, but this is not the case in some difficult regression tasks, e.g. when a small number of training instances do not represent the real distribution of the data well, or when the data is noisy, or when the learned model is overly complex.
In the early work of GPSR, the learning was focused on a training set only, typically model/curve fitting [18, 28], which was described in the previous section. In ML tasks, including symbolic regression, achieving good performance on an unseen test set is more important than receiving good results on the training set only. In this section, we will provide more details on how to enhance generalisation performance on unseen data via the representation, local search, model complexitymeasures, evaluation metrics and the selection operator.
5.3.1 Representation
In GPSR, an expression/parse tree is a common representation for candidate regression models where each node could be a constant, an input feature/variable, or a function/operator. The flexibilityof this representation makes it suitable to comprise a large set of possible functions. However, a notable drawback of this representation is that it may include redundant expressions, thus leading to unnecessarily complex models with poor generalisation.
Instead of working with an expression tree, Sotto et al. [29] propose to use linear GP for SR, which represents the regression models using linear programs. With an enhanced selection method to increase the number of effective codes in the programs, linear GP evolves compact SR models with a better generalisation ability. Multiple Regression Genetic Programming (MRGP) [13] proposed a new way to use the output of an expression tree. MRGP decouples the subtrees of GP trees and combines them in linear regression models for the target variable. Specifically, they utilise multiple LASSO regression, which contains the subtrees for each GP tree to approximate the target outputs. Due to its ensemble nature, MRGP is able to generate fitter and more robust regression models than traditional GP. A similar idea is presented by La Cava et al. [46], where they develop the Feature EngineeringAutomation Tool (FEAT). With FEAT, each GP individual represents a set of candidate features that are used in ridge regression. The Interaction-Transformation Evolutionary Algorithm (ITEA) [31] has a new representation named Interaction-Transformation, which does not produce overcomplex mathematical expressions and thus only allows GPSR to generate simpler regression models that could be well generalised.
GPSR works by searching a model space where models are a combination of functions/operators from a predefined function set. To ensure the model space is broad enough to cover the optimal/nearly-optimal model, the function set of GP usually contains many diverse functions, anticipating that the searching process will discover the optimal model or at least a good model. The function set typically includes the basic arithmetic operators, e.g. addition, subtraction, multiplication and the protected division. For complex SR tasks, GPSR applications could also include more advanced mathematical functions, such as trigonometric functions, exponential functions, protected logarithms and square root. Keijzer [40] demonstrated that the protected division often hinders the generalisation of GPSR as they often generate asymptotes which produce extremely large outputs when testing the learnt models on unseen data, therefore leading to a poor generalisation. Ni et al. [59] propose the Analytic Quotient (AQ) operator as a replacement for the protected division. AQ has the general properties of division, but enables regression models using it to be twice differentiable and therefore can apply regularisation techniques, thus resulting in lower generalisation error. Nicolau et al. [60] recently further investigate the detrimental effects of some other functions/operators on the generalisation of GPSR models, and propose alternative function sets that are more robust.
5.3.2 Gradient Descent Search for Learning/Optimising Constants in GP Evolved Programs
GP as an evolutionary algorithm can be seen as a kind of stochastic beam search. The search process of GPSR is guided by fitness-based selection. There are some situations where promising model structures are ignored due to low fitness caused by unfit coefficients. Gradient descent is a well-established search technique and can guarantee to find a local minimum that might not be the best solution but often meets the requirement for the given task. While weights in an artificial neural network are often adjusted by gradient descent, terminal constants in GP trees are typically randomly generated without much learning effort. This is a major “issue” in standard GPSR. Topchy et al. [72] empirically compare vanilla GP with a GP with the Ephemeral Random Constants (ERC) learning method. In their hybrid GP method, gradient descent seeks better model coefficients at the same time as GP searches for the regression model structure. The coefficients in every GP program are trained using a simple gradient algorithm at every generation. The learning effectiveness of the approach is demonstrated on several symbolic regressionproblems. Later, inspired by [86], Chen et al. [22] investigate the effectiveness of applying gradient descent with various frequencies during the learning process in GPSR and find that only applying the gradient descent to the top GP programs in every kgenerations achieve much better generalisation than vanilla GP and using the gradient descent in every program at every generation. Figure 5.3shows an example of gradient descent method for learning coefficients for a GP model with two numerical/constant terminals. It is important to note that these constant terminals here are actually model coefficients, which are being modified by gradient descent search, so are not constants any more.
Fig. 5.3
An example of GP with gradient descent for coefficient learning
5.3.3 Complexity Measures and VC Dimension
There is a group of methods that rank and select GP models with a lower estimated generalisation error during the evolutionary possess. They estimate the generalisation error using various functions of the training error, model complexityand the number of training instances, where a critical component in these methods is the measure of model complexity
Generalisation and model complexityare closely related to each other in GPSR [12, 25]. Too low a model complexitycould lead to a poor generalisation ability since the model may not fit the data adequately, while a too complex model tends to overfit the training data as the model has more than enough space for the noise and specific patterns in the training data (but not in the test set). A number of efforts have been devoted to control model complexitythus improving their generalisation in GPSR [27, 49]. These approaches can be broadly divided into two groups: controlling the structural/syntax complexity of models in GP and controlling their functional/behavioural complexity.
The structural complexity of GPSR models usually involves the size/length and the shape of the expression tree. Traditionally, structural complexity measures typically work by counting the number of structural elements, e.g. the number of nodes, leaves or layers of GP trees. The functional complexity of a model usually involves estimating the behaviour of models over the input space. There are a large number of structural complexity measures, while only a limited number of functional complexity measures have been investigated/proposed for GPSR to date. Functional complexity measures are typically more effective at enhancing the generalisation of GPSR as they are more closely linked to overfitting. However, the estimation of the functional complexity of a GPSR model is difficult thus hindering its application as a standard setting in GPSR. Estimating the generalisation ability of models via model complexityfrees GPSR from holding out a set of validation data. This is helpful when there is insufficient data available. However, these methods [21, 27, 79, 82] have not gained much attention so far.
Vladislavleva et al. [82] propose a behavioural complexity measure—order of nonlinearity for GPSR where the complexity of the model is equal to the minimum order of Chebyshev polynomials best approximating the model. A notable weakness of this complexity measure lies in the difficulty to construct the Chebyshev polynomial approximation satisfying a given accuracy and the nontrivial computational procedure in obtaining the true minimal degree of Chebyshev approximation. Vanneschi et al. [79] present a complexity measure inspired by the concept of curvature. They measure the complexity of the current best model to explore the relationship between this measure with bloat and overfitting. Chen et al. [21, 27] introduce the Vapnik-Chervonenkis (VC) dimension [80] to measure the complexity of GPSR models. They introduce a method to measure the VC dimension of GPSR models empirically by minimising the difference between the theoretical and experimental maximal deviations obtained by the model on any two datasets. The obtained VC dimension is later used in the evaluation metric of structural riskminimisation to favour complexity-simpler models. GPSR achieves a good trade-off between learning accuracy and model complexitywhen introducing the VC dimension in the fitness function of GP. Despite the impressive benefits of generalisation, it is complicated and expensive to calculate the VC dimension of GPSR models, which hinders the proposed complexity measure to be widely used in GPSR in practice. In more recent work [25, 62], another complexity measure based on Rademacher complexity is incorporated into GPSR to reduce the functional complexity. The Rademacher complexity of a GP individual is measured by the ability to fit some random Rademacher variable(s). The Rademacher complexity also has the benefit on enhancing the generalisation of GPSR but is more economic than, and can perhaps be regarded as a simulation of the VC dimension measure.
While it is important to define model complexitymeasures that are highly related to the generalisation of GPSR models, it is also crucial to ensure the measures are empirically achievable. Therefore, further investigations are desired to find an efficient way to measure model complexity, which can thus be utilised for better generalisation in GPSR.
5.3.4 Multi-objective Approaches
Multi-objective approaches for increasing the generalisation ability of GPSR typically minimise the training error and a second objective reflecting the generalisation error [42, 68]. Commonly, this could be the model size or the model complexityas discussed in the previous section. Figure 5.4shows an example of a front of GP individuals with two objectives of error and complexity. As the model complexity/size is explicitly defined as an objective, the evolved models are either more parsimonious or simpler compared to those generated by the vanilla single-objective GPSR method thus generalising better.
Fig. 5.4
An example of a front of models evolved by multi-objective GP. Among the six models, Ind 2, which has the best balance between the two objectives, is typically considered to be the best model
Kommenda et al.[42] propose a multi-objective GPSR method where an efficient complexity measure that includes both the syntactical and the semantic information of the model is treated as another objective. Their method has been shown to evolve simpler and better generalised GPSR models.
Chen et al. [24] develop a new evaluation measure based on the correlation between variables and residuals of GPSR models to prevent learning from noisy data. They introduce a powerful statistical association measure, Maximal Information Coefficient, to identify the correlation between variables and residuals. Their multi-objective GPSR method, which minimises both the training error and the correlation between residuals and variables, has a good generalisation capacity and evolves compact regression models.
5.3.5 Lexicase Selection
Lexicase selection[38] is a parent selection method that considers the error of GP models on one training instance in a random order at a time instead of the common fitness value aggregated over all the training instances. Figure 5.5shows three examples of lexicase selection. The original lexicase selection performs well in discrete error spaces but not in continuous spaces. A new form of lexicaseselection, -lexicase selection, has been developed for GPSR [47]. The new lexicase selectionredefines the pass condition for individuals on each examination and makes it less strict in GPSR with continuous error spaces. -lexicase selectionwith automatic threshold adaptation has been shown to promote population diversityand make use of more fitness cases when selecting parents thus enhancing the performance of GPSR.
Fig. 5.5
Examples of lexicase selection
5.4 Semantic GP for Regression
The semantics ofcomputer programs provides grounding for their syntax [45], more specifically, the semantics of programs determine their behaviours. But most GP algorithms manipulate programs only on the structure level without considering their semantics. Considering program/model structure alone allows GP to rely on simple search operators. However, this comes at a cost that the genotype-phenotype mappingin GP is complicated. It is usually hard to predict how the modification of the structure of a program will impact its output. A small change in the program structure can have a profound impact on its semantics and thus its fitness. This causes the fitness landscape of GP to be complex and limits its scalability [45].
In recent years, much research was devoted to investigating the importance of semantics in GP and forming a “new” research track of Semantic GP(SGP). Although it is still young, the obtained results of the research track so far are promising and encouraging.
5.4.1 Semantics in GP
Developing semantic GPmethods first requires a formal definition of program semantics. Krawiec et al. [45] give a formal definition for the semantics of a GP program based on the given training instances which specify the desired behaviour of GP programs. They define the semantics of a GP program as a vector, each element of which is the corresponding output of the program produced on each input/training instance. An example of semantics of GP based on this definition is shown in Fig.  5.6
Fig. 5.6
An example of semantics in GPSR
The semantics can then be viewed as a point in an N-dimensional space, which is referred to as semantic space, where Nis the number of fitness cases, i.e. number of training instances. Since fitness cases in GPSR could not enumerate all the possible inputs, researchers also call these semantics-sampling semantics[76].
Many semantic GPSR methods have been proposed since 2009 [61, 76]. Basically, these semantic GPmethods still act on the structure of programs either at the individual or sub-individual level, but rely on specific genetic operators or semantic survival criteria to achieve the desired semantic requirements indirectly.
Nguyen et al. [76] explore the effect of semantic guidance for crossover in GPSR. They proposed a new semantic aware crossover (SAC) that requires exchanging subtrees having equivalence on their approximate semantics. To determine the semantic equivalence of two expressions, they propose sampling semantics, which consists of the evaluation results of the expressions on a random set of points sampled from the domain. Two subtrees are designated as semantically equivalent if their output on the random sample set is close enough according to the semantic sensitivity. Nguyen et al. [76, 77] later extend and improve SAC by introducing semantic similarity-based crossover (SSC), which encourages exchanges of subtrees with not wildly different semantics. SSC forces a change in the semantics of GPSR programs in the population but keeps the change bounded. In this way, SSC helps to improve locality. However, it can also lead to the loss of diversityin some cases. Thus, a multi-population SSC approach [61] was developed to maintain population diversity. The experimental results showed that the multi-population method further improves the results obtained by SSC. [75] investigates the impact of incorporating semantic awareness into GP on its generalisation ability.
5.4.2 Geometric Semantic GP
Krawiec et al. [44] propose the approximately geometric crossover (AGC), where the semantic distance between the candidate child and the parents in the semantic space is used to guide the crossover operator. AGC generates a number of child programs. Among them, only the offspring whose semantics are most similar to those of their parents will survive. Another notable finding is that geometric crossover breeds offspring that are guaranteed to be not worse than the worst parent, while geometric mutation induces a unimodal fitness landscape. However, there is no clear or easy way to explore these properties due to the complex genotype-phenotype mappinginGP.
Fig. 5.7
An example of geometric semantic operators for generating a child program s(o) under the Euclidean metric
Moraglio et al. [56] propose the geometric semantic GPmethod (GSGP) introducing the precise genetic operators that search directly in the semantic space. In GSGP for SR, geometric semantic crossover on two parents , generates the offspring , where is a random real function with the output in the range of [0,  1]. Geometric semantic mutation of an individual Pcreates the individual ), where and are two random real functions and ms—the mutation step is a constant to decide the perturbation caused by mutation. Figure 5.7shows an example of geometric semantic operators under the Euclidean metric. The precise geometric semantic operators are promising as they are able to induce a unimodal fitness landscape. However, they have an obvious limitation of leading to rapid growth in the size of individuals. The precise geometric semantic operators generate offspring consisting of the complete structure of the parent(s), the random tree(s) and some additional operators. After a few generations, fitness evaluation becomes extremely expensive, thus the population is unmanageable. Moraglio et al. [56] suggest adding an automatic simplification step in each generation. Vanneschi et al. [78] propose an improved method to implement the geometric semantic operators where they do not generate the programs explicitly during the evolutionary process. Instead, in their implementation, only the materials that are needed to reconstruct the programs are stored in memory. These include the individuals in the initial population, the involved random trees and a set of memory pointers to previous generations of individuals.
5.4.3 Angle-Aware Geometric Semantic Operators
GSGPpresents a solid theoretical framework for designing precise geometric search operators. Utilising the geometry of the semantic space could be helpful for enhancing search in GP. However, it is difficult to realise efficient precise geometric search operators that search directly in the semantic space. Moreover, there are some potential limitations on the effectiveness of geometric semantic operators. Geometric semantic crossover can generate children outperforming both parents onlywhen the target semantics are between the semantics of parents. In addition, geometric semantic crossover becomes less effective with a large number of data instances that lead to a high-dimensional semantic space. For geometric semantic mutation, the search performance is sensitive to the mutation step. It is always difficult to bound the variation of this mutation operator. Small mutation steps decrease the explorationability of GP, thus leading to underfitting, while too big a mutation step could lead to overfitting, in turn leading GSGP to generalise poorly on unseen data.
Chen et al. [19] develop an angle-aware mating scheme for geometric semantic crossover. Given a set of candidate parents selected by tournament selection, with the angle-aware mating scheme, the geometric crossover will perform on two parents with the largest angle distance between their relative semantics to the target. The large angle distance benefits the effectiveness of crossover by reducing semantic duplication among the child programs and also their parents. Moreover, the mating scheme also helps foster the explorationability of GP and leads to faster convergence to the target semantics. However, along with the evolutionary process in GPSR, the candidate parents become closer to each other in the semantic space, thus it is extremely difficult to find parents with a large angle distance at the final stage of the evolutionary process.
To further explore the geometric properties of the semantic space and obtain a greater generalisation gain for GPSR, Chen et al.[23] later propose angle-driven geometric semantic GP(ADGSGP). To further utilise the angle-awareness mating, an angle-driven selection (ADS) for geometric crossover is proposed. The new selection operator selects pairs of parents that have a large angle distance in the semantic space from the GP population directly without utilising the tournament selection. In addition, two new geometric semantic operators are also developed, perpendicular crossover (PC) and random segment mutation (RSM). PC further explores the geometric properties of the parent semantics anticipating to eliminate the ineffectiveness of the original geometric semantic crossover. While RSM aims to make it easy to determine the mutation step, it works by generating a child program highly correlated to its parent and approximating the target semantics considering the right direction toward it. To fulfil the semantics of the new geometric operators, they develop a new algorithm named semantic context replacement (SCR). SCR addresses the potential drawbacks of a previous method, semantic backpropagation (SB), which is sensible but generally produces overly complex programs. They find that GSGP methods, including ADGSGP, usually generate complex models but generalise well. This conflicts with the common observation that complex programs usually overfit the training data. A possible reason for the generalisation gain of these complex models could be that they exhibit the same strength as ensembles. Ensemble learning tackles the generalisation problem by combining a number of accurate models to reduces the possibility of overfitting. The negative contribution of the overfitted models will be eliminated by models that generalise well in the final solution. For an effective ensemble learning, a necessary condition is that the ensemble has a mix of accurate and diverse models [89]. GSGP can be seen as an ensemble learning method, since geometric genetic operators always combine parent individuals and some randomly generated individuals to produce new individuals. These parents can be seen as the accurate models while the randomly generated individuals can be treated as providing the necessary diversity. The competitive generalisation of GSGP may derive a combination of these models, which can be treated as the ensembles.
5.5 GP for Regression with Incomplete Data
Data incompleteness isa pervasive problem in machine learning, of course including GPSR. There are three main categories of missing values according to how the missingness forms. They are missing completely at random (MCAR), missing not at random (MNAR) and missing at random (MAR) [35]. In MCAR, a random subset of the given data is missing for a completely random reason. The probability of a missing value is not related to any other values. For MCAR, most techniques for handling missingness give unbiased results since no gain can be guaranteed by analysing the available data for estimating missing values. For MNAR, the probability that a missing value is related to some non-observed information. There is no universal method to handle this type of missingness due to the unavailable of valuable information. Usually, missing values are neither MCAR nor MNAR. MAR refers to the probability of missingness depending on other existing values. Methods for handling missing values can be categorised into three different groups. They are:
Delete incomplete instances and learn from the complete data portion only. This is a simple approach but may suffer from losing informative data;
Impute incomplete data and learn using a complete data set after imputationand
Directly deal with the original incomplete data without imputation
In the rest of this section, we will mainly present the latter two groups.
5.5.1 Imputation Methods
Data imputationis a popular approach to handling data incompleteness. It estimates missing values based on existing data. The imputed data can then be used by any learning algorithm. There are two main types of imputation: single imputation and multiple imputation [35]. Single imputation directly replaces missing values with estimated ones based on observed data. The common problem with single imputation is that it ignores uncertainty and underestimates the variance. Multiple imputationaddresses this problem by considering both within-imputation uncertainty and between-imputationuncertainty.
Data imputationapproaches can also be categorised as data-driven approaches and regression-based approaches [35]. Data-driven imputation is typically based on instance similarity. A popular data-driven imputation method is K nearest neighbour (KNN)-based imputation. Another type of imputation method is based on feature predictability where regression algorithms are utilised to predict incomplete features. GPSR-based imputation is a typical example of this kind. Instance similarity-based imputation methods utilise the relationships between instances, while feature predictability-based imputation methods employ the relationships between features. However, both the instance similarity-based approach and the feature predictability-based approach undermine the other’s factor. There is a desire for a new approach to combining the benefits from both of them.
Vladislavleva et al. [81] treat missing values in a certain range of the feature space as an unbalanced problem. They address the problem with an automatic instance weighting method according to the proximity, remoteness, surrounding and nonlinear deviation of the instances from their nearest neighbours. Al-Helali et al. [2] develop a hybrid imputation method called GP-KNN for GPSR with missing values. GP-KNN utilises the KNN-based instances proximity while employing the GP-based feature regression predictability. GPSR builds regression-based imputationmodels, which learn from similar instances selected by KNN. GP-KNN outperforms many popular imputation methods, e.g. KNN, random forest (RF), and classification and regression trees (CART). However, GP-KNN has a notable limitation of the need to build imputation modelsfor imputing new incomplete instances, which can be computationally expensive. To address this limitation, Al-Helali et al. [10] extend GP-KNN by combining a weighted KNN (WKNN) with GP. The weighted KNN is used to increase the contributions of instances that are close to the incomplete instances when building the imputationmodels. GP imputation models learn from the instances retrieved by WKNN. To improve the efficiency, the regression models constructed in the training process are used on new unseen data with missing values, to avoid the expensive computational costof building new imputationmodels.
5.5.2 Interval Functions
Anotherapproach to dealing with incomplete datais to build learning algorithms that can directly work with missing values. Some attempts have been made for utilising interval arithmeticto enable GP to learn directly from incomplete data.
Interval arithmetic refers to dealing with intervals that are sets of real numbers defined by . Four basic interval arithmetic operators are
;
;
;
The original idea of incorporating interval arithmetic into GP is from Keijzer [40], who presents the idea of replacing protected operators by using the known ranges of input features to compute the expected output interval of a GPSR model. Since the introduction of using interval arithmetic in GP, it has been used primarily for the evaluation of GPSR models. Later, Dick [32, 33] explores the potential of interval arithmetic for an effective GP search. The execution interval of the models based on the known interval of the input variablesis obtained and checked. Only models with a valid execution interval could be evaluated normally. Otherwise, they will be assigned the lowest fitness. Tran et al. [73, 74] use GP with interval functionsto construct features [74] and classifiers [73] for classification tasks with incomplete data. Al-Helali et al. [1] develop a feature selectionmethod based on the interval functionmethod for GPSR on incomplete regression data. In their method, missing values are replaced by an interval of the feature estimated based on its minimum and maximum of existing values, e.g. the original instances with missing values (?,  2.2,  1.3) and (?,  ?,  3.3) where the interval of the three features corresponding to , and would be converted to the interval-valued instances and . With interval arithmetic operators, the proposed GP for feature selectionmethod works on intervals instead of numerical values and is able to select informative features and predictors for imputation. The selected incomplete features are then imputed and fed into the GPSR learning process.
Fig. 5.8
An example of GP with hybrid tree-vector representation to perform symbolic regression and instance selection simultaneously on incomplete data
(Adapted from [8])
5.5.3 Instance Selection and Feature Selection
Instance selection is a process to find a set of useful data instances to improve learning performance. It is typically very useful for discarding noisy and outlier instances, thus improving the effectiveness and efficiency of learning algorithms. However, instance selection methods usually use algorithms that work only on complete data such that very few of them are applicable to incomplete data
Al-Helali et al. [4, 8] integrate instance selection into data imputationfor GPSR on incomplete data. To this aim, they investigate using KNN for instance selection for imputation [4], and later propose a tree-vector mixed representation for GP to perform instance selection and symbolic regressionsimultaneously on incomplete data. Figure 5.8shows an example of the tree-vector mixed representation for GP. Using this representation, each individual has two components, i.e. an expression tree and a bit vector. The tree component constructs GPSR models, while the bit vector represents selected instances that are used in the imputation method. The proposed method enhances the imputationperformance of the WKNN method in both effectiveness and efficiency.
High dimensionality data is a challenge for regression, which is even more difficult on incomplete data. To address this problem in GPSR, Al-Helali et al. [3, 5] explore incorporating feature selectionpressure into the fitness function of GP in the form of the ratio of selected features [3] and the model complexity[5]. They develop a regularised GP method [5], where the complexity measure based on the Hessian matrix of the phenotype of GP models is utilised as a regulariser for feature/predictor selection for imputation
5.5.4 Transfer Learning
Transfer learning, which reuses knowledge learned from the source domain(s) to improve the learning in the target domain(s), has been considered a key solution for the problem of learning when lacking knowledge in the target domain(s). Incompleteness is one of the main causes of knowledge shortage in many machine learning tasks. However, transfer learning has seldom been used to address this issue. Al-Helali et al. [6] propose a multi-tree GP-based feature transformation method for transferring data from a complete source domain to a different, incomplete target domain. This method is enhanced by reducing the source-target domain distribution dissimilarity in [7]. Later, the same authors further extend these methods [9] to utilise the impact of the source features and instances on SR in the source domain to improve the transformation, as shown in Fig. 5.9
Fig. 5.9
An example of GP for feature construction to minimise the distribution mismatching between the source domain and the target domain with incomplete data
5.6 Interpretable GP/Evolving Interpretable Models for  Regression
Interpretable regression models, which describe everything explicitly in a mathematical form, could provide a good insightinto the decision-making process. Due to its symbolic nature, GP has inherent interpretability. Typically, with a tree-based representation, a GP program is explicitly represented as an expression tree, which is often consistent with how humans understand a structure of a computer program in a tree representation. Many other GP variants, either with a linear representation or with a graph representation, are also interpretable, e.g. linear GP represents a program as a sequence of instructions, which is essentially equivalent to human-written programs (e.g. in C, Java and Python). An interpretable regression model should be accurate at predicting the target variable, while at the same time being as simple as possible to facilitate the interpretation of the decision-making process and the discovery of new knowledge from the data.
5.6.1 Representation
França et al. [30] present a new representation named Interaction-Transformation (IT) for GPSR, which constrains the search space. A regression model with IT representation approximates the target model in a form of where s are the coefficients, and the composition function , is a one-dimensional transformation function, and is a d-dimensional interaction function. This representation does not comprehend the description of a more complicated set of mathematical expressions thus preventing GPSR from finding overly complex regression models that are difficult to interpret. Raymond et al. [63] propose a new representation for GP named adaptive weighted splines, which uses a fixed model structure. With adaptive weighted splines, each GP model is composed of a number of feature splines. Each feature in the input space is represented by a smoothing spline with two coefficients determining its influence on the final prediction of the model. This semi-structured GP method no longer fully presents the symbolic capability of GP, but the models developed can be regulated far more easily and also more interpretable.
