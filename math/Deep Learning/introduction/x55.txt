behaves.
5.2
L1 and L2 Regularization
As we have noted earlier, regularization means adding a term to the error function,
so we have:
Eimproved := Eoriginal + RegularizationT erm
As one might guess, adding different regularization terms give rise to different
regularization techniques. In this book, we will address the two most common types
of regularization, L1 and L2 regularization. We will start with L2 regularization and
explore it in detail, since it is more useful in practice and it is also easier to grasp
the connections with vector spaces and the intuition we developed in the previous
section. Afterwards we will turn briefly to L1 and later in this chapter we will address
dropout which is a very useful technique unique to neural networks and has effects
similar to regularization.
L2 regularization is known under many names, ‘weight decay’, ‘ridge regression’,
and ‘Tikhonov regularization’. L2 regularization was first formulated by the Soviet
mathematician Andrey Tikhonov in 1943 [1], and was further refined in his paper [2].
The idea of L2 regularization is to use the L2 or Euclidean norm for the regularization
term.
The L2 norm of a vector x = (x1, x2, . . . , xn) is simply
�
x2
1 + x2
2 + . . . + x2n.
The L2 norm of the vector x can be denoted by L2(x) or, more commonly, by ||x||2.
The vector used is the weights of the final layer, but a version using all weights in
the network can also be used (but in that case, our intuition will be off). So now we
can rewrite the preliminary L2-regularized error function as:
Eimproved := Eoriginal + ||w||2
But, in the machine learning community, we usually do not use the square root,
so instead of ||w||2 we will use the square of the L2 norm, i.e. (||w||2)2 = ||w||2
2
which is actually just �
w w2. We will also want to add a hyperparameter to be able
to adjust how much of the regularization we want to use (called the regularization
parameter or regularization rate, and denoted by λ), and divide it by the size of the

110
5
Modifications and Extensions to a Feed-Forward Neural Network
batch used (to account for the fact that we want it to be proportional), so the final
L2-regularized error function is:
Eimproved := Eoriginal + λ
n ||w||2
2 = Eoriginal + λ
n
�
wiinwo
w2
i
Let us work a bit on the explanation1 what L2 regularization does. The intuition
is that during the learning procedure, smaller weights will be preferred, but larger
weights will be considered if the overall decrease in error is significant. This explains
why it is called ‘weight decay’. The choice of λ determines how much will small
weights be preferred (when λ is large, the preference for small weights will be great).
Let us work through a simple derivation. We start with our regularized error function:
Enew = Eold + λ
n
�
w
w2
By taking the partial derivatives of this equation we get:
∂Enew
∂w
= ∂Eold
∂w
+ λ
n w
Taking this back to the general weight update rule we get:
wnew = wold − η · (∂Eold
∂w
+ λ
n w)
One might wonder whether this would actually make the weights converge to 0,
but this is not the case, since the first component ∂Eold
∂w
will increase the weights if
the reduction in error (this part controls the unregularized error) is significant.
We can now proceed to briefly sketch L1 regularization. L1 regularization, also
known as ‘lasso’ or ‘basis pursuit denoising’ was first proposed by Robert Tibshirani
in 1996 [4]. L1 regularization uses the absolute value instead of the squares:
Eimproved := Eoriginal + λ
n ||w||1 = Eoriginal + λ
n
�
wiinwo
|wi|
Let us compare the two regularizations to expose their peculiarities. For most
classification and prediction problems, L2 is better. However, there are certain tasks
where L1 excels [5]. The problems where L1 is superior are those that contain a
lot of irrelevant data. This might be either very noisy data, or features that are not
informative, but it can also be sparse data (where most features are irrelevant because
1We will be using a modification of the explanation offered by [3]. Note that this book is available
online at http://neuralnetworksanddeeplearning.com.

5.2
L1 and L2 Regularization
111
they are missing). This means that there are a number of useful applications of L1
regularization in signal processing (e.g. [6]) and robotics (e.g. [7]).
