∂E
∂wi
=
�
n
∂y(n)
∂wi
∂E
∂y(n) = −
�
n
x(n)
i
y(n)(1 − y(n))(t(n) − y(n))
Note that this is very similar to the delta rule for the linear neuron, but it has also
y(n)(1 − y(n)) extra: this part is the slope of the logistic function.
4.6 Backpropagation
So far we have seen how to use derivatives to learn the weights of a logistic neuron,
and without knowing it we have already made excellent progress with understanding
backpropagation, since backpropagation is actually the same thing but applied more
than once to ‘backpropagate’ the errors through the layers. The logistic regression
(consisting of the input layer and a single logistic neuron), strictly speaking, did
not need to use backpropagation, but the weight learning procedure described in the
previous section actually is a simple backpropagation. As we add layers, we will
not have more complex calculations, but just a large number of those calculations.
Nevertheless, there are some things to watch out for.
We will write out all the necessary details for backpropagation for the feedforward
neural networks, but first, we will build up the intuition behind it. In Chap.2 we
have explained gradient descent, and we will revisit some of the concepts here as

94
4
Feedforward Neural Networks
needed. Backpropagation of errors is basically just gradient descent. Mathematically
speaking, backpropagation is:
wupdated = wold − η∇E
where w is the weigh, η is the learning rate (for simplicity you can think of it just
being 1 for now) and E is the cost function measuring overall performance. We could
also write it in computer science notation as a rule that assigns to w a new value:
w ← w − η∇E
This is read as ‘the new value of w is w minus η∇E’. This is not circular,13 since
it is formulated as an assignment (←), not a definition (= or :=). This means that
first, we calculate the right-hand side, and then we assign to w this new value. Notice
that if were to write out this mathematically, we would have a recursive definition.
We may wonder whether we could do weight learning in a more simple man-
ner, without using derivatives and gradient descent.14 We could try the following
approach: select a weight w and modify it a bit and see if that helps. If it does, keep
the change. If it makes things worse, then change it in the opposite direction (i.e.
instead of adding the small amount from the weight, subtract it). If this makes it
better keep the change. If neither change improves the final result, we can conclude
that w is perfect as it is and move to the next weight v.
Three problems arise right away. First, the process takes a long time. After the
weight change, we need to process at least a couple of training examples for each
weight to see if it is better or worse than before. Simply speaking, this is a compu-
tational nightmare. Second, by changing the weights individually we will never find
out whether a combination of them would work better, e.g. if you change w or v
separately (either by adding the small amount or subtracting to one or the other), it
might make the classification error worse, but if you were to change them by adding a
small amount to both of them it would make things better. The first of these problems
will be overcome by using gradient descent,15 while the second will be only partially
resolved. This problem is usually called local optima.
The third problem is that near the end of learning, changes will have to be small,
and it is possible that the ‘small change’ our algorithm test will be too large to
successfully learn. Backpropagation also has this problem, and it is usually solved
by using a dynamic learning rate which gets smaller as the learning progresses.
13A definition is circular if the same term occurs in both the definiendum (what is being defined)
and definiens (with which it is defined), i.e. on both sides of = (or more precisely of :=) and in
our case this term could be w. A recursive definition has the same term on both sides, but on the
defining side (definiens) it has to be ‘smaller’ so that one could resolve the definition by going back
to the starting point.
14If you recall, the perceptron rule also qualifies as a ‘simpler’ way of learning weights, but it had
the major drawback that it cannot be generalized to multiple layers.
15Although it must be said that the whole field of deep learning is centered around overcoming the
problems with gradient descent that arise when using it in deep networks.

4.6
Backpropagation
95
If we formalize this approach we will get a method called finite difference approx-
imation16:
1. Each weight wi, 1 ≤ i ≤ k is adjusted by adding to it a small constant ε (e.g.
whose value is 10−6) and the overall error (with only wi changed) is evaluated
(we will denote this by E+
i )
2. Change back the weight to its initial value wi and subtract ε from it and reevaluate
the error (this will be E−
i )
3. Do this for all weights wj, ≤ j ≤ k
4. Once finished, the new weights will be set to wi ← wi − E+
i −E−
i
2ε
The finite difference approximation does a good job in approximating the gradient,
and nothing more than elementary arithmetic is used. If we recall what a derivative is
and how it is defined from Chap.2, the finite difference approximation makes sense
even in terms of the ‘meaning’ of the procedure. This method can be used to build
up the intuition how weight learning proceeds in full backpropagation. However,
most current libraries which have tools for automatic differentiation perform gradi-
ent descent in a fraction of the time it would take to compute the finite difference
approximation. Performance issues aside, the finite difference approximation would
