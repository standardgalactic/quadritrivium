1. A.M. Turing, On computable numbers, with an application to the entscheidungsproblem. Proc.
Lond. Math. Soc. 42(2), 230–265 (1936)
2. V. Peckhaus, Leibniz’s influence on 19th century logic. in The Stanford Encyclopedia of Phi-
losophy, ed. by E.N. Zalta (2014)
3. J.S. Mill, A System of Logic Ratiocinative and Inductive: Being a connected view of the Prin-
ciples of Evidence and the Methods of Scientific Investigation (1843)
4. G. Boole, An Investigation of the Laws of Thought (1854)
5. A.M. Turing, Computing machinery and intelligence. Mind 59(236), 433–460 (1950)
6. R. Carnap, Logical Syntax of Language (Open Court Publishing, 1937)
7. A.N. Whitehead, B. Russell, Principia Mathematica (Cambridge University Press, Cambridge,
1913)
8. J.Y. Lettvin, H.R. Maturana, W.S. McCulloch, W.H. Pitts, What the frog’s eye tells the frog’s
brain. Proc. IRE 47(11), 1940–1959 (1959)
9. N.R. Smalheiser, Walter pitts. Perspect. Biol. Med. 43(1), 217–226 (2000)
10. A. Gefter, The man who tried to redeem the world with logic. Nautilus 21 (2015)
11. F. Rosenblatt, Principles of Neurodynamics: perceptrons and the theory of brain mechanisms
(Spartan Books, Washington, 1962)
12. F. Rosenblatt, Recent work on theoretical models of biological memory, in Computer and
Information Sciences II, ed. by J.T. Tou (Academic Press, 1967)
13. S. Russell, P. Norvig, Artificial Intelligence: A Modern Approach, 3rd edn. (Pearsons, London,
2010)
14. H. Moravec, Mind Children: The Future of Robot and Human Intelligence (Harvard University
Press, Cambridge, 1988)
15. M. Minsky, S. Papert, Perceptrons: An Introduction to Computational Geometry (MIT Press,
Cambridge, 1969)
16. L.R. Graham, Science in Russia and the Soviet Union. A Short History (Cambridge University
Press, Cambridge, 2004)
17. S. Pinker, The Blank Slate (Penguin, London, 2003)
18. B.F. Skinner, The Possibility of a Science of Human Behavior (The Free House, New York,
1953)
19. E.L. Gettier, Is justified true belief knowledge? Analysis 23, 121–123 (1963)
20. T.S. Kuhn, The Structure of Scientific Revolutions (University of Chicago Press, Chicago, 1962)
21. N. Chomsky, Aspects of the Theory of Syntax (MIT Press, Cambridge, 1965)
22. N. Chomsky, A review of B. F. Skinner’s verbal behavior. Language 35(1), 26–58 (1959)
23. A. Newell, J.C. Shaw, H.A. Simon, Elements of a theory of human problem solving. Psychol.
Rev. 65(3), 151–166 (1958)
24. J. Lighthill, Artificial intelligence: a general survey, in Artificial Intelligence: A Paper Sympo-
sium, Science Research Council (1973)
25. Paul J. Werbos, Beyond Regression: New Tools for Prediction and Analysis in the Behavioral
Sciences (Harvard University, Cambridge, 1975)

16
1
From Logic to Cognitive Science
26. D.B. Parker, Learning-logic. Technical Report No. 47 (MIT Center for Computational Research
in Economics and Management Science, Cambridge, 1985)
27. Y. LeCun, Une procédure d’apprentissage pour réseau a seuil asymmetrique. Proc. Cogn. 85,
599–604 (1985)
28. D.E. Rumelhart, G.E. Hinton, R.J. Williams, Learning internal representations by error propa-
gation. Parallel Distrib. Process. 1, 318–362 (1986)
29. J.J. Hopfield, Neural networks and physical systems with emergent collective computational
abilities. Proc. Natl. Acad. Sci. USA 79(8), 2554–2558 (1982)
30. N. Cristianini, J. Shawe-Taylor, An Introduction to Support Vector Machines and Other Kernel-
based Learning Methods (Cambridge University Press, Cambridge, 2000)
31. S. Hochreiter, J. Schmidhuber, Long short-term memory. Neural Comput. 9(8), 1735–1780
(1997)
32. Y. LeCun, L. Bottou, Y. Bengio, P. Haffner, Gradient-based learning applied to document
recognition. Proc. IEEE 86(11), 2278–2324 (1998)
33. G.E. Hinton, S. Osindero, Y.-W. Teh, A fast learning algorithm for deep belief nets. Neural
Comput. 18(7), 1527–1554 (2006)
34. J. Schmidhuber, Deep learning in neural networks: an overview. Neural Netw. 61, 85–117
(2015)
35. P. Domingos, The Master Algorithm: How the Quest for the Ultimate Learning Machine Will
Remake Our World (2015)
36. M.S. Gazzanga, R.B. Ivry, G.R. Mangun, Cognitive Neuroscience: The Biology of Mind, 4th
edn. (W. W. Norton and Company, New York, 2013)
37. A. Santos, Limitations of prompt-based training. J. Appl. Companion Anim. Behav. 3(1), 51–55
(2009)
38. J. Fodor, Z. Pylyshyn, Connectionism and cognitive architecture: a critical analysis. Cognition
28, 3–71 (1988)
39. T. Mikolov, T. Chen, G. Corrado, J, Dean, Efficient estimation of word representations in vector
space, in ICLR Workshop (2013), arXiv:1301.3781

2
Mathematical and Computational
Prerequisites
2.1 Derivations and Function Minimization
In this chapter, we give most of the mathematical preliminaries necessary to under-
stand the later chapters. The main engine of deep learning is called backpropagation
and it consists mainly of gradient descent, which is a move along the gradient, and
the gradient is a vector of derivations. And the first section of this chapter is about
derivations, and by the end of it, the reader should know what a gradient is and what
is gradient descent. We will not return to this topic, but we will make heavy use of
it in all the remaining chapters of this book.
One basic notational convention we will be using is ‘:=’; ‘A := xy’ means ‘We
define A to be xy’, or ‘xy is called A’. This is called naming xy with the name A. We
take the set to be the basic mathematical concept as most other concepts can be build
upon or explained by using sets. A set is a collection of members and it can have both
other sets and non-sets as members. Non-sets are basic elements called urelements,
such as numbers or variables. A set is usually denoted with curly braces, so for
example A := {0, 1, {2, 3, 4}} is a set with three members containing the elements
0, 1 and {2, 3, 4}. Note that {2, 3, 4} is an element of A, not a subset. A subset of A
would be for example {0, {2, 3, 4}}. A set can be written extensionally by naming the
members such as {−1, 0, 1} or intensionally by giving the property that the members
must satisfy, such as {x|x ∈ Z ∧ |x| < 2} where Z is the set of integers and |x| is the
absolute value of x. Notice that these two denote the same set, since they have the
same members. This principle of equality is called the axiom of extensionality, and
it says that two sets are equal if and only if they have the same members. This means
that {0, 1} and {1, 0} are equal, but also {1, 1, 1, 1, 0} and {0, 0, 1, 0} (all of them
have the same members, 0 and 1).1
