morning
no
afternoon no
afternoon yes
afternoon yes
morning
yes
Time
yes no TOTAL
morning
3
2
5
afternoon 4
1
5
evening
2
1
3
TOTAL
9
4
13
is
5
13 = 0.3846. The probability of ‘afternoon’ is
5
13 = 0.3846. The probability of
‘evening’ is 3
13 = 0.2307. Ok, that takes care of all the probabilities which we can
calculate just by counting from the dataset (the so-called ‘priors’ we addressed in
Sect.2.3 of Chap.2). We will be needing one more thing but we will get to it.
Imagine now we are given a new case for which we do not know the target label
and we must predict it. This new case is the row vector (morning)12 and we want
to know whether it is a ‘yes’ or a ‘no’, so we need to calculate
P(yes|morning) = P(morning|yes)P(yes)
P(morning)
We can plug in the priors P(yes) = 0.6923 and P(morning) = 0.3846 we calcu-
lated above. Now, we only need to calculate P(morning|yes), which is the percent-
age of times the ‘morning’ occurs if we restrict ourselves to the rows which have
‘yes’, which is present 9 times, and out of these, three have also a ‘yes’, so we have
P(morning|yes) = 3
9 = 0.3333. Taking it all to Bayes’ theorem, we have
P(yes|morning) = P(morning|yes) · P(yes)
P(morning)
= 0.3333 · 0.6923
0.3846
= 0.5999
12If we were to have n features, this would be an n-dimensional row vector such as (x1, x2, . . . , xn),
but now we have only one feature so we have a 1D row vector of the form (x1). A 1D vector is
exactly the same as the scalar x1 but we keep referring to it as a vector to delineate that in the general
case it would be an n-dimensional vector.

3.3
A Simple Classifier:Naive Bayes
61
We also know that P(no|morning) = 1 − P(yes|morning) = 0.4. This means
that the datapoint gets the label ‘yes’, since the value is over 0.5 (we have two classes).
In general, if we were to have n classes, 1
n is the value over which the probability
would have to be.
The diligent reader could say that we could have calculated P(yes|morning)
directly from the table as we did with P(morning|yes), and this is true. The problem
is that we can do it by counting from the table only if there is a single feature, so
for the case of multiple features we would have to use calculation we actually used
(with the expanded formula for multiple features).
Naive Bayes is a simple algorithm, but it is still very useful for large datasets. In
fact, if we adopt a probabilistic view of machine learning and claim that all machine
learning algorithms actually learn only P(y|x), we could say that naive Bayes is the
simplest machine learning algorithm, since it has only the bare necessities to make
the ‘flip’ from P( f |t) to P(t| f ) work (from counting to predicting). This is a specific
(probabilistic) view of machine learning, but it is compatible with the deep learning
mindset, so feel free to adopt it as a pet.
One important thing to remember is that naive Bayes makes the conditional inde-
pendence assumption.13 So it cannot handle any dependencies in the features. Some-
times, we might want to be able to model sequences like this, e.g. when the order
of the feature matters (we will see this come into play for language modelling or
for sequences of events in time), and naive Bayes is unable to do this. Later in the
book, we will present deep learning models fully capable of handling this. Before
continuing on, notice that the naive Bayes classifier had to draw a hyperplane to be
able to classify the new datapoints. Suppose we had a binary classification at hand.
Then, naive Bayes expanded the space by one dimension (so the row vectors are
augmented to include this value), and that dimension accepts values between 0 and
1. In this dimension, the hyperplane is visible and it passes through the value 0.5.
3.4 A Simple Neural Network: Logistic Regression
Supervised learning is usually divided into two types of learning. The first one is
classification, where we have to predict the class. We have seen that already with
naive Bayes, we will see it again countless times in this book. The second one is
regression where we predict a value, and we will not be exploring regression in
this book.14 In this section, we explore logistic regression which is not a regression
algorithm but a classification algorithm. The reason behind this is that it is considered
a regression model in statistics and the machine learning community just adopted it
and began using it as a classifier.
13That is, the assumption that features are conditionally independent given the target.
14Regression problems can be simulated with classification. An example would be if we had to find
the proper value between 0 and 1, and we had to round it in two decimals, then we could treat it as
a 100-class classification problem. The opposite also holds, and we have actually seen this in the
naive Bayes section, where we had to pick a threshold over which we would consider it a 1 and
below which it would be a 0.
