f (x) = sinx · x5.
14These rules are not independent, since both ChainExp and Exp are a consequence of
CHAINRULE.

2.1 Derivations and Function Minimization
25
• Const: Constant rule c′ = 0.
• ChainExp: Chain rule for exponents [ef (x)]′ = ef (x) · f ′(x).
• DerDifVar: Deriving the differentiation variable dy
dz z = 1.
• Exp: Exponent rule [f (x)n]′ = n · f (x)n−1 · f ′(x).
• CHAINRULE: Chain rule dy
dx = dy
du · du
dx (for some u).
2.2 Vectors,Matrices and Linear Programming
Before we continue, we will need to define one more concept, the Euclidean dis-
tance. If we have a 2D coordinate system, and have two points p1 := (x1, y1)
and p2 := (x2, y2) in it, we can define their distance in space as d(p1, p2) :=
�
(x1 − x2)2 + (y1 − y2)2. This distance is called the Euclidean distance and defines
the behaviour of the whole space; in a sense, the distance in a space is a fundamental
thing upon the whole behaviour of the space behaves. If we use the Euclidean dis-
tance when reasoning about space, we will get Euclidean spaces. Euclidean spaces
are the most common type: they follow our spatial intuitions. In this book, we will
use only Euclidean spaces.
Now, we turn our attention to developing tools for vectors. Recall that an n-
dimensional vector x is (x1, . . . , xn) and that all the individual xi are called compo-
nents. It is quite a normal thing to imagine n-dimensional vectors living as points
in an n-dimensional space. This space (when fully furnished) will be called a vec-
tor space, but we will return to this a bit later. For now, we have only a bunch of
n-dimensional vectors from Rn.
Let us introduce the notion of scalar. A scalar is just a number, and it can be
thought of as a ‘vector’ from R1. And n-dimensional vectors are simply sequences of
n scalars. We can always multiply a vector by a scalar, e.g. 3 · (1, 4, 6) = (3, 12, 18).
Vector addition is quite simple. If we want to add two vectors a = (a1, . . . , an)
and b = (b1, . . . , bn), they must have the same number of components. Then
a + b := (a1 + b1, . . . , an + bn). For example, (1, 2, 3) + (4, 5, 6) = (1 + 4, 2 +
5, 3 + 6) = (5, 7, 9). This gives us a hint that we must stick with vectors of the same
dimensionality (but we will always include scalars even though they are technically
1D vectors). Once we have scalar multiplication and vector addition, we have a vector
space.15
Let us take an in-depth view of the space our vectors live in. For simplicity, we
will talk about 3D entities, but anything we will say can be easily generalized to the
n-dimensional case. So, to recap, a 3D space is the place where 3D vectors live: they
are represented as points in this space. A question can be asked whether there is a
minimal set of vectors which ‘define’ the whole vector universe of 3D vectors. This
15We deliberately avoid talking about fields here since we only use R, and there is no reason to
complicate the exposition.

26
2
Mathematical and Computational Prerequisites
question is a bit vague but the answer is yes. If we take three16 vectors e1 = (1, 0, 0),
e2 = (0, 1, 0) and e3 = (0, 0, 1), we can express any vector in this space with the
formula:
s1e1 + s2e2 + s3e3
(2.2)
where s1, s2 and s3 are scalars chosen so that we get the vector we want. This shows
how mighty scalars are and how they control everything that happens—they are a
kind of aristocracy in the vector realm. Let us turn to an example. If we want to
represent the vector (1, 34, −28) in this way, we need to take s1 = 1, s2 = 34 and
s3 = −28 and plug them in Eq.2.2. This equation is called linear combination: every
vector in a vector field can be defined as a (linear) combination of the vectors e1, e2
and e3, and appropriately chosen scalars. The set {e1, e2, e3} is called the standard
basis of the 3D vector space (which is usually denoted as R3).
The reader may notice that we have been talking about the standard basis without
defining what a basis is. Let V be a vector space and B ⊆ V . Then, B is called a basis
if and only if all vectors in B are linearly independent (i.e. are not linear combinations
of each other) and B is a minimally generating subset of V (i.e. it must be a minimal17
subset which can produce with the help of Eq.2.2) every vector in V .
We turn our attention to defining the single most important operation with vectors
we will need in this book, the dot product. The dot product of two vectors (which
must have the same dimensions) is a scalar. It is defined as
a · b = (a1, . . . , an) · (b1, . . . , bn) :=
n
�
i=1
aibi = a1b1 + a2b2 + . . . anbn
(2.3)
This means that (1, 2, 3) · (4, 5, 6) = 1 · 4 + 2 · 5 + 3 · 6 = 32. If two vectors
have the a dot product equal to zero, they are called orthogonal. Vectors also have
lengths. To measure the length of a vector a, we compute its L2 or Euclidean norm.
The L2 norm of the vector is defined as
||a||2 :=
�
a2
1 + a2
2 + . . . + a2n
(2.4)
Bear in mind not to confuse the notation for norms with the notation for the
absolute value. We will see more about the L2 norm in the later chapters. We can
convert any vector a to a so-called normalized vector by dividing it with its L2 norm:
ˆa :=
a
||a||2
(2.5)
Two vectors are called orthonormal if they are normalized and orthogonal. We
will be needing these concepts in Chaps.3 and 9. We not turn our attention to matrices
16One for each dimension.
