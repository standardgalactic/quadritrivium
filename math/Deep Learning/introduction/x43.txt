kj.
When representing operations in neural networks as vectors and matrices, we want to
minimize the use of transpositions (since each one of them has a computational cost),
and keep the operations as natural and simple as possible. On the other hand, matrix
transposition is not that expensive, and it is sometimes better to keep things intuitive
rather than fast. In our case, we will want to represent a weight w which connects the
Fig.4.2 Weights in a
network

4.2
Representing Network Components with Vectors and Matrices
83
second neuron in layer 1 and the third neuron in layer 2 with a variable named w23. We
see that the index retains information on which neurons in the layers are connected,
but one might ask where do we store the information which layers are in question.
The answer is very simple, that information is best stored in the matrix name in the
program code, e.g. input_to_hidden_w. Note that we can call a matrix by its
‘mathematical name’, e.g. u or by its ‘code name’ e.g. hidden_to_output_w.
So, following Fig.4.2 we write the weight matrix connecting the two layers as:
�w11(= 0.1) w12(= 0.2) w13(= 0.3)
w21(= 1)
w22(= 2)
w23(= 3)
�
Let us call this matrix w (we can add subscripts or superscripts to its name).
Using matrix multiplication w⊤x we get a 3 × 1 matrix, namely the column vector
z = (21, 42, 63)⊤.
With this we have described, alongside the structure of the neurons and connec-
tions the forwarding of data through the network which is called the forward pass.
The forward pass is simply the sum of calculations that happen when the input travels
through the neural network. We can view each layer as computing a function. Then,
if x is the input vector, y is the output vector and fi, fh and fo are the overall functions
calculated at each layer, respectively,(products, sums and nonlinearities), we can say
that y = fo(fh(fi(x))). This way of looking at a neural network will be very important
when we will address the correction of weights through backpropagation.
For a full specification of a neural network we need:
• The number of layers in a network
• The size of the input (recall that this is the same as the number of neurons in the
input layer)
• The number of neurons in the hidden layer
• The number of neurons in the output layer
• Initial values for weights
• Initial values for biases
Note that the neurons are not objects. They exist as entries in a matrix, and as such,
their number is necessary for specifying the matrices. The weights and biases play a
crucial role: the whole point of a neural network is to find a good set of weights and
biases, and this is done through training via backpropagation, which is the reverse of
a forward pass. The idea is to measure the error the network makes when classifying
and then modify the weight so that this error becomes very small. The remainder
of this chapter will be devoted to backpropagation, but as this is the most important
subject in deep learning, we will introduce it slowly and with numerous examples.

84
4
Feedforward Neural Networks
4.3 The Perceptron Rule
As we noted before, the learning process in the neurons is simply the modification or
update of weights and biases during training with backpropagation. We will explain
the backpropagation algorithm shortly. During classification, only the forward pass
is made. One of the early learning procedures for artificial neurons is known as
perceptron learning. The perceptron consisted of a binary threshold neuron (also
known as binary threshold units) and the perceptron learning rule and altogether
looks like a modified logistic regression. Let us formally define the binary threshold
neuron:
z = b +
�
i
wixi
y =
�
1, z ≥ 0
0, otherwise
Where xi are the inputs, wi the weights, b is the bias and z is the logit. The second
equation defines the decision, which is usually done with the nonlinearity, but here a
binary step function is used instead (hence the name). We take a digression to show
that it is possible to absorb the bias as one of the weights, so that we only need a
weight update rule. This is displayed in Fig.4.3: to absorb the bias as a weight, one
needs to add an input x0 with value 1 and the bias is its weight. Note that this is
exactly the same:
z = b +
�
i
wixi = w0x0(= b) + w1x1 + w2x2 + . . .
According to the above equation, b could either be x0 or w0 (the other one must
be 1). Since we want to change the bias with learning, and inputs never change, we
must treat it as a weight. We call this procedure bias absorption.
Fig.4.3 Bias absorption

4.3
The Perceptron Rule
85
The perceptron is trained as follows (this is the perceptron learning rule3):
1. Choose a training case.
2. If the predicted output matches the output label, do nothing.
3. If the perceptron predicts a 0 and it should have predicted a 1, add the input vector
to the weight vector
4. If the perceptron predicts a 1 and it should have predicted a 0, subtract the input
vector from the weight vector
As an example, take the input vector to be x = (0.3, 0.4)⊤ and let the bias be
b = 0.5, the weights w = (2, −3)⊤ and the target4 t = 1. We start by calculating the
