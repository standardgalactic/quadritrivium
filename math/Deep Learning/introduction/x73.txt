Autoencoders
8.1 Learning Representations
In this and the next chapter we turn our attention to unsupervised deep learning,
also known as learning distributed representations or representation learning. But
first we need to fill in a blank we had from Chap.3. There we discussed PCA as a
form of learning distributed representations, and formulated the problem as finding
Z = X Q, where all features have been decorrelated. Here we will calculate the
matrix Q. We will need to have a covariance matrix of X. The covariance matrix
of a given matrix shows the entries of the original matrix. The covariance of two
random variables X and Y is defined as COV (X, Y) := E((X − E(X))(Y − E(Y)))
and show how two random variables change together. Remember that with a bit of
hand waving everything relating to data can be thought of as a random variable.
Also, with a bit more of hand waving, for a random variable X we may think of
E(X) = M E AN(X).1 This will only hold if the distribution of X is uniform, but it
can be helpful from a practical perspective even when it is not, especially since in
machine learning we will probably have some optimization somewhere so we can
be a bit sloppy.
The attentive reader may notice that E(X) was actually a vector, while M E AN(X)
is a single value, but we will use something called broadcasting to make it right again.
Broadcasting a value v into an n-dimensional vector v means simply to put the same
v in every component of v, or simply:
broadcast(v, n) = (v, v, v, . . . , v)
�
��
�
n
(8.1)
1The expected value is actually the weighted sum, which can be calculated from a frequency table.
If 3 out of five students got the grade ‘5’, and the other two got a grade ‘3’, E(X) = 0.6 · 5 + 0.4 · 3.
© Springer International Publishing AG, part of Springer Nature 2018
S. Skansi, Introduction to Deep Learning, Undergraduate Topics
in Computer Science, https://doi.org/10.1007/978-3-319-73004-2_8
153

154
8
Autoencoders
We will denote the covariance matrix of the matrix X as �(X). This is not a
standard notation, but (unlike the standard notation C or �) this notation will avoid
confusion, since we are using the standard notations in a different sense in this
book. To address the covariance matrix more formally, if we have a column vector
X = (X1, X2, . . . , Xd)⊤ populated with random variables, the covariance matrix
�X (which can also be denoted as �i j) can be defined as �i j = COV (Xi, X j) =
E((Xi − E(Xi))(X j − E(X j))), or if we write the whole d × d matrix:
�X =
⎡
⎢⎢⎢⎣
E((X1 − E(X1))(X1 − E(X1))) · · · E((X1 − E(X1))(Xd − E(Xd)))
E((X2 − E(X2))(X1 − E(X1))) · · · E((X2 − E(X2))(Xd − E(Xd)))
...
...
...
E((Xd − E(Xd))(X1 − E(X1))) · · · E((Xd − E(Xd))(Xd − E(Xd)))
⎤
⎥⎥⎥⎦
(8.2)
It should now be clear that the covariance matrix actually measures ‘self’-
covariance, i.e. covariance between its own elements. Let us see what properties
does a matrix �(X) have. First, it must be symmetric, since the covariance of X with
Y is the same as the covariance of Y with X. �(X) is also a positive-definite matrix,
which means that the scalar v⊤Xz is positive for every non-zero vector v.
Let us turn to a slightly different topic, eigenvectors. Eigenvectors of a d × d
matrix A are vectors whose direction does not change (but the length does) when
they are multiplied by A. It can be proved that there are exactly d of them. How to
find the eigenvectors is the hard part, and there are number of approaches, and one
of the more popular ones is gradient descent. Since all numerical libraries can find
eigenvectors for us, we will not go into details.
So the eigenvectors when multiplied by a matrix A do not change direction, only
the length. It is common practice to normalize the eigenvectors and denote them
by vi. This change of length is called the eigenvalue, usually denoted by λi. This
actually gives rise to a fundamental property of eigenvectors and eigenvalues of a
matrix, namely Avi = λivi
Once we have the vs and λs, we start by arranging the lambdas in descending
order:
λ1 > λ2 > . . . > λd
This also creates an arrangement in the corresponding eigenvectors v1, v2, …, vd
(note that each of them is of the form vi = (v(1)
i
, v(2)
i
, . . . , v(d)
i
), 1 ≤ i ≤ d) since
there is a one to one correspondence between them and the eigenvalues, so we can
simply ‘copy’ the order of the eigenvalues on the eigenvectors. We create a d × d
matrix with the eigenvectors as columns which are sorted with ordering of the the
corresponding eigenvalues (in the last step we are simply renaming the entries to

8.1
Learning Representations
155
follow the usual matric entry naming conventions):
V = (v⊤
1 , v⊤
2 , . . . , v⊤
d ) =
⎡
⎢⎢⎢⎢⎣
v(1)
1
