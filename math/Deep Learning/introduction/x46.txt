(t(n) − y(n))2
where the (t(n) denotes the target for the training case n (same for (y(n), but this is the
prediction). The training case n is simply a training example, such as a single image
or a row in a table. The mean squared error sums the error across all the training
cases n, and after that we will update the weights. The natural choice for measuring
how far were we from the bullseye would be to use the absolute value as a measure of
distance that does not depend on the sign, but the reason behind choosing the square
of the difference is that by simply squaring the difference we get a measure similar

90
4
Feedforward Neural Networks
to absolute values (albeit larger in magnitude, but this is not a problem, since we
want to use it in relative, not absolute terms), but we will get as a bonus some nice
properties to work with down the road.
Let us see, how we can derive the delta rule from the SSE to see that they are
the same.10 We start with the above equation defining the mean squared error and
differentiate E with respect to wi and get:
∂E
∂wi
= 1
2
�
n
∂y(n)
∂wi
dE(n)
dy(n)
The partial derivatives are here just because we have to consider a single wi and
treat all others as constants, but the overall behaviour apart from that is the same as
with ordinary derivatives. The above formula tells us a story: it tells us that to find out
how E changes with respect to wi, we must find out how y(n) changes with respect to
wi and how E changes with respect to y(n). This is a nice example of the chain rule
of derivations in action. We explored the chain rule in the second chapter but we will
give a cheatsheet for derivations shortly so you do not have to go back. Informally
speaking, the chain rule is similar to fraction multiplication, and if one recalls that a
shallow neural network is a structure of the general form y = fo(fh(fi(x))), it is easy
to see that there will be a lot of places to use the chain rule, especially as we go on
to deep learning and add more layers.
We will explain the derivations shortly. The above equation means the weight
updates are proportional to the error derivations in all training cases added together:
�wi = −η ∂E
∂wi
=
�
n
ηx(n)
i
(t(n) − y(n))
Let us proceed to the actual derivation. We will be deriving the result for a logistic
neuron (also called a sigmoid neuron), which we have already presented before, but
we will define it once more:
z = b +
�
i
wixi
y =
1
1 + e−z
Recall that z is the logit. Let us absorb the bias right away, so we do not have to
deal with it separately. We will calculate the derivation of the logistic neuron with
respect to the weights, and the reader can adapt the procedure to the simpler linear
neuron if she likes. As we noted before, the chain rule is your best friend for obtaining
derivations, and the ‘middle variable’ of the chain rule will be the logit. The first
10Not in the sense that they are the same formula, but that they refer to the same process and that
one can be derived from the other.

4.5
From the Logistic Neuron to Backpropagation
91
part ∂z
∂wi which is equal to xi since z = �
i wixi (we absorbed the bias). By the same
argument ∂z
∂xi = wi.
The derivation of the output with respect to the logit is a simple expression ( dy
dz =
y(1 − y)) but is not easy to derive. Let us restate the derivation rules we use11
• LD: Differentiation is linear, so we can differentiate the summands separately and
take out the constant factors: [f (x)a + g(x)b]′ = a · f ′(x) + b · g′(x)
• Rec: Reciprocal rule [ 1
f (x)]′ = − f ′(x)
f (x)2
• Const: Constant rule c′ = 0
• ChainExp: Chain rule for exponents [ef (x)]′ = ef (x) · f ′(x)
• DerDifVar: Deriving the differentiation variable dy
dz z = 1
• Exp: Exponent rule [f (x)n]′ = n · f (x)n−1 · f ′(x)
We can now start deriving dy
dz . We start with the definition for y, i.e. with
dy
dz
1
1 + e−z
From this expression by application of the Rec rule we get
−
dy
dz (1 + e−z)
(1 + e−z)
From this by applying LD we get
