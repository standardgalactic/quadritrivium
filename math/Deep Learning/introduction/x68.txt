easy to look down on SRNs today, but when they were first proposed, it became the
first model that could operate on words of a text without having to rely on an ‘alien’
representation such as the bag of words or n-grams. In a sense, those representations
seemed to suggest that language processing is something very foreign to a computer,
since people do not use anything like the Bag of words for understanding language.
The SRN made a decisive move towards the language processing as word sequence
processing paradigm we have today, and made the whole process much closer to
human intelligence. Consequently, SRNs should be considered a milestone in AI,
since they have made that crucial step: what previously seemed impossible was now
conceivable. But a couple of years later, a stronger architecture would come and take
over all practical applications, but this strength comes with a price: LSTMs are much
slower to train than SRNs.

142
7
Recurrent Neural Networks
7.5 Long Short-Term Memory
In this section, we will give a graphical illustration of the workings of the long
short-term memory (LSTM), and the interested reader should have no problem in
coding LSTMs from scratch just by following our explanation and the accompanying
images. All images in the current section on LSTMs are reproduced from Christopher
Olah’s blog.4 We follow the same notation as is used there (except from a couple of
minor details), and we omit the weights in Fig.7.3 to simply exposition, but we will
add them when addressing individual components of the LSTM in the later images.
Since we know from Eq.7.5 that y(t) = fo(wo · h(t)) ( fo is the nonlinearity of
choice for the output layer), in this chapter y(t) is the same as h(t), but we still
point to the places, where h(t) is to be multiplied by wo to get y(t) by simply noting
y(t) = h(t). This is really not that important from a purely formal point of view, but
we hope to be more clear by holding a place for y(t).
Figure7.3 shows a bird’s-eye perspective on LSTMs and compares them to SRNs.
One thing that can be seen right away is that SRNs have one link from one unit to the
next (it is the flow of h(t)), whereas the LSTMs have the same h(t) but also C(t).
This C(t) is called the cell state, an this is the main flow of information through
the LSTMs. Figuratively speaking, the cell state is the ‘L’, the ‘T’ and the ‘M’ from
‘LSTM’, i.e. it is the long-term memory of the model. Everything else that happens
is just different filters to decide what should be kept or added to the cell state. The
Fig.7.3 SRN and LSTM units zoomed
4http://colah.github.io/posts/2015-08-Understanding-LSTMs/,
accessed 2017-03-22.

7.5
Long Short-Term Memory
143
Fig.7.4 Cell state (a), forget gate (b), input gate (c) and output gate (d)
cell state is emphasized on Fig.7.4a (for now you should ignore the f (t) and i(t) on
the image, you will see how they are calculated in a couple of paragraphs).
The LSTM adds or removes information from the cell with so-called gates, and
these make up the rest of the unit in an LSTM. The gates are actually very simple.
They are a combination of addition, multiplication and nonlinearities. The nonlin-
earities are used simply to ‘squash’ information. The logistic or sigmoid function
(denoted as SIGM in the images) is used to ‘squash’ information to values between
0 and 1, and the hyperbolic tangent (denoted as TANH in the images) is used to
‘squash’ the information to values between −1 and 1. You can think of it in the fol-
lowing way: SIGM makes a fuzzy ‘yes’/‘no’ decision, while TANH makes a fuzzy
‘negative’/‘neutral’/‘positive’ decision. They do nothing else except this.
The first gate is the forget gate, which is emphasized in Fig.7.4b. The name ‘gate’
comes from analogies with the logic gates. The forget gate at unit t is denoted by
f (t), and is simply f (t) := σ(w f (x(t)+h(t −1))). Intuitively, it controls how much
of the weighted raw input and weighted previous hidden state is to be remembered.
Note that the σ is the symbol for the logistic function.
Regarding weights, there are different approaches, but we consider the most intu-
itive to be the one which breaks up wh into several different weights, w f , wff , wC and
wfff .5 The point to remember is that there are different ways to look at the weights
and some of them try to keep the same names as they had in simpler models, but the
most natural approach for deep learning is to think of an architecture as composed
5Notice that we are not quite precise here and that the w f in the LSTMs is actually the same as wx
in the SRN and not a component of the old wh.

144
7
Recurrent Neural Networks
of basic ‘building blocks’ to be assembled together like LEGO® bricks, and then
each block should have its own set of weights. All of the weight in a complete neural
network are trained together with backpropagation and the joint training actually
makes a neural network a connected whole (like each LEGO brick normally has its
own studs to connect to other bricks to make a structure).
The next gate (emphasized in Fig.7.4c), called the input gate, is a bit more com-
plex. It basically decides on what to put in the cell state. It is composed of another
forget gate (which we unimaginatively denote with ff (t)) but with different weights,
but it also has an additional module which creates candidates to be added to the cell
state. The ff (t) can be thought of as a saving mechanism, which controls how much
of the input we will save to the cell state. In symbols:
ff (t) := σ(wff (x(t) + h(t − 1))),
(7.8)
i(t) := ff (t) · C∗(t).
(7.9)
What we are missing is a calculation for the candidates (denoted by C∗(t)). Cal-
culating the candidates is pretty easy: C∗(t) := τ(wC ·(x(t)+h(t −1))), where τ is
the symbol for the hyperbolic tangent or tanh. We are using the hyperbolic tangent
here to squash the results to values which range between −1 and 1. Intuitively, the
negative part of the range (−1 to 0) can be seen as a way to get quick ‘negations’,
so that even opposites would be considered to get, for example a quick processing
of linguistic antonyms.
As we have seen before, an LSTM unit has three outputs: C(t), y(t) and h(t). We
have all we need to compute the current cell state C(t) (this calculation is shown in
Fig.7.4a):
C(t) := f (t) · C(t − 1) + i(t).
(7.10)
Since y(t) = go(wo · h(t)) (where go is a nonlinearity of choice), all that is left
is to compute h(t). To compute h(t), we will need a third copy of the forget gate
