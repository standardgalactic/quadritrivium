y = np.transpose(results).tolist()[1]
n = list(word2index.keys())
fig, ax = plt.subplots()
ax.scatter(x, y)
for i, txt in enumerate(n):
ax.annotate(txt,(x[i],y[i]))
plt.savefig(’word_vectors_in_2D_space.png’)
plt.show()
This produces Fig.9.2. Note that we need a significantly larger dataset than our
nine word sentence to be able to learn similarities (and to see them in the plot), but
you can experiment with different datasets using the parser we used with recurrent
neural networks.
Reasoning with word vectors is also quite straightforward. We need to take the
corresponding vectors from embedding_weight_matrix and do simple arith-
metic with them. They are all of the same dimensionality, which means it is quite easy
to add and subtract them. Let w2v(someword) denote the trained word embedding
Fig.9.2 Word similarity clusters in transformed 2D space
3More precisely: to transform the matrix into a decorrelated matrix whose columns are arranged in
descending variance and then keep the first two columns.

9.4
Walking Through the Word-Space:An Idea That Has Eluded Symbolic AI
173
for the word ‘someword’. To recreate the classic example, take w2v(king), subtract
from it w2v(man) add to it w2v(woman) and the result would be near w2v(queen).
The same holds even if we use PCA to transform the vectors and keep just the first
two or three components, although it is sometimes more distorted. This depends on
the quality and size of the dataset, and we suggest the reader to try to make a script
which does this over a large dataset as an exercise.
References
1. R.W. Hamming, Error detecting and error correcting codes. Bell Syst. Tech. J. 29(2), 147–160
(1950)
2. V.I. Levenshtein, Binary codes capable of correcting deletions, insertions, and reversals. Sov.
Phys. Dokl. 10(8), 707–710 (1966)
3. M.A. Jaro, Advances in record linkage methodology as applied to the 1985 census of tampa
florida. J. Am. Stat. Assoc. 84(406), 414–420 (1989)
4. W.E. Winkler, String comparator metrics and enhanced decision rules in the fellegi-sunter
model of record linkage, in Proceedings of the Section on Survey Research Methods (American
Statistical Association, 1990), pp. 354–359
5. A. Singhal, Modern information retrieval: a brief overview. Bull. IEEE Comput. Soc. Tech.
Comm. Data Eng. 24(4), 35–43 (2001)
6. T. Mikolov, T. Chen, G. Corrado, J. Dean, Efficient estimation of word representations in vector
space, in ICLR Workshop (2013), arXiv:1301.3781
7. Z. Harris, Distributional structure. Word 10(23), 146–162 (1954)
8. J.R. Firth, A synopsis of linguistic theory 1930–1955, in Studies in Linguistic Analysis (Philo-
logical Society, 1957), pp. 1–32
9. L. Wittgenstein, Philosophical Investigations (MacMillan Publishing Company, London, 1953)
10. H. Moravec, Mind Children: The Future of Robot and Human Intelligence (Harvard University
Press, Cambridge, 1988)

10
An Overview of Different Neural
Network Architectures
10.1 Energy-Based Models
Energy-based models are a specific class of neural networks. The simplest energy
model is the Hopfield Network dating back from the 1980s [1]. Hopfield networks
are often thought to be very simple, but they are quite different from what we have
seen before. The network is made of neurons, and all of these neurons are connected
among them with weights wi j connecting neurons ni and n j. Each neuron has a
threshold associated with it, and we denote it by bi. All neurons have 1 or −1 in
them. If you want to process and image, you can think of −1 as white and 1 as black
(no shades of grey here). We denote the inputs we place in neurons by xi. A simple
Hopfield network is shown in Fig. 10.1a.
Once a network is assembled, the training can start. The weights are updated by
the following rule, where n denotes an individual training sample:
wi j =
N
�
n=1
x(n)
i
x(n)
j
(10.1)
Then we compute activations for each neuron:
yi =
�
j
wi jx j
(10.2)
There are two possibilities on how to update weights. We can either do it syn-
chronously (all weights at the same time) or asynchronously (one by one, this is the
standard way). In Hopfield networks there is no recurrent connections, i.e. wii = 0
for all i, and all connections are symmetric, i.e. wi j = w ji. Let us see how the simple
Hopfield Network shown in Fig. 10.1b processes the simple 1 by 3 pixel ‘images’
in Fig. 10.1c, which we represent by vectors a = (−1, 1, −1), b = (1, 1, −1) and
© Springer International Publishing AG, part of Springer Nature 2018
S. Skansi, Introduction to Deep Learning, Undergraduate Topics
in Computer Science, https://doi.org/10.1007/978-3-319-73004-2_10
175

176
10
An Overview of Different Neural Network Architectures
Fig.10.1 Hopfield networks
c = (−1, −1, 1). Using the equation above, we calculate the weight updates with
the update equation:
w11 = w22 = w33 = 0
w12 = a1a2 + b1b2 + c1c2 = −1 · 1 + 1 · 1 + (−1) · (−1) = 1
w13 = −1
