layer is to be transmitted to a given neuron of the next layer. Of course, the weight is
not dependent on the initial neuron, but it depends on the initial neuron–destination
neuron pair. This means that the link between say neuron N5 and neuron M7 has a
weight wk while the link between the neurons N5 and M3 has a different weight, wj.
These weights can happen to have the same value by accident, but in most cases,
they will have different values.
The flow of information through the neural network goes from the first-layer
neurons (input layer), via the second-layer neurons (hidden layer) to the third-layer
neurons (output neurons). We return now to Fig.4.1. The input layer consists of three
neurons and each of them can accept one input value, and they are represented by
variables x1, x2, x3 (the actual input values will be the values for these variables).
Accepting input is the only thing the first layer does. Every neuron in the input
Fig.4.1 A simple neural network

4.1
Basic Concepts and Terminology for Neural Networks
81
layer can take a single output. It is possible to have less input values than input
neurons (then you can hand 0 to the unused neurons), but the network cannot take in
more input values than it has input neurons. Inputs can be represented as a sequence
x1, x2, . . . , xn (which is actually the same as a row vector) or as a column vector
x := (x1, x2, . . . , xn)⊤. These are different representations of the same data, and we
will always choose the representation that makes it easier and faster to compute the
operations we might need. In our choice of data representation, we are not constrained
by anything else but computational efficiency.
As we already noted, every neuron from the input layer is connected to every
neuron from the hidden layer, but neurons of the same layer are not interconnected.
Every connection between neuron j in layer k and neuron m in layer n has a weight
denoted by wkn
jm, and, since it is usually clear from the context which layers are
concerned, we may omit the superscript and write simply wjm. The weight regulates
how much of the initial value will be forwarded to a given neuron, so if the input is
12 and the weight to the destination neuron is 0.25, the destination will receive the
value 3. The weights can decrease the value, but they can also increase it since they
are not bound between 0 and 1.
Once again we return to Fig.4.1 to explain the zoomed neuron on the right-hand
side. The zoomed neuron (neuron 3 from layer 2) gets the input which is the sum of
the products of the inputs from the previous layer and respective weights. In this case,
the inputs are x3, x2 and x3, and the weights are w13, w23 and w33. Each neuron has a
modifiable value in it, called the bias, which is represented here by b3, and this bias
is added to the previous sum. The result of this is called the logit and traditionally
denoted by z (in our case, z23).
Some simpler models1 simply give the logit as the output, but most models apply a
nonlinear function (also called a nonlinearity or activation function and represented
by ‘S’ in Fig.4.1) to the logit to produce the output. The output is traditionally denoted
with y (in our case the output of the zoomed neuron is y23)2 The nonlinearity can
be generically refered to as S(x) or by the name of the given function. The most
common function used is the sigmoid or logistic function. We have encountered this
function before, when it was the main function in logistic regression. The logistic
function takes the logit z and returns as its output σ(z) =
1
1+e−z . The logistic function
‘squashes’ all it receives to a value between 0 and 1, and the intuitive interpretation
of its meaning is that it calculates the probability of the output given the input.
A couple of remarks. Different layers may have different nonlinearities which
we shall see in the later chapters, but all neurons of the same layer apply the same
nonlinearity to its logits. Also, the output of a neuron is the same value in every
direction it sends it. Returning to the zoomed neuron in Fig.4.1, the neuron sends
y23 in to directions, and both of them are the same value. As a final remark, following
Fig.4.1 again, note that the logits in the next layer will be calculated in the same
manner. If we take, for example z31, it will be calculated as z31 = b31 + w23
11y21 +
1These models are called linear neurons.
2From linear neurons we still want to use the same notation but we set y23 := z23.

82
4
Feedforward Neural Networks
w23
21y22 + w23
31y23 + w23
41y24. The same is done for z32, and then by applying the
chosen nonlinearity to z31 and z32 we obtain the final output.
4.2 Representing Network Components with Vectors
and Matrices
Let us recall the general shape of a m × n matrix (m is the number of rows and n is
the number of columns):
⎡
⎢⎢⎢⎣
a11 a12 a13 . . . a1n
a21 a22 a23 . . . a2n
...
...
...
...
...
am1 am2 am3 . . . amn
⎤
⎥⎥⎥⎦
Suppose we need to define with matrix operations the process sketched in Fig.4.2.
In Chap.3 we have seen how to represent the calculations for logistic regression
with matrix operators. We follow the same idea here but for simple feedforward
neural networks. If we want the input to follow the vertical arrangement as it is in
the picture, we can represent it as a column vector, i.e. x = (x1, x2)⊤. The Fig.4.2
also offers us the intermediate values in the network, so we can verify each step of
our calculation. As explained in the earlier chapters, if A is a matrix, the matrix entry
in the j row and k column is denoted by Aj,k or by Ajk. If we want to ‘switch’ the
j and k, we need the transpose of the matrix A denoted A⊤. So for all entries in the
matrices A and A⊤ the following holds: Ajk has the same value as A⊤
kj, i.e. Ajk = A⊤
