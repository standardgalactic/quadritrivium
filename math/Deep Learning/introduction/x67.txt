making them best suited for images and other limited sequences.
Recurrent neural networks work not by adding new layers to a simple feedforward
neural network, but by adding recurrent connections on the hidden layer. Figure7.1a
shows a simple feedforward neural netwok and Fig.7.1b shows how to add recurrent
connections to the simple feedforward neural network from Fig.7.1a. The outputs
from a given layer are denoted by I, O and H for the simple feedforward network, and
by H1, H2, H3, H4, H5, . . . when we add recurrent connections. The weights in the
simple feedforward network are denoted by wx (input-to-hidden) and wo (hidden-
to-output). It is very important not to confuse multiple outputs from a hidden layer
with multiple hidden layers, since a layer is actually defined in terms of weights, i.e.
each layer has its own set of weights, and here all Hn share the same set of weights,
viz. wh. Figure7.1c is exactly the same as Fig.7.1b with the only difference being
that we condensed the individual neurons (circles) into vectors (rectangles), which
we have been doing since Chap.3 in our calculations, but now we do it on the visual
display as well. Notice that to add the recurrent connection, we had to add a set of
weights, wh, to the calculation and this is all that is needed to add recurrence to the
network.
Note that the recurrent neural network can be unfolded so that the recurrent con-
nections are all specified. Figure7.2a shows the previous network and Fig.7.2 shows
how to unfold the recurrent connections. Figure7.2c is the same as Fig.7.2b but with
Fig.7.1 Adding recurrent connections to a simple feedforward neural network

140
7
Recurrent Neural Networks
Fig.7.2 Unfolding a recurrent neural network
the proper and detailed notation used in the recurrent neural network literature, and
we will focus on this representation for commenting on the fly how a recurrent neural
network works. The next section will use the sub-image C of Fig.7.2 for reference,
and this will be our standard notation for the rest of the chapter.3
7.4 Elman Networks
Let us comment on the Fig.7.2c. wx represent input weights, wh represent the recur-
rent connection weights and the wo the hidden-to-output weights. The xs are inputs,
and the ys are outputs, just like before. But here we have an additional sequential
nature, which tries to capture time. So x(1) is the first input, and later it gets x(2)
and so on. The same holds of outputs. If we have the classic setting, we would only
be using x(1) (to give the input vector) and y(4) to catch the (overall) output. But
for the sequential and predict-next settings, we would be using all xs and ys.
Notice that unlike the situation we had in simple feedforward networks, here we
also have the h, and they represent the inputs for the recurrent connection. We need
something to start with, and we can generate h(0) by simply setting all its entries to
0. We give an example calculation where it can be seen how to calculate all elements
and it will be much more insightful than giving a piece by piece calculation. By f ,
we will be denoting a nonlinearity, and you can think of it as the logistic function.
A bit later we will see a new nonlinearity called softmax, which can be used here
and has natural fit with recurrent neural networks. So, the recurrent neural network
3We used the shades of grey just to visually denote the gradual transition to the proper notation.

7.4
Elman Networks
141
calculates the output y at a final time t. The calculation can be unfolded to the
following recursive structure (which makes it clear why we need h(0)):
y(t) = f (w⊤
o h(t)) =
(7.1)
= f (w⊤
o f (w⊤
h h(t − 1) + w⊤
x x(t))) =
(7.2)
= f (w⊤
o f (w⊤
h f (w⊤
h h(t − 2) + w⊤
x x(t − 1)) + w⊤
x x(t))) =
(7.3)
= f (w⊤
o f (w⊤
h f (w⊤
h f (w⊤
h h(t − 3) + w⊤
x x(t − 2)) + w⊤
x x(t − 1)) + w⊤
x x(t))).
(7.4)
We can make this more readable by condensing it to two equations:
h(t) = fh(w⊤
h h(t − 1) + w⊤
x x(t))
(7.5)
y(t) = fo(w⊤
o h(t)),
(7.6)
where fh is the nonlinearity of the hidden layer, and fo is the nonlinearity of the
output layer, which are not necessarily the same function, but they can be the same
if we want. This type of recurrent neural network is called Elman networks [3], after
the linguist and cognitive scientist Jeffrey L. Elman.
If we change the h(t − 1) for y(t − 1) in Eq.7.5, so that it becomes as follows:
h(t) = fh(w⊤
h y(t − 1) + w⊤
x x(t)).
(7.7)
We obtain a Jordan network [4], which are named after the psychologist, mathe-
matician and cognitive scientist Michael I. Jordan. Both Elman and Jordan networks
are known in the literature as simple recurrent networks (SRN for short). Simple
recurrent networks are seldom used in applications today, but they are the main
teaching method for explaining recurrent networks before running in the much more
complex LSTMs, which are the main recurrent architecture used today. It is very
