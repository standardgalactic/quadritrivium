
62
3
Machine Learning Basics
Fig.3.5 Schematic view
of logistic regression
Logistic regression was first introduced in 1958 by D. R. Cox [6], and a consid-
erable amount of research was done both on logistic regression and using logistic
regression. Logistic regression is mainly used today for two reasons. First, it gives
an interpretation of the relative importance of features, which is nice to have if we
wish to build an intuition on a given dataset.15 The second reason, which is much
more important to us, is that the logistic regression is actually a one-neuron neural
network.16
By understanding logistic regression, we are taking a first and important step
towards neural networks and deep learning. Since logistic regression is a supervised
learning algorithm, we will have to have the target values for training included
in the row vectors for the training set. Imagine that we have three training cases,
xA = (0.2, 0.5, 1, 1) , xB = (0.4, 0.01, 0.5, 0) and xC = (0.3, 1.1, 0.8, 0). Logistic
regression has a much input neurons as it has features in the row vectors,17 which is
in our case 3.
You can see a schematic representation of logistic regression in Fig. 3.5. As for
the calculation part, the logistic regression can be divided into two equations:
z = b + w1x1 + w2x2 + w3x3,
which calculates the logit (also known as the weighted sum) and the logistic or
sigmoid function:
y = σ(z) =
1
1 + e−z
15Afterwards, we may do a bit of feature engineering and use an all-together different model. This
is important when we do not have an understanding of the data we use which is often the case in
industry.
16We will see later that logistic regression has more than one neuron, since each component of the
input vector will have to have an input neuron, but it has ‘one’ neuron in the sense of having a single
‘workhorse’ neuron.
17If the training set consists of n-dimensional row vectors, then there are exactly n − 1 features—the
last one is the target or label.

3.4
A Simple Neural Network:Logistic Regression
63
If we join them and tidy up a bit, we have simply
y = σ(b + w1x1 + w2x2 + w3x3)
Now, let us comment on these equations. The first equation shows how to calculate
the logit from the inputs. The inputs in deep learning are always denoted by x, the
output of the neuron is always denoted by y and the logit is denoted by z or sometimes
a. The equations above make use of all the notational abuse which is common in
the machine learning community, so be sure to understand why the symbols are
employed as they are.
To calculate the logit, we need (asides from the inputs) the weights w and the bias
b. If you look at the equations, you will notice that everything except the bias and
weights is either an input or calculated. The elements which are not given as inputs
or are constants like e are called parameters. For now, the parameters are the weights
and biases, and the point of logistic regression is to learn a good vector of weights
and a good bias to achieve good classification. This is the only learning in logistic
regression (and deep learning): finding a good set of weights.
But what are the weights and biases? The weights control how much of each
feature from the input we should let in. You can think about them as if they represent
percentages. They are not limited to the interval between 0 and 1, but this is a good
intuition to have. For weights over 1, you could think of them as ‘amplifications’. The
bias is a bit more tricky. Historically,18 it has been called threshold and it behaved
a bit differently. The idea was that the logit would simply calculate the weighted
sum of the inputs, and if it was above the threshold, the neuron would output a 1,
otherwise a 0. The 1 and 0 part was replaced by our equation for σ(z), which does
not output a sharp 0 or 1, but instead it ranges from 0 to 1. You can see the different
plots on Fig. 3.6. Later, in Chap.4, we will see how to incorporate the bias as one of
the weights. For now, it is enough to know that the bias can be absorbed as one of
Fig.3.6 Historic and actual neuron activation functions
18Mathematically, the bias is useful to make an offset called the intercept.

64
3
Machine Learning Basics
the weights so we can forget about the bias knowing it will be taken care of and it
will become one of the weights.
Let us make a calculation based on our inputs which will explain the mechanics
of logistic regression. We will need a starting value for the weights and bias, and we
usually produce this at random. This is done from a gaussian random variable, but to
keep things simple, we will generate a set of weights and bias by taking random values
between 0 and 1. Now, we would need to pass the input row vectors through one-hot
encoding and normalize them, but suppose they already have been one-hot encoded
and normalized. So we have xA = (0.2, 0.5, 0.91, 1), xB = (0.4, 0.01, 0.5, 0) and
xC = (0.3, 1.1, 0.8, 0) and assume that the randomly generated weight vector is
w = (0.1, 0.35, 0.7) and the bias is b = 0.66. Now we turn to our equations, and put
in the first input:
yA = σ(0.66 + 0.1 · 0.2 + 0.35 · 0.5 + 0.7 · 0.91) = σ(1.492) =
1
1 + e−1.492 = 0.8163
We note the result 0.8163 and the actual label 1. Now we do the same for the
second input:
yB = σ(0.66 + 0.1 · 0.4 + 0.35 · 0.01 + 0.7 · 0.5) = σ(1.0535) =
1
1 + e−1.0535 = 0.7414
Noting again the result 0.7414 and label 0. And now we do it for the last input
row vector:
yC = σ(0.66 + 0.1 · 0.3 + 0.35 · 1.1 + 0.7 · 0.8) = σ(1.635) =
1
1 + e−1.635 = 0.8368
Noting again the result 0.8368 and the label 0. It seems quite clear that we did
good on the first, but failed to classify the second and third input correctly. Now, we
should update the weights somehow, but to do that we need to calculate how lousy
