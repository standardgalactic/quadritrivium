indeed work in a feedforward neural network.
Now, we turn to backpropagation. Let us examine what happens in the hidden
layer of the feedforward neural network. We start with randomly initialized weights
and biases, multiply them with the inputs, add them together, and take them through
the logistic regression which “flattens” them to a value between 0 and 1, and we do
that one more time. At the end, we get a value between 0 and 1 from the logistic
neuron in the output layer. We can say that everything above 0.5 is 1 and below is 0.
But the problem is that if the network gives a 0.67 and the output should have been
0, we know only the error the network produced (the function E), and we should
use this. More precisely, we want to measure how E changes when the wi change,
which means that we want to find the derivative of E with regard to the activities of
the hidden layer. We want to find all the derivatives at the same time, and for this,
we use vector and matrix notations and, consequently, the gradient. Once we have
the derivatives of E with regard to the hidden layer activity, we will easily compute
the changes for the weights themselves.
We will address the procedure illustrated in Fig.4.4. To keep the exposition as
clear as possible, we will use only two indices, as if each layer has only one neuron.
In the following section, we shall expand this to a fully functional feedforward neural
network. As illustrated in Fig.4.4 we will use the subscripts o for the output layer and
h for the hidden layer. Recall that z is the logit, i.e. everything except the application
of the nonlinearity.
16Cf. G. Hinton’s Coursera course, where this method is elaborated.

96
4
Feedforward Neural Networks
As we have
E = 1
2
�
o∈Output
(to − yo)2
the first thing we need to do is turn the difference between the output and the target
value into an error derivation. We have done this already in the previous sections of
this chapter:
∂E
∂yo
= −(to − yo)
Now, we need to reformulate the error derivative with regard to yo into an error
derivative with regard to zo. For this, we use the chain rule:
∂E
∂zo
= ∂yo
∂zo
∂E
∂yo
= yo(1 − yo) ∂E
∂yo
Now we can calculate the error derivative with respect to yh:
∂E
∂yh
=
�
o
dzo
dyh
∂E
∂zo
=
�
o
who
∂E
∂zo
These steps we made from ∂E
∂yo to ∂E
∂yh are the heart of backpropagation. Notice
that now we can repeat this to go through as many layers as we want. There will be
a catch though, but for now is all good. A few remarks about the above equation.
From the previous section, when we addressed the logistic neuron we know that
dzo
dyh = who. Once, we have ∂E
∂zo it is very simple to get the error derivative with regard
to the weights:
Fig.4.4 Backpropagation

4.6
Backpropagation
97
∂E
∂who
= ∂zo
∂who
∂E
∂zo
= yi
∂E
∂zj
The rule for updating weights is quite straightforward, and we call it the general
weight update rule:
wnew
i
= wold
i
+ (−1)η ∂E
∂wold
i
The η is the learning rate and the factor −1 is here to make sure we go towards
minimizing E, otherwise we would be maximizing it. We can also state it in vector
notation17 to get rid of the indices:
