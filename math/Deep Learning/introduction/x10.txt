1.3
From Cognitive Science to Deep Learning
13
imitation of any mental process taking place in the human cortex. Philosophy also
wants to abstract away from the brain, and define its terms in a more general setting.
A working definition of ‘cognitive process’ might be: any process taking place in
a similar way in the brain and the machine. This definition commits us to define
‘similar way’, and if we take artificial neural networks to be a simplified version of
the real neuron, this might work for our needs here.
This leads us to the bigger issue. Some cognitive processes are simpler, and we
could model them easily. Advances in deep learning sweep away one cognitive
process at the time, but there is one major cognitive process eludes deep learning—
reasoning. Capturing and describing reasoning is the very core of philosophical
logic, and formal logic as the main method for a rigorous treatment of reasoning has
been the cornerstone of GOFAI. Will deep learning ever conquer reasoning? Or is
learning simply a process fundamentally different from reasoning? This would mean
that reasoning is not learnable in principle. This discussion resonates the old philo-
sophical dispute between rationalists and empiricists, where rationalists argued (in
different ways) that there is a logical framework in our minds prior to any learning.
A formal proof that no machine learning system could learn reasoning which is con-
sidered a distinctly human cognitive process would have a profound technological,
philosophical and even theological significance.
The question about learning to reason can be rephrased. It is widely believed
that dogs cannot learn relations.20 A dog would then be an example of a trainable
cognitive system incapable of learning relations. Suppose we want to teach a dog
the relation ‘smaller’. We could devise a training setting where we hand the dog
two different objects, and the dog should pick the smaller one when hearing the
command ‘smaller’ (and he is rewarded for the right pick). But the task for the dog is
very complex: he has to realize that ‘smaller’ is not a name of a single object which
changes reference from one training sample to the next, but something immaterial
that comes into existence when you have both objects, and then resolves to refer to
a single object (the smaller one). If you think about it like that, the difficulties of
learning relations become clearer.
Logic is inherently relational, and everything there is a relation. Relational rea-
soning is accomplished by formal rules and poses no problem. But logic has the
very same problem (but seen from the other side): how to learn content for relations?
The usual procedure was to hand define entities and relations and then perhaps add
a dynamical factor which would modify them over time. But the divide between
patterns and relations exists on both sides.
20Whether this is true or not, is irrelevant for our discussion. The literature on animal cognitive
abilities is notoriously hard to find as there are simply not enough academic studies connecting
animal cognition and ethology. We have isolated a single paper dealing with limitations of dog
learning [37], and therefore we would not dare to claim anything categorical—just hypothetical.

14
1
From Logic to Cognitive Science
The paper that exposed this major philosophical issue in artificial neural networks
and connectionism, is the seminal paper by Fodor and Pylyshyn [38]. They claimed
that thinking and reasoning as a phenomena is inherently rule-based (symbolic,
relational), and this was not so much a natural mental faculty but a complex ability
that evolved as a tool for preserving truth and (to a lesser extent) predicting future
events. They pose it as a challenge to connectionism: if connectionism will be able
to reason, the only way it will be able to do so (since reasoning is inherently rule-
based) is by making an artificial neural network which produces a system of rules.
This would not be ‘connectionist reasoning’ but symbolic reasoning whose symbols
are assigned meaningful things thanks to artificial neural networks. Artificial neural
networks fill in the content, but the reasoning itself is still symbolic.
You might notice that the validity of this argument rests on the idea that thinking
is inherently rule-based, so the most easy way to overcome their challenge it is to
dispute this initial assumption. If thinking and reasoning would not be completely
rule-based, it would mean that they have aspects that are processed ‘intuitively’, and
not derived by rules. Connectionists have made an incremental but important step
in bridging the divide. Consider the following reasoning: ‘it is to long for a walk, I
better take my van’, ‘I forgot that my van is at the mechanic, I better take my wife’s
car’. Notice that we have deliberately not framed this as a classic syllogism, but in
a form similar to the way someone would actually think and reason.21 Notice that
what makes this thinking valid,22 is the possibility of equating ‘car’ with ‘van’ as
similar.23 Word2vec [39] is a neural language model which learns numerical vectors
for a given word and a context (several words around it), and this is learned from
texts. The choice of texts is the ‘big picture’. A great feature of word2vec is that
it clusters words by semantic similarity in the big picture. This is possible since
semantically similar words share a similar immediate context: both Bob and Alice
can be hungry, but neither can Plato nor the number 4. But substituting similar for
similar is just proto-inference, the major incremental advance towards connectionist
reasoning made possible by word2vec is the native calculations it enables. Suppose
that v(x) is the function which maps x (which is a string) to its learned vector.
Once trained, the word vectors word2vec generates are special in the sense that one
can calculate with them like v(king) − v(man) + v(woman) ≈ v(queen). This is
called analogical reasoning or word analogies, and it is the first major landmark in
developing a purely connectionist approach to reasoning.
We will be exploring reasoning in the final chapter of the book in the context of
question answering. We will be exploring also energy-based models and memory
models, and the best current take on the issue of reasoning is with memory-based
21Plato defined thinking (in his Sophist) as the soul’s conversation with itself, and this is what we
want to model, whereas the rule-based approach was championed by Aristotle in his Organon.
More succinctly, we are trying to reframe reasoning in platonic terms instead of using the dominant
Aristotelian paradigm.
22At this point, we deliberately avoid talking of ‘valid inference’ and use the term ‘valid thinking’.
23Note that this interchangeability dependent on the big picture. If I need to move a piano, I could
not do it with a car, but if I need to fetch groceries, I can do it with either the car or the van.

1.5
Philosophical and Cognitive Aspects
15
models. This is perhaps surprising since in the normal cognitive setting (undoubtedly
under the influence of GOFAI), we consider memory (knowledge) and reasoning as
two rather distinct aspects, but it seems that neural networks and connectionism do
not share this dichotomy.
References
