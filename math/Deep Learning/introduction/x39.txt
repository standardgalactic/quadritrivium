Building distributed representations by hand is hard, and yet this is the essence
of what artificial neural networks do. Every layer builds its own distributed rep-
resentation and this facilitates learning (this is perhaps the very essence of deep
learning—learning many layers of distributed representations). We will show the
simplest method of building a meaningful distributed representation, but we will
write all the mathematical details of it only in Chap.9. It is quite hard, and this is
why we want deep learning to build such things for us. This method of building
distributed representations is called the principal component analysis or PCA for
short. In this chapter, we will provide a bird’s-eye view of PCA and we will give all
30We have to use the same number of centroids in both clusterings for this to work.
31These features are known as latent variables in statistics.

3.7
Learning Different Representations:PCA
73
the details in Chap.9.32 PCA has the following form:
Z = X Q,
(3.17)
where X is the input matrix, Z is the transformed matrix and Q is the ‘tool-matrix’
with which we do the transformation. If X is an n × d matrix, Z should also be
n × d. This gives us our first information about Q: it has to be a d × d matrix for the
multiplication to work. We will show how to find the appropriate Q in Chap.9. In
the remainder of this section, we will introduce the intuition behind PCA as a whole
and some of the elements needed to build Q. We will also describe in detail what do
we want PCA to do and for what we want to be able to use it.
In general terms, PCA is used to preprocess the data. This means that it has to
transform the data before the data is fed in a classifier, to make it more digestible.
PCA is helpful for preprocessing in a couple of ways. We have seen above that
we will use it to build distributed representations of data to eliminate correlation.
We will also be able to use PCA for dimensionality reduction. We have seen how
dimensionscanexpandwithone-hotencodingandmanualfeatureengineering.When
we make distributed representations with artificial features such as ‘Argh’, ‘Haght’
and ‘Waght’, we would like to be able to order them in terms of informativity, so
that we can discard the uninformative features. Informativity is just variance33: if a
feature varies more, it carries more information.34 This is something we want our Z
to be like: the feature that has the most variance should be in the first column, the
one with the second most variance in the second column of Z and so on.
To illustrate how the variance can change with simple transformations, see Fig.
3.10 where we have a simple case of six 2D datapoints. The part A of Fig. 3.10
illustrates the starting position. Note that the variance along the x coordinate is
relatively small: the projections of the datapoints on the x axis are tightly packed
together. The variance along the y axis is better, and the y coordinates are further
apart. But we can do even better. Take a look at the part B of Fig. 3.10: we have
obtainedthisbyrotatingthecoordinatesystemabit.Noticethatalldatastaysthesame
and we are changing our representation of the data, i.e. the axes (which correspond
to features). The new ‘coordinate system’ is actually, mathematically speaking, just
a different basis for the points in this 2D vector space. You are not changing the
points (i.e. 2D vectors), but the ‘coordinate system’ they live in. You are actually
not even changing the coordinate system, but simply the basis of the vector space.
The question of how to do this mathematically is actually the same as asking how to
find a matrix Q such that it behaves in this way, and we will answer this in Chap.9.
Along the axes, we have plotted the distance between the first and last datapoint
coordinates, which may be seen as a ‘graphical proxy’ for variance. In the B part of
32One of the reasons for this is that we have not yet developed all the tools we need to write out the
details now.
33See Chap.2.
34And if a feature is always the same, it has a variance of 0 and it carries no information useful for
drawing the hyperplane.

74
3
Machine Learning Basics
Fig.3.10 Variance under rotation of the coordinate system
Fig. 3.10, we have compared the black (original coordinate system) with the grey
(transformed) variance side-by-side (next to the black coordinate system). Notice
that the variance along the y axis (the axis which had more variance in the original
system) has increased, while the variance on the x axis (the axis which had less
variance in the original system) has actually decreased.
Before continuing, let us make a final remark about PCA and preprocessing. One
of the most fundamental problems with any kind of data is that it is noisy. Noise can be
defined as everything except relevant information. If our dataset has enough training
samples, then it should have non-random information and random noise. They are
usually mixed up in features. But if we can build a distributed representation, this
means we can extract as separate features the parts which have more variance and
part which have less variance; we could assume that noise (which is random) has low
variance (it is ‘equally random’ everywhere), whereas information has high variance.
Suppose we have used PCA on a 20-dimensional input matrix. Then, we can keep
the first 10 new features and by doing so we have eliminated a lot of noise (low
variance features) by eliminating only a little bit of information (since they are low
variance features—not ‘no variance’ features).
PCA has been around a long time. It was first discovered by Karl Pearson of the
University College London [13] in 1901. Since then variants of the PCA went by
many names, and often there were subtle differences. The details of the relations
between various variants of the PCA are interesting, but unfortunately they would
require a whole book to explore, and consequently are beyond the scope of this
volume.

3.8
Learning Language:The Bag of Words Representation
75
3.8 Learning Language:The Bag of Words Representation
So far we have addressed numerical features, ordinal features and categorical fea-
tures. We have seen how to do one-hot encoding for categorical features. We have
not addressed a whole field, namely natural language processing. We refer the reader
to [14] or [15] for a thorough introduction to natural language processing. In this
section, we will see how to process language by using one of the simplest models,
the bag of words.
Let us first define a couple of terms for natural language processing. A corpus is
a whole collection of texts we have. A corpus can be decomposed into fragments.
Fragments can be single sentences, paragraphs or multi-page documents. Basically,
