Let us try to develop an intuition behind the two regularizations. The L2 reg-
ularization tries to push down the square of the weights (which does not increase
linearly as the weights increase), whereas L1 is concerned with absolute values
which is linear, and therefore L2 will quickly penalize large weights (it tends to
concentrate on them). L1 regularization will make much more weights slightly
smaller, which usually results in many weights coming close to 0. To simplify the
matter completely, take the plots of the graphs f (x) = x2 and g(x) = |x|. Imag-
ine that those plots are physical surfaces like bowls. Now imagine putting some
points in the graphs (which correspond to the weights) and adding ‘gravity’, so
that they behave like physical objects (tiny marbles). The ‘gravity’ corresponds to
gradient descent, since it is a move towards the minimum (just like gravity would
push to a minimum in a physical system). Imagine that there is also friction, which
corresponds to the idea that E does not care anymore about the weights that are
already very close to the minimum. In the case of f (x), we will have a number
of points around the point (0, 0), but a bit dispersed, whereas in g(x) they would
be very tightly packed around the (0, 0) point. We should also note that two vec-
tors can have the same L1 norm but different L2 norms. Take v1 = (0.5, 0.5) and
v2 = (−1, 0). Then ||v1||1 = |0.5| + |0.5| = 1 and ||v2||1 = | − 1| + |0| = 1, but
||v1||2 =
√
0.52 + 0.52 =
1
√
2 and ||v2||2 =
√
12 + 02 = 1.
5.3 Learning Rate,Momentum and Dropout
In this section, we will revisit the idea of the learning rate. The learning rate is an
example of a hyperparameter. The name is quite unusual, but there is actually a
simple reason behind it. Every neural network is actually a function which assigns to
a given input vector (input) a class label (output). The way the neural network does
this is via the operations it performs and the parameters it is given. Operations include
the logistic function, matrix multiplication, etc., while the parameters are all numbers
which are not inputs, viz. weights and biases. We know that the biases are simply
weights and that the neural network finds a good set of weights by backpropagationg
the errors it registers. Since operations are always the same, this means that all of
the learning done by a neural network is actually a search for a good set of weight,
or in other words, it is simply adjusting its parameters. There is nothing more to
it, no magic, just weight adjusting. Now that this is clear, it is easy to say what
a hyperparameter is. A hyperparameter is any number used in the neural network
which cannot be learned by the network. An example would be the learning rate or
the number of neurons in the hidden layer.
This means that learning cannot adjust hyperparameters, and they have to be
adjusted manually. Here machine learning leans heavily towards art, since there is
no scientific way to do it, it is more a matter of intuition and experience. But despite
the fact that finding a good set of hyperparameters is not easy, there is a standard

112
5
Modifications and Extensions to a Feed-Forward Neural Network
procedure how to do it. To do this, we must revisit the idea of splitting the data set
in a training set and a testing set. Suppose we have kept 10% of the datapoints for
testing, and the rest we wanted to use as the training set. Now we will take another
10% of datapoints from the training set and call it a validation set. So we will have
80% of the datapoints in the training set for training, 10% we use for a validation set,
and 10% we keep for a test set. The idea is to train on the train set with a given set of
hyperparameters and test it on the validation set. If we are not happy, we re-train the
network and test the validation set again. We do this until we get a good classification.
Then, and only then we test on the test set to see how it is doing.
Remember that a low train error and a high test error is a sign of overfitting.
When we are just training and testing (with no hyperparameter tuning), this is a
good rule to stick to. But if we are tuning hyperparameter, we might get overfitting
to both the training and validation set, since we are changing the hyperparameters
until we get a small error on the validation set. If the errors can become misleadingly
small since the classifier learns the noise of the training set, and we manually change
the hyperparameters to suit the noise of the validation set. If, after this, there is
proportionately small error on the test set, we have a winner, otherwise it is back to
the drawing board. Of course, it is possible to alter the sizes of the train, validation and
test sets, but these are the standard starting values (80%, 10% and 10% respectively).
We return to the learning rate. The idea of including a learning rate was first
explicitly proposed in [8]. As we have seen in the last chapter, the learning rate
controls how much of the update we want, since the learning rate is part of the
general weight update rule, i.e. it comes into play in the very end of backpropagation.
Before turning to the types of the learning rate, let us explore why the learning rate
is important in an abstract setting.2 We will construct an abstract model of learning
by generalizing the idea with the parabola we proposed in the previous section. We
need to expand this to three dimensions just so we can have more than one way to
move. The overall shape of the 3D surface we will be using is like a bowl (Fig. 5.2).
Its lateral view is given by the axes x and y (we do not see z). Seen from the top
(axes x and z visible, axis y not visible), it looks like a circle or ellipse. When we
‘drop’ a point at (xk, zk), it will get the value yk from the curve at the coordinates
(xk, zk). In other words, it will be as if we drop the point and it falls towards the
bowl and stops as soon as it meets the surface of the bowl (imagine that our point is
a sticky object, like a chewing gum). We drop it at a precise (xk, zk) (this is the ‘top
view’), we do not know the final ‘height’ of the sticky object, but we will measure it
when it falls to the side of the bowl.
The gradient is like gravity, and it tries to minimize y. If we want to continue our
analogy, we must make a couple of changes to the physical world: (i) we will not
have sticky objects all the time (we needed them to explain how can we get the y of
a point if we only have (x, z)), but little marbles which turn to sticky objects when
they have finished their move (or you may think that they ‘freeze’), (b) there is no
2We take the idea for this abstraction from Geoffrey Hinton’s courses.

5.3
Learning Rate,Momentum and Dropout
113
Fig.5.2 Gradient bowl
friction or inertia and, perhaps the most counterintuitive, (c) our gravity is similar to
physical gravity but different.
Let us explain (c) in more detail. Suppose we are looking from the top, so we see
