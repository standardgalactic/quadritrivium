upon. This means that we do not have to re-implement everything anew—there is
no use in reinventing the wheel. But there is value to be gained in understanding the
idea behind the invention of the wheel and in trying to make a wheel by yourself. In
this sense, you should try to recreate the codes we will be exploring, and see how
they work and even try to re-implement a completed Keras layer in plain Python. It
is quite probable that if you manage your solution will be considerably slower, but
you will have gained insight. When you feel you understand it as much as you
would like, you should just use Keras or any other framework as building bricks to
go on and build more elaborate things.
In today’s world, everything worth doing is a team effort and every job is then
divided in parts. My part of the job is to get the reader started in deep learning.
I would be proud if a reader would digest this volume, put it on a shelf, become and
active deep learning researcher and never consult this book again. To me, this
would mean that she has learned everything there was in this book and this would
entail that my part of the job of getting one started9 in deep learning was done well.
In philosophy, this idea is known as Wittgenstein’s ladder, and it is an important
practical idea that will supposedly help you in your personal exploration–ex-
ploitation balance.
I have also placed a few Easter eggs in this volume, mainly as unusual names in
examples. I hope that they will make reading more lively and enjoyable. For all
who wish to know, the name of the dog in Chap. 3 is Gabi, and at the time of
publishing, she will be 4 years old. This book is written in plural, following the old
academic custom of using pluralis modestiae, and hence after this preface I will no
longer use the singular personal pronoun, until the very last section of the book.
I would wish to thank everyone who has participated in any way and made this
book possible. In particular, I would like to thank Siniša Urošev, who provided
valuable comments and corrections of the mathematical aspects of the book, and to
Antonio Šajatović, who provided valuable comments and suggestions regarding
memory-based models. Special thanks go to my wife Ivana for all the support she
gave me. I hold myself (and myself alone) responsible for any omissions or mis-
takes, and I would greatly appreciate all feedback from readers.
Zagreb, Croatia
Sandro Skansi
7This is called the benchmark for a given problem, it is something you must surpass.
8Usually in the form of a new dataset constructed from a controlled version of a philosophical
problem or set of problems. We will have an example of this in the later chapters when we will
address the bAbI dataset.
9Or, perhaps, ‘getting initiated’ would be a better term—it depends on how fond will you become
of deep learning.
viii
Preface

References
1. I. Goodfellow, Y. Bengio, A. Courville, Deep Learning (MIT press, Cambridge, 2016)
2. A. Gulli, S. Pal, Deep Learning with Keras (Packt publishing, Birmingham, 2017)
3. G. Montavon, G. Orr, K.R. Muller, Neural Networks: Tricks of the Trade (Springer, New
York, 2012)
Preface
ix

Contents
1
From Logic to Cognitive Science . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
The Beginnings of Artificial Neural Networks . . . . . . . . . . . . . .
1
1.2
The XOR Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.3
From Cognitive Science to Deep Learning . . . . . . . . . . . . . . . . .
8
1.4
Neural Networks in the General AI Landscape. . . . . . . . . . . . . .
11
1.5
Philosophical and Cognitive Aspects . . . . . . . . . . . . . . . . . . . . .
12
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
2
Mathematical and Computational Prerequisites . . . . . . . . . . . . . . . .
17
2.1
Derivations and Function Minimization . . . . . . . . . . . . . . . . . . .
17
2.2
Vectors, Matrices and Linear Programming . . . . . . . . . . . . . . . .
25
2.3
Probability Distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
32
2.4
Logic and Turing Machines . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
2.5
Writing Python Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
2.6
A Brief Overview of Python Programming. . . . . . . . . . . . . . . . .
43
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
3
Machine Learning Basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
3.1
Elementary Classification Problem . . . . . . . . . . . . . . . . . . . . . . .
51
