are not committed to the logistic function. In this view, the logistic function is a
nonlinearity,21 i.e. it is the component which enables complex behaviour (especially
when we expand the model beyond a single workhorse neuron of the classic logistic
regression). There are many types of nonlinearity, and they all have a slightly dif-
ferent behaviour. The logistic regression ranges between 0 and 1. Another common
nonlinearity is the hyperbolic tangent or tanh, which we will denote by τ to enforce
a bit of notational consistency. The τ nonlinearity ranges between −1 and 1, and has
a similar shape like the logistic function. It is calculated by
τ(z) = ez − e−z
ez + e−z
(3.14)
The choice of which activation function to use in neural networks is a matter of
preference, and it is often guided by the results one obtains using them. If, we use the
hyperbolic tangent in logistic regression instead of the logistic function, it will still
work nicely, but technically that is not logistic regression anymore. Neural networks,
on the other hand, are still neural networks regardless of which nonlinearity we use.
21In the older literature, this is sometimes called activation function.

68
3
Machine Learning Basics
Fig.3.7 A single MNIST
datapoint
3.5 Introducing the MNIST Dataset
The MNIST dataset is a modification of the National Institute of Standards and
Technology of the United States dataset consisting of handwritten digits. The original
datasets are described in [7] and the MNIST (modified NIST) is a modification of
the Special Database 1 and Special Database 3 of the original dataset compiled by
Yann LeCun, Corinna Cortes and Christopher J. C. Burges. The MNIST dataset was
first used in the paper [8]. Geoffrey Hinton called MNIST ‘the fruit fly of machine
learning’22 since a lot of research in machine learning was performed on it and it
is quite versatile for a number of simple tasks. Today, MNIST is available from a
variety of sources, but the ‘cleanest’ source is probably Kaggle where the data is
kept in a simple CSV file,23 accessible by any software with ease. In Fig. 3.7 (image
taken from [9]), we can see an example of a MNIST digit.
MNIST images are 28 by 28 pixels in greyscale, so the value for each pixel ranges
between 0 (white) and 255 (black). This is different from the usual greyscale where
0 is black and 255 is white, but the community thought it might make more sense
since it can be stored in less space this way, but this is a minor point today for a
dataset of the size of MNIST.
There is one issue here, to which we will return at the very end of the book.
The problem is that all currently available supervised machine learning algorithms
can only process vectors as inputs: no matrices, graphs, trees, etc. This means that
whatever we are trying to do, we have to find a way to put in vector form and transform
all of our inputs in n-dimensional vectors. The MNIST dataset consists of 28 by 28
images, so, in essence, the inputs are matrices. Since they are all of the same size,
we can transform them in 784-dimensional vectors.24 We could do this by simply
‘reading’ them as we would a written page: left to right, after the row of pixels ends,
move to the leftmost part of the next line and continue again. By doing this, we have
transformed a 28 × 28 matrix into a 784-dimensional vector. This is a rather simple
transformation (note that it only works if all input samples are of the same size), and
if we want to learn graphs and trees, we have to have a vector representation of them.
We will return to this as an open problem at the very end of this book.
There is one additional point we want to make here. MNIST consists of greyscale
images. What could we do if it was RGB? Recall that an RGB image consists of three
22See http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec1.pdf.
23Available at https://www.kaggle.com/c/digit-recognizer/data.
24The interested reader may look up the details in Chap.4 of [10].

3.5
Introducing the MNIST Dataset
69
Fig.3.8 Greyscale for all colours, red channel, green channel and blue channel
component ‘images’ called channels: red, green and blue. They are joined to form
the complete (colour) image. We could print these in colour (each pixel of the red
channel would have a value from 0 to 255 to denote how much red is in it), but we have
actually converted the colours to greyscale without noticing (see Fig. 3.8). It might
seem weird to represent the red channel as grey, but that is exactly what a computer
does. The name of the channel image is ‘red’ but the values in pixels are between 0
and 255, which is, computationally speaking, grey. This is because an RGB pixel is
simply three values from 0 to 255. The first one is called ‘red’, but computationally
it is red just because it is in the first place. There is no intrinsic ‘redness’ or qualia
in it. If we were to display the pixels without providing the other two components, 0
will be interpreted as black and 255 as white, making it a greyscale. In other words,
an RGB image would have a pixel with the value (34, 67, 234), but if we separate a
channel by taking only the red component 34 we would get a greyscale. To get the
‘redness’ in the display, we must state it as (34, 0, 0) and keep it as an RGB image.
And the same goes for green and blue. Returning to our initial question, if we were
processing RGB images would have several options:
• Average the components to produce and average greyscale representation (this is
the usual way to create greyscale images from RGB).
• Separate the channels and form three different datasets and train three classifiers.
When predicting, we take the average of their result as the final result. This is an
example of a committee of classifiers.
• Separate the channels in distinct images, shuffle them and train a single classifier
on all of them. This approach would be essentially dataset augmentation.
• Separate the channels in distinct images, train three instances of the same classifier
on each (same size and parameters), and then use a fourth classifier to make the
final call. This is the approach that leads to convolutional neural networks which
we will explore in detail in Chap.6.
Each of these approaches has its merit, and depending on the problem at hand, any
of them can be a good choice. You can consider other options, deep learning has an
exploratory element to it, and an unorthodox method which contributes to accuracy
will be appreciated.

70
3
Machine Learning Basics
3.6 Learning Without Labels: K-Means
We now turn our attention to two algorithms for unsupervised learning, the K-means
