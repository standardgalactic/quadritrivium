prints out accuracy as a formatted string. Next, we take in the new_data.csv file
which simulates new data on our website and we try to predict what will happen
using the ffnn trained model:
new_data = np.array(pd.read_csv("new_data.csv"))
results = ffnn.predict(new_data)
print(results)
References
1. M. Hassoun, Fundamentals of Artificial Neural Networks (MIT Press, Cambridge, 2003)
2. I.N. da Silva, D.H. Spatti, R.A. Flauzino, L.H.B. Liboni, S.F. dos Reis Alves, Artificial Neural
Networks: A Practical Course (Springer, New York, 2017)
3. I. Goodfellow, Y. Bengio, A. Courville, Deep Learning (MIT Press, Cambridge, 2016)
4. G. Montavon, G. Orr, K.R. Müller, Neural Networks: Tricks of the Trade (Springer, New York,
2012)
5. M. Minsky, S. Papert, Perceptrons: An Introduction to Computational Geometry (MIT Press,
Cambridge, 1969)
6. P.J. Werbos, Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sci-
ences (Harvard University, Cambridge, 1975)
7. D.B. Parker, Learning-logic. Technical Report-47 (MIT Center for Computational Research in
Economics and Management Science, Cambridge, 1985)
8. Y. LeCun, Une procédure d’apprentissage pour réseau a seuil asymmetrique. Proc. Cogn. 85,
599–604 (1985)
9. D.E. Rumelhart, G.E. Hinton, R.J. Williams, Learning internal representations by error propa-
gation. Parallel Distrib. Process. 1, 318–362 (1986)

5
Modifications and Extensions
to a Feed-Forward Neural Network
5.1 The Idea of Regularization
Let us recall the ideas of variance and bias. If we have two classes (denoted by X
and O) in a 2D space and the classifier draws a very straight line we have a classifier
with a high bias. This line will generalize well, meaning that the classification error
for the new points (test error) will be very similar to the classification error for the
old points (training error). This is great, but the problem is that the error will be too
large in the first place. This is called underfitting. On the other hand, if we have a
classifier that draws an intricate line to include every X and none of the Os, then we
have high variance (and low bias), which is called overfitting. In this case, we will
have a relatively low training error a much larger testing error.
Let us take and abstract example. Imagine that we have the task of finding orcas
among other animals. Then our classifier should be able to locate orcas by using
the properties that are common to all orcas but not present in other animals. Notice
that when we said ‘all’ we wanted to make sure we are identifying the species, not a
subgroup of the specie: e.g. having a blue tag on the tail might be something that some
orcas have, but we want to catch only those things that all orcas have and no other
animal has. A ‘species’ in general is called a type (e.g. orcas), whereas an individual
is called a token (e.g. the orca Shamu). We want to find a property that defines the
type we are trying to classify. We call such a property a necessary property. In case
of orcas this might be simply the property (or query):
orca(x) := mammal(x) ∧ livesInOcean(x) ∧ blackAndWhite(x)
But, sometimes it is not that easy to find such a property. Trying to find such
a property is what a supervised machine learning algorithm does. So the problem
might be rephrased as trying to find a complex property which defines a type as
best as possible (by trying to include the biggest possible number of tokens and try
© Springer International Publishing AG, part of Springer Nature 2018
S. Skansi, Introduction to Deep Learning, Undergraduate Topics
in Computer Science, https://doi.org/10.1007/978-3-319-73004-2_5
107

108
5
Modifications and Extensions to a Feed-Forward Neural Network
to include only the relevant tokens in the definition). Therefore, overfitting can be
understood in another way: our classifier is so good that we are not only capturing
the necessary properties from our training examples, but also the non-necessary or
accidental properties. So, we would like to capture all the properties which we need,
but we want something to help us stop when we begin including the non-necessary
properties.
Underfitting and overfitting are the two extremes. Empirically speaking, we can
reallygofromhighbiasandlowvariancetohighvarianceandlowbias.Wanttostopat
a point in between, and we want this point to have better-than-average generalization
capabilities (inherited from the higher bias), and a good fit to the data (inherited from
high variance). How to find this ‘sweet spot’ is the art of machine learning, and the
received wisdom in the machine learning community will insist it is best to find this
by hand. But it is not impossible to automate, and deep learning, wanting to become
a contender for artificial intelligence, will automate as much as possible. There is
one approach which tries to automate our intuitions about overfitting, and this idea
is called regularization.
Why are we talking about overfitting and not underfitting? Remember that if have
a very high bias we will end up with a linear classifier, and linear classifiers cannot
solve the XOR or similar simple problems. What we want then is to significantly
lower the bias until we have reached the point after which we are overfitting. In the
context of deep learning, after we have added a layer to logistic regression, we have
said farewell to high bias and sailed away towards the shores of high variance. This
sounds very nice, but how can we stop in time? How can we prevent overfitting. The
idea of regularization is to add a regularization parameter to the error function E, so
we will have
Eimproved := Eoriginal + RegularizationT erm
Before continuing to the formal definitions, let us see how we can develop a visual
intuition on what regularization does (Fig. 5.1).
The left-hand side of the image depicts the classical various choices of hyperplanes
we usually have (bias, variance, etc.). If we add a regularization term, the effect will
be that the error function will not be able to pinpoint the datapoints exactly, and the
Fig.5.1 Intuition about regularization

5.1 The Idea of Regularization
109
effect will be similar to the points becoming actually little circles. In this way, some
of the choices for the hyperplane will simply become impossible, and the one that are
left will be the ones that have a good “neutral zone” between Xs and Os. This is not
the exact explanation of regularization (we will get to that shortly) but an intuition
which is useful for informal reasoning about what regularization does and how it
