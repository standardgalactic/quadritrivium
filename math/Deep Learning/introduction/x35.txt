we were at classifying. For measuring this, we will be needing an error function and
we will be using the sum of squared error or SSE19:
E = 1
2
�
n
(t(n) − y(n))2
The ts are targets or labels, and the ys are the actual outputs of the model. The
weird exponents (t(n)) are just indices which range across training samples, so (t(k))
would be the target for the kth training row vector. You will see in a moment why
19There are other error functions that can be used, but the SSE is one of the simplest.

3.4
A Simple Neural Network:Logistic Regression
65
do we need such weird notation now and a bit later how to dispense with it. Let us
calculate our SSE:
E = 1
2
�
n
(t(n) − y(n))2 =
(3.1)
= 1
2((1 − 0.8163)2 + (0 − 0.7414)2 + (0 − 0.8368)2) =
(3.2)
= 0.0337 + 0.5496 + 0.7002
2
=
(3.3)
= 0.64175
(3.4)
We now update the w and b by using magic, and get w = (0.1, 0.36, 0.3) and
b = 0.25. Later (in Chap.4), we will see it is actually done by something called the
general weight update rule. This completes one cycle of weight adjustment. This is
colloquially called an epoch, but we will redefine this term later in Chap.4 to make
it more precise. Let us recalculate the outputs and the new SSE to see whether the
new set of weights is better:
ynew
A
= σ(0.25 + 0.1 · 0.2 + 0.36 · 0.5 + 0.3 · 0.91) = σ(0.723) =
1
1 + e−0.723 = 0.6732
(3.5)
ynew
B
= σ(0.25 + 0.1 · 0.4 + 0.36 · 0.01 + 0.3 · 0.5) = σ(0.4436) =
1
1 + e−0.4436 = 0.6091
(3.6)
ynew
C
= σ(0.25 + 0.1 · 0.3 + 0.36 · 1.1 + 0.3 · 0.8) = σ(0.916) =
1
1 + e−1.635 = 0.7142
(3.7)
Enew = 1
2((1 − 0.6732)2 + (0 − 0.6091)2 + (0 − 0.7142)2) =
(3.8)
= 0.1067 + 0.371 + 0.51
2
=
(3.9)
= 0.4938
(3.10)
We can see clearly that the overall error has decreased. We can continue this
procedure a number of times, and the error will decrease, until at one point it will stop
decreasing and stabilize. On rare occasions, it might even exhibit chaotic behaviour.
This is the essence of logistic regression, and the very core of deep learning—
everything we do will be an upgrade or modification of this.
Let us turn our attention to data representation. So far we have used an expanded
view of the process so that we may see clearly everything, but let us see how we
can make the procedure more compact and computationally faster. Notice that even
though a dataset is a set (and the order does not matter), it might make a bit of sense
to put xA, xB and xC in a vector, since we will be using them one by one (the vector
would then simulate a queue or stack). But since they also share the same structures
(same features in the same place in each row vector), we might opt for a matrix

66
3
Machine Learning Basics
to represent the whole training set. This is important in the computational sense as
well since most deep learning libraries have somewhere in the background C, and
arrays (the programming equivalent of matrices) are a native data structure in C, and
computation on them is incredibly fast.
So what we want to do is first turn the n d-dimensional input vectors into and
input matrix of the size n × d. In our case, this is a 3 × 3 matrix:
x =
⎡
⎣
0.2 0.5 0.91
0.4 0.01 0.5
0.3 1.1
0.8
⎤
⎦
We will be keeping the targets (labels) in a separate vector, and we have to be
extremely careful not to shuffle neither the target vector nor the dataset matrix from
this point onwards, since the order of the matrix rows and vector components is the
only thing that can join them again. The target vector in our case is t = (1, 0, 0).
