17A minimal subset such that a property P holds is a subset (of some larger set) of which we can
take no proper subset such that P would still hold.

2.2
Vectors,Matrices and Linear Programming
27
which are a natural extension of vectors. A matrix is a structure similar to a table as
it is made by rows and columns. To understand what a matrix is, take for example
the following matrix and try to make some sense of it with what we have already
covered when we were talking about vectors:
A =
⎡
⎢⎢⎣
a11 a12 a13
a21 a22 a23
a31 a32 a33
a41 a42 a43
⎤
⎥⎥⎦
Right away we see a couple of things. First, the entries in the matrix are denoted
by ajk and j denotes the row, and k denotes the column of the given entry. A matrix
has dimensions similar to a vector, but it has to have two of them. The matrix A
is a 4 × 3 dimensional matrix. Note that this is not the same as a 3 × 4 dimen-
sional matrix. We can look at a matrix as a vector of vectors (this idea has a couple
of formal problems that need to be ironed out, but it is a good intuition). Here,
we have two options: It could be viewed as vectors a1x = (a11, a12, a13), a2x =
(a21, a22, a23), a3x = (a31, a32, a33) and a4x = (a41, a42, a43) stacked in a new vec-
tor A = (a1x, a2x, a3x, a4x) or it could be seen as vectors ax1 = (a11, a21, a31, a41),
ax2 = (a12, a22, a32, a42) and ax3 = (a13, a23, a33, a43) which are then bundled
together as A = (ax1, ax2, ax3).
Either way we look at it something is off since we have to keep track of what is
vertical and what is horizontal. It is clear that now need to distinguish a standard,
horizontal vector, called a row vector (a row of the matrix taken out which is now
just a vector), which is a 1 × n dimensional matrix
ah = (a1, a2, a3, . . . , an) =
�a1 a2 a3 · · · an
�
from a vertical vector called column vector, which is a n × 1 dimensional matrix:
av =
⎡
⎢⎢⎢⎢⎢⎣
a1
a2
a3
...
an
⎤
⎥⎥⎥⎥⎥⎦
We will need an operation to transform row vectors in column vectors and in
general, to transform a m × n dimensional matrix into a n × m dimensional matrix
while keeping the order in both the rows and columns. Such an operation is called a
transposition, and you can imagine it as having a matrix written down on a transparent
sheet of A4 paper in portrait orientation, and then by holding the top-left corner flip
it to landscape orientation (and you read the number through the paper). Formally, if
we have a n × m matrix A, we can define another matrix B as the matrix constructed
from A by taking each ajk and putting it in place of bkj. B is then called the transpose
of A and is denoted by A⊤. Note that transposing a column vector gives a standard

28
2
Mathematical and Computational Prerequisites
row vector and vice versa. Transposition is used a lot in deep learning to keep all
operations running smoothly and quickly. If we have an n × n matrix A (called a
square matrix) for which A = A⊤ holds, then such a matrix is called symmetric.
Now we turn to operations with matrices. We start with scalar multiplication. We
can multiply a matrix A by a scalar s by multiplying each entry in the matrix by the
scalar:
sA =
⎡
⎢⎢⎣
s · a11 s · a12 s · a13
s · a21 s · a22 s · a23
s · a31 s · a32 s · a33
s · a41 s · a42 s · a43
⎤
⎥⎥⎦
And we note that the multiplication of a matrix and a scalar is commutative (matrix
by matrix multiplication will not be commutative). If we want to apply a function
f (x) to a matrix A, we do it by applying the function to all elements:
f (A) =
⎡
⎢⎢⎣
f (a11) f (a12) f (a13)
f (a21) f (a22) f (a23)
f (a31) f (a32) f (a33)
f (a41) f (a42) f (a43)
⎤
⎥⎥⎦
Now, we turn to matrix addition. If we want to add two matrices A and B, they
must have the same dimensions, i.e. they must be both n × m, and then we add18 the
corresponding entries. The result will also be a n × m matrix. To take an example:
A + B =
⎡
⎢⎢⎣
3
−4 5
−19 10 12
1
45 9
−45 −1 0
