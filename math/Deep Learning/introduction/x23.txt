32Note that ideally we would like an estimator to be a perfect predictor of the future in all cases,
but this would be equal to having foresight. Scientifically speaking, we have models and we try to
make them as accurate as possible, but perfect prediction is simply not on the table.
33‘Disjoint’ means A ∩ B = ∅.

2.3
Probability Distributions
37
Finally, we can define the conditional probability of two events. The conditional
probability of A given B (or in logical notation, the probability of B → A) is defined
as
P(A|B) = P(B → A) := P(A ∩ B)
P(B)
(2.16)
Now, we have enough definitions to prove Bayes’ theorem:
Theorem 2.1 P(X |Y) = P(Y|X )P(X )
P(Y)
Proof By the above definition of conditional probability (Eq.2.16), we have that
P(X |Y) = P(X ∩Y)
P(Y) . Now, we must reformulate P(X ∩ Y), and we will also be using
the definition of conditional probability. By substituting X for B and Y for A in
Eq.2.16, we get P(Y|X ) = P(Y∩X )
P(X ) . Since ∩ is commutative, this is the same as
P(Y|X ) = P(X ∩Y)
P(X ) .Now,wemultiplytheexpressionbyP(X )andgetP(Y|X )P(X ) =
P(X ∩ Y). We now know what is P(X ∩ Y) and substitutes it in P(X |Y) = P(X ∩Y)
P(Y)
to get P(X |Y) = P(Y|X )P(X )
P(Y)
, which concludes the proof.
□
This is the first and only proof in this book,34 but we have included it since it is a
very important piece of machine learning culture, and we believe that every reader
should know how to produce it on a blank piece of paper. If we assume conditional
independence of Y1, . . . , Yn, then there is also a generalized form of the Bayes’
theorem to account for multiple conditions (Yall consists of Y1 ∧ . . . ∧ Yn):
P(X |Yall) = P(Y1|X ) · P(Y1|X ) · . . . · P(Yn|X ) · P(X )
P(Yall)
(2.17)
We see in the next chapter how this is useful for machine learning. Bayes’ theorem
is named after Thomas Bayes, who first proved it, but the result was only published
posthumously in 1763.35 The theorem underwent formalization and the first rigorous
formalization was given by Pierre-Simon Laplace in his 1774 Memoir on Inverse
probability and later in his Théorie analytique des probabilités form 1812.A com-
plete treatment of Laplace’s contributions we have mentioned is available in [9,10].
Before leaving the green plains of probability for the desolate mountains of logic
and computability, we must address briefly another probability distribution, the nor-
mal or Gaussian distribution. The Gaussian distribution is characterized by the fol-
lowing formula:
1
√
2 · V AR · π
e− (x−MEAN)2
2·V AR
(2.18)
34There are others, but they are in disguise.
35A version of Bayes’ original manuscript is available at http://www.stat.ucla.edu/
history/essay.pdf.

38
2
Mathematical and Computational Prerequisites
Itisquiteaweirdequation,butthemainthingabouttheGaussiandistributionisnot
the elegance of calculation, but rather the natural and nice shape of the graph, which
can be used in a number of ways. You can see an illustration of how the Gaussian
distribution with mean 0 and standard deviation 1 looks like (see Fig.2.2a).
The idea behind the Gaussian distribution is that many natural phenomena seem
to follow it, and in machine learning it is extremely useful for initializing values
that are random but at the same time are centred around a value. This value is the
mean, and it is usually set to 0, but it can be anything. There is a related concept of
a Gaussian cloud, which is made by sampling a Gaussian distribution with mean 0
for two values at a time, adding the values to a point with coordinates (x, y) (and
drawing the results if one wishes to see it). Visually, it looks like a ‘dot’ made with
the spray paint tool from an old graphical editing program (see Fig.2.2b).
Fig.2.2 Gaussian distribution and Gaussian cloud

2.4
Logic and Turing Machines
39
2.4 Logic and Turing Machines
We have already encountered logic in the very beginnings of artificial neural net-
works, and again with the XOR problem, but we have not really discussed it. Since
logic is a highly evolved and mathematical science, an in-depth introduction to logic
is far beyond the scope of this book, and we point the reader to [11] or [12], which
are both excellent introductions. We are going to give only a very quick tour here,
and focus exclusively on the parts which are of direct theoretical and practical sig-
nificance to deep learning.
Logic is the study of foundations of mathematics, and as such it has to take
something to be undefined. This is called a proposition. Propositions are represented
by symbols A, B, C, P, Q, . . . , A1, B1, . . .. Usually, the first letters are reserved for
atomic propositions, while the Ps and Qs are reserved for denoting any proposi-
tion, atomic or compound. Compound propositions are built over atomic ones with
logical connectives, ∧ (‘and’), ∨ (‘or’), ¬ (‘not’), → (‘if…then’) and ≡ (‘if and
only if’). So if A and B are propositions, so is A → (A ∨ ¬B). All of the connec-
tives are binary, except for negation which is unary. Another important aspect is
truth functions. Intuitively, an atomic proposition is assigned either 0 or 1, and a
compound proposition gets 0 or 1 depending on whether its components are 0 or 1.
So if t(X ) is a truth function, t(A ∧ B) = 1 if and only if t(A) = 1 and t(B) = 1,
t(A ∨ B) = 1 if and only if t(A) = 1 or t(B) = 1, t(A → B) = 0 if and only if
t(A) = 1andt(B) = 0,t(A ≡ B) = 1ifandonlyift(A) = 1andt(B) = 1ort(A) = 0
