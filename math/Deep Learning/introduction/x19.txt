⎡
⎢⎢⎣
8 9 0
1 2 3
4 5 6
7 8 9
⎤
⎥⎥⎦ =
� 30
36
42
110 132 114
�
We show the calculation for all elements of C:
• c11 = 0 · 8 + 1 · 1 + 2 · 4 + 3 · 7 = 30
• c12 = 0 · 9 + 1 · 2 + 2 · 5 + 3 · 8 = 36
• c13 = 0 · 0 + 1 · 3 + 2 · 6 + 3 · 9 = 42
• c21 = 4 · 8 + 5 · 1 + 6 · 4 + 7 · 7 = 110
• c22 = 4 · 9 + 5 · 2 + 6 · 5 + 7 · 8 = 132

30
2
Mathematical and Computational Prerequisites
• c23 = 4 · 0 + 5 · 3 + 6 · 6 + 7 · 9 = 114
Before continuing, we must define two more classes of matrices. The first one is
the zero matrix. A zero matrix can be of any size and all of its entries are zeros. Its
dimensions will depend on what do we want to do with it, i.e. it will depend on the
dimensions of the matrix we want to multiply it with. The second (and much more
useful) is a unit matrix. A unit matrix is always a square matrix (i.e. both dimensions
are the same). It will have the value 1 along the diagonal and all other entries are 0,
i.e. ajk = 1 if and only if j = k and ajk = 0 otherwise. Note that a unit matrix is a
symmetric matrix. Note that there is only one unit matrix for every dimension, so we
can give it a name, In,n. Since it is a square matrix (a n × n matrix), we do not have
to specify both dimensions, so we can just write In. Just to show how they look:
I1 =
�1�
, I2 =
�1 0
0 1
�
, I3 =
⎡
⎣
1 0 0
0 1 0
0 0 1
⎤
⎦ , In =
⎡
⎢⎢⎢⎣
1 0 · · · 0
0 1 · · · 0
... ... ... ...
0 0 · · · 1
⎤
⎥⎥⎥⎦ .
Now we can define orthogonality for matrices. An n × n square matrix A is called
orthogonal if and only if AA⊤ = A⊤A = In.
Notice that vectors had one dimension, so we talked about n-dimensional vectors.
Matrices have 2D parameters, so we talk about n × m matrices. What if we add an
extra dimension? What would be a n × k × j dimensional object? Such objects are
called tensors and behave similarly to matrices. Tensors are an important topic in
deep learning but unfortunately are beyond the scope of this book. We point the
interested reader to [3].
So far we have talked about derivatives and vectors separately, but it is time to
see how they can combine to form the one of the most important structures in deep
learning, the gradient. We have seen how to compute the derivative of a function of
a single variable f (x), but could we extend the notion to multiple variables? Could
we get the slope in a point of a mathematical object that needs two variables to
be defined? The answer is yes, and we do that by employing partial derivatives.
Let us see on an example. Take the simple case of f (x, y) = (x − y)2. First, we must
transform it in x2 − 2xy + y2. Now, we must focus on it as a function of one variable,
which means to treat the other one as an unknown constant: fy(x) = x2 − 2xy + y2,
or even better fa(x) = x2 − 2xa + a2. We are now committed to finding the partial
derivative of f with respect to x. So we are solving df
dx for f (x) = x2 − 2xa + a2 (or
equivalently f ′(x) = x2 − 2xa + a2). Note that we cannot safely use the notation dy
dx
but we must write df
dx to avoid confusion. Since differentiation is linear, by the rule LD
from the previous section we get df
dx x2 − 2a df
dx x + df
dx a2. By using the exponent rule
Exp on the first term, the differentiation variable rule DerDifVar on the second
term and the constant rule Const on the third term, we get 2x − 2a + 0, which
simplifies to 2(x − a). Let us see what we did: we took the (full) derivative of fa(x)
(with a constant a in place of y), which is the same as taking the partial derivative

2.2
Vectors,Matrices and Linear Programming
31
of f (x, y). In symbols, we calculated dfy(x)
dx , and the corresponding partial derivative
is denoted as ∂f (x,y)
∂x
and is obtained by re-substituting the variable we took out with
the constant we put in. In other words ∂f (x,y)
∂x
= 2(x − y).
