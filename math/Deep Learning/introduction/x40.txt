a fragment is something we wish to treat as a training sample. If we are analysing
clinical documents, each patient admission document might be one fragment; if we
are analysing all PhD theses from a major university, each 200-page thesis is one
fragment; if we are analysing sentiment on social media, each user comment is one
fragment; and so on. A bag of words model is made by turning each word from the
corpus in a feature and in each row, under that word, counting how many times the
word occurs in that fragment. Clearly, the order of the words is lost by creating a bag
of words.
The bag of words model is one of the main ways to convert language in features to
be fed to a machine learning algorithm, and only deep learning has good alternatives
to it as we shall see in Chaps.6, 7 and 8. Other machine learning methods use the
bag of words or variations35 almost exclusively, and for many language processing
tasks, the bag of words is a great language model even in deep learning. Let us see
how the bag of words works in a simple social media dataset36:
User Comment
Likes
S. A you dont know
22
F. F as if you know
13
S. A i know what i know 9
P. H i know
43
We need to convert the column ‘Comment’ into a bag of words. The columns
‘User’ and ‘Likes’ are left as they are for now. To create a bag of words from the
comments, we need to make two passes. The first just collects all the words that occur
and turns them into features (i.e. collects the unique words and creates the columns
from them) and the second writes in the actual values:
35An example of an expansion of the basic bag of words model is a bag of n-grams. An n-gram is
a n-tuple consisting of n words that occur next to each other. If we have a sentence ‘I will go now’,
the set of its 2-grams will be {(‘I ′, ‘will′), (‘will′, ‘go′), (‘go′, ‘now′)}.
36For most language processing tasks, especially tasks requiring the use of data collected from social
media, it makes sense to convert all text to lowercase first and get rid of all commas apostrophes
and non-alphanumerics, which we have already done here.

76
3
Machine Learning Basics
User you dont know as if i what Likes
S. A 1
1
1
0 0 0 0
22
F. F 1
0
1
1 1 0 0
13
S. A 0
0
2
0 0 2 1
9
P. H 0
0
1
0 0 1 0
43
Now, we have the bag of words of the column ‘Comment’ and we need to do
one-hot encoding on the column ‘User’ before being able to feed the dataset in a
machine learning algorithm. We do this as we have explained earlier and get the final
input matrix:
S. A F. F P. H you dont know as if i what Likes
1
0
0
1
1
1
0 0 0 0
22
0
1
0
1
0
1
1 1 0 0
13
1
0
0
0
0
2
0 0 2 1
9
0
0
1
0
0
1
0 0 1 0
43
This example shows the difference between one-hot encoding and the bag of
words. In one-hot, each row has only 1 or 0 and, moreover, it must have exactly
one 1. This means that it can be represented rather compactly by noting only the
column number where it is 1. Take the fourth example in the upper column: we
