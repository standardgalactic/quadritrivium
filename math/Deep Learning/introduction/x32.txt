There is a standard way to display the number of true positives (TP), false positives
(FP), true negatives (TN) and false negatives (FN) in a more visual way and this
method is called a confusion matrix. For a two-class classification (also known as
binary classification), the confusion matrix is a 2 × 2 table of the form:
Classifier says YES
Classifier says NO
In reality YES Number of true positives
Number of false negatives
In reality NO
Number of false negatives Number of true negatives
Once we have a confusion matrix, precision, recall, accuracy and any other eval-
uation, metric can be calculated directly from it.
The values for all classifier evaluation metrics range from 0 to 1 and can be
interpreted as probabilities. Note that there are trivial modifications that can make
either the precision or recall reach 100% (but not both at the same time). If we want
the precision to be 1, we can simply make a classifier that selects no datapoint, i.e.
for each datapoint it should say ‘O’. The opposite works for recall: just select all
datapoints as Xs, and recall will be 1. This is why we need all three metrics to get a
meaningful insight on how good a classifier is and how to compare two classifiers.
Now that we know about evaluation metrics, let us turn to the question of evalu-
ating the classifier performance from a procedural point of view. When faced with a
classification task, as noted earlier we have a classification algorithm and a training
set. We train the algorithm on the training set and now we are ready to use it for
prediction. But where is the evaluation part? The usual strategy is not to use the
whole training set for training, but keep a part of it for testing. This is usually 10%,
but it can be more or less than that.10 The 10% we held out and did not train on it
is called the test set. In the test set, we separate the labels from the other features,
so that we have row vectors of the same form we would be getting when predicting.
When we have a trained model on the 90% (the training set), we use it to classify the
test set, and we compare the classification results with the labels. In this way, we get
the necessary information for calculating the precision, recall, and accuracy. This is
9If we will be needing more we will keep more decimals, but in this book we will usually round off
to four.
10It is mostly a matter of choice, there is no objective way of determining how much to split.

3.2
Evaluating Classification Results
59
called splitting the dataset in training and testing sets or simply the train–test split.
The test set is designed to be a controlled simulation of how well will the classifier
behave. This approach is sometimes called out-of-sample validation to distinguish it
from out-of-time validation where the 10% of the data are not chosen randomly from
all datapoints, but a time period spanning around 10% of the datapoints is chosen.
Out-of-time validation is generally not recommended since there might be seasonal
trends in the data which would seriously cripple the evaluation.
3.3 A Simple Classifier: Naive Bayes
In this section, we sketch the simplest classifier we will explore in this book, called
the naive Bayes classifier. The naive Bayes classifier has been used from at least 1961
[5], but, due to its simplicity, it is hard to pinpoint where research on the applications
of Bayes’ theorem ends and the research on the naive Bayes classifier begins.
The naive Bayes classifier is based on Bayes’ theorem which we saw earlier in
Chap.2 (this accounts for the ‘Bayes’ in the name), and it makes and additional
assumption that all features are conditionally independent from each other (this
is why there is ‘Naive’ in the name). This means that each feature carries ‘its own
weight’ in terms of predictive power: there is no piggy-backing or synergy of features
going on. We will rename the variables in the Bayes theorem to give it a more
‘machine learning feel’:
P(t| f ) = P( f |t)P(t)
P( f )
,
where P(t) is the prior probability11 of a given target value (i.e. the class label), P( f )
is the prior probability of a feature, P( f |t) is the probability of the feature f given
the target t, and, of course, P(t| f ) is the probability of the target t given only the
feature f which is what we want to find.
Recall from Chap.2 that we can convert Bayes’ theorem to accommodate for a
(n-dimensional) vector of features, and in that case we have the following formula:
P(t| fall) = P( f1|t) · P( f2|t) · . . . · P( fn|t) · P(t)
P( fall)
Let us see a very simple example to demonstrate how the naive Bayes classifier
works and how it draws its hyperplane. Imagine that we have the following table
detailing visits to a webpage:
We first need to convert this into a table with counts (called a frequency table,
similar to one-hot, but not exactly the same):
Now, we can calculate some basic prior probabilities. The probability of ‘yes’ is
9
13 = 0.6923. The probability of ‘no’ is 4
13 = 0.3076. The probability of ‘morning’
11The prior probability is just a matter of counting. If you have a dataset with 20 datapoints and in
some feature there are five values of ‘New Vegas’ while the others (15 of them) are ‘Core region’,
the prior probability P(New Vegas) = 0.25.

60
3
Machine Learning Basics
Time
Buy
morning
no
afternoon yes
evening
yes
morning
yes
morning
yes
afternoon yes
evening
no
evening
yes
