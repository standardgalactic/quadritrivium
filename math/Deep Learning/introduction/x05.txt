from Alan Turing in 1936 [1] (we will return to Turing shortly), but would become
clear to a wider scientific community only in the late 1970s with the advent of the
personal computer. The ideas of the characteristica universalis and the calculus rati-
ocinator are Leibniz’ central ideas, and are scattered throughout his work, so there
is no single point to reference them, but we point the reader to the paper [2], which
is a good place to start exploring.
The journey towards deep learning continues with two classical nineteenth century
worksinlogic.Thisisusuallyomittedsinceitisnotclearlyrelatedtoneuralnetworks,
there was a strong influence, which deserves a couple of sentences. The first is John
Stuart Mill’s System of Logic from 1843 [3], where for the first time in history,
logic is explored in terms of a manifestation of a mental process. This approach,
© Springer International Publishing AG, part of Springer Nature 2018
S. Skansi, Introduction to Deep Learning, Undergraduate Topics
in Computer Science, https://doi.org/10.1007/978-3-319-73004-2_1
1

2
1
From Logic to Cognitive Science
called logical psychologism, is still researched only in philosophical logic,1 but even
in philosophical logic it is considered a fringe theory. Mill’s book never became
an important work, and his work in ethics overshadowed his contribution to logical
psychologism, but fortunately there was a second book, which was highly influential.
It was the Laws of Thought by George Boole, published in 1854 [4]. In his book,
Boole systematically presented logic as a system of formal rules which turned out
to be a major milestone in the reshaping of logic as a formal science. Quickly after,
formal logic developed, and today it is considered a native branch of both philosophy
and mathematics, with abundant applications to computer science. The difference in
these ‘logics’ is not in the techniques and methodology, but rather in applications.
The core results of logic such as De Morgan’s laws, or deduction rules for first-order
logic, remain the same across all sciences. But exploring formal logic beyond this
would take us away from our journey. What is important here is that during the first
half of the twentieth century, logic was still considered to be something connected
with the laws of thinking. Since thinking was the epitome of intelligence, it was only
natural that artificial intelligence started out with logic.
Alan Turing, the father of computing, marked the first step of the birth of artifi-
cial intelligence with his seminal 1950 paper [5] by introducing the Turing test to
determine whether a computer can be regarded intelligent. A Turing test is a test in
natural language administered to a human (who takes the role of the referee). The
human communicates with a person and a computer for five minutes, and if the ref-
eree cannot tell the two apart, the computer has passed the Turing test and it may be
regarded as intelligent. There are many modifications and criticism, but to this day
the Turing test is one of the most widely used benchmarks in artificial intelligence.
The second event that is considered the birth of artificial intelligence was the
DartmouthSummerResearchProjectonArtificialIntelligence.Theparticipantswere
John McCarthy, Marvin Minsky, Julian Bigelow, Donald MacKay, Ray Solomonoff,
John Holland, Claude Shannon, Nathanial Rochester, Oliver Selfridge, Allen Newell
and Herbert Simon. Quoting the proposal, the conference was to proceed on the basis
of the conjecture that every aspect of learning or any other feature of intelligence
can in principle be so precisely described that a machine can be made to simulate
it.2 This premise made a substantial mark in the years to come, and mainstream
AI would become logical AI. This logical AI would go unchallenged for years,
and would eventually be overthrown only in the 21 millennium by a new tradition,
known today as deep learning. This tradition was actually older, founded more than
a decade earlier in 1943, in a paper written by a logician of a different kind, and
his co-author, a philosopher and psychiatrist. But, before we continue, let us take a
small step back. The interconnection between logical rules and thinking was seen as
directed. The common knowledge is that the logical rules are grounded in thinking.
Artificial intelligence asked whether we can impersonate thinking in a machine with
1Today, this field of research can be found under a refreshing but very unusual name: ‘logic in the
wild’.
2The full text of the proposal is available at https://www.aaai.org/ojs/index.php/
aimagazine/article/view/1904/1802.

1.1
The Beginnings of Artificial Neural Networks
3
logical rules. But there was another direction which is characteristic of philosophical
logic: could we model thinking as a human mental process with logical rules? This
is where the neural network history begins, with the seminal paper by Walter Pitts
and Warren McCulloch titled A Logical Calculus of Ideas Immanent in Nervous
Activity and published in the Bulletin of Mathematical Biophysics. A copy of the
paper is available at http://www.cs.cmu.edu/~epxing/Class/10715/
reading/McCulloch.and.Pitts.pdf, and we advise the student to try to
read it to get a sense of how deep learning began.
Warren McCulloch was a philosopher, psychologist and psychiatrist by degree,
but he would work in neurophysiology and cybernetics. He was a vivid character,
embodying many academic stereotypes, and as such was a curious person whose
interests could be described as interdisciplinary. He met the homeless Walter Pitts
in 1942 when he got a job at the Department of Psychiatry at the University of
Chicago, and invited Pitts to come to live with his family. They shared a lifelong
interested in Leibniz, and they wanted to bring his ideas to fruition an create a
machine which could implement logical reasoning.3 The two men worked every
night on their idea of capturing reasoning with a logical calculus inspired by the
biological neurons. This meant constructing a formal neuron with capabilities similar
to that of a Turing machine. The paper had only three references, and all of them
are classical works in logic: Carnap’s Logical Syntax of Language [6], Russell’s and
Whitehead’s Principa Mathematica [7] and the Hilbert and Ackermann Grundüge
der Theoretischen Logik. The paper itself approached the problem of neural networks
as a logical one, proceeding from definitions, over lemmas to theorems.
Their paper introduced the idea of the artificial neural network, as well as some
of the definitions we take for granted today. One of these is what would it mean for a
logical predicate to be realizable on a neural network. They divided the neurons in two
groups, the first called peripheral afferents (which are now called ‘input neurons’),
and the rest, which are actually output neurons, since at this time there was no hidden
layer—the hidden layer came to play only in the 1970s and 1980s. Neurons can be in
two states, firing and non-firing, and they define for every neuron i a predicate which
is true when the neuron is firing at the moment t. This predicate is denoted as Ni(t).
The solution of a network is then an equivalence of the form Ni(t) ≡ B where B is
a conjunction of firings from the previous moment of the peripheral afferents, and i
