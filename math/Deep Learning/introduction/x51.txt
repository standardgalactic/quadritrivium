We take these values back to the chain rule and get:

100
4
Feedforward Neural Networks
∂E
∂w5
= ∂E
∂yF
· ∂yF
∂zF
· ∂zF
∂w5
= −0.3844 · 0.2365 · 0.5868 = −0.0533
We repeat the same process19 to get
∂E
∂w6 = −0.0535. Now, all we have to do is
use these values in the general weight update rule20 (we use a learning rate, η = 0.7):
wnew
5
= wold
5
− η ∂E
∂w5
= 0.2 − (0.7 · 0.0533) = 0.2373
wnew
6
= 0.6374
Now we can continue to the next layer. But an important note first. We will be
needing a value for w5 and w6 to find the derivatives of w1, w2, w3 and w4, and we
will be using the old values, not the updated ones. We will update the whole network
when we will have all the updated weights. We proceed to the hidden layer. What we
need to now is to find the update for w3. Notice that to get from the output neuron F
to w3 we need to go across C, so we will be using:
∂E
∂w3
= ∂E
∂yC
· ∂yC
∂zC
· ∂zC
∂w3
The process will be similar to ∂E
∂w3 , but with a couple of modifications. We start
with:
∂E
∂yC
= ∂zF
∂yC
∂E
∂zF
= w5
∂E
∂zF
= w5
∂yF
∂zF
· ∂E
∂yF
= 0.2 · 0.2365 · (−0.3844) = 0.2 · (−0.0909) = −0.0181
Now we need ∂yC
∂zC :
∂yC
∂zC
= yC(1 − yC) = 0.5868 · (1 − 0.5868) = 0.2424
And we also need ∂zC
∂w3 . Recall that zC = x1 · w1 + x2 · w2, and therefore:
∂zC
∂w3
= x1 · 0 + x2 · 1 = x2 = 0.82
Now we have:
∂E
∂w3
= ∂E
∂yC
· ∂yC
∂zC
· ∂zC
∂w3
= −0.0181 · 0.2424 · 0.82 = −0.0035
19The only difference is the step for ∂zF
∂w5 , where there is a 0 now for w5 and a 1 for w6.
20Which we discussed earlier, but we will restate it here: wnew
k
= wold
k
− η ∂E
∂wk .

4.6
Backpropagation
101
Using the general weight update rule we have:
wnew
3
= 0.4 − (0.7 · (−0.0035)) = 0.4024
We use the same steps (across C) to find wnew
1
= 0.1007. To get wnew
2
