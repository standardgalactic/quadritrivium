we focus on the most famous of the neural language models, the Word2vec model,
which learns vectors which represent words with a simple neural network.
This is similar to the predict-next setting for recurrent neural networks, but it
gives an added bonus: we can calculate word distances and have similar words only
a short distance away. Traditionally, we can measure the distances of two words
as strings with the Hamming distance [1]. For measuring the Hamming distance,
two strings have to be of the same length and the distance is simply the number of
characters that are different. The Hamming distance between the words ‘topos’ and
‘topoi’ is 1, while the distance between ‘friends’ and ‘fellows’ is 5. Note that the
distance between ‘friends’ and ‘0r$8MMs’ is also 5. It can easily be normalized to
a percentage by dividing it by the words’ length. You can probably see already how
this would be a useful but very limited technique for processing language.
The Hamming distance is the simplest method from a wide variety of string
similarity measures collectively known as string edit distance metrics. More evolved
forms such as Levenshtein distance [2] or Jaro–Winkler [3,4] distance can compare
strings of different lengths and penalize differently various errors, such as insertion,
deletion or edit. All of these are measures of a word by the form of the word. They
would be useless in comparing ‘professor’ and ‘teacher’, since they would never
recognize the similarity in meaning. This is why, we want to embed a word in a
© Springer International Publishing AG, part of Springer Nature 2018
S. Skansi, Introduction to Deep Learning, Undergraduate Topics
in Computer Science, https://doi.org/10.1007/978-3-319-73004-2_9
165

166
9
Neural Language Models
vector in a way which will convey information about the meaning of the word (i.e.
its use in our language).
If we represent words as vectors, we need to have a distance measure between
vectors. We have touched upon this idea a number of times before, but we can now
introduce the notion of the cosine similarity of vectors. A good overview of cosine
similarity is given in [5]. Cosine similarity of two n-dimensional vectors v and u is
given by:
CS(v, u) :=
v · u
||v|| · ||u|| =
�n
i=1 viui
��n
i=1 v2
i
��n
i=1 u2
i
(9.1)
Where vi and ui are components of v and u, and ||v|| and ||u|| denote the norms
of the vectors v and u respectively. The cosine similarity ranges from 1 (equal) to −1
(opposite), and 0 means that there is no correlation. When using the bag of words,
one-hot encoding s or similar word embeddings the cosine similarity ranges from 0
to 1, since the vectors representing fragments do not contain negative components.
This means that 0 takes the meaning of ‘opposite’ in such contexts.
We will now continue to show the Word2vec neural language model [6]. In par-
ticular, we will address the questions of what input does it need, what will it give
as an output, does it have parameters to tune it and how can we use it in a complete
system, i.e. how does it interact with other components of a bigger system.
9.2 CBOW and Word2vec
The Word2vec model can be built with two different architectures, the skip-gram
and the Word2vec. Both of these are actually shallow neural networks with a twist.
To see the difference, we will use the sentence ‘Who are you, that you do not know
your history?’. First, we clean the sentence from uppercase and interpunction. Both
architectures use the context of the word (the words around it) as well as the word
itself. We must define in advance how large will the context be. For the sake of
simplicity, we will be using a context of size 1. This means that the context of a word
consists of one word before and one word after. Let us break or sentence into word
and context pairs:
We have already noted that both versions of the Word2vec are learned models, and
this means they must learn something. The skip-gram model learns to predict a word
from the context given the middle word. This means that if we give the model ‘are’
it should predict ‘who’, if we give it ‘know’ it should predict ‘not’ or ‘your’. The
CBOW version does the opposite, assuming the context to be 1, it takes two words1
from the context (we will call them c1 and c2) and uses it to predict the middle or
main word (which we will denote by m).
1If the context were 2, it would take 4 words, two before the main word and two after.

9.2
CBOW and Word2vec
167
Context
Word
‘are’
‘who’
‘who’, ‘you’
‘are’
‘are’, ‘that’
‘you’
‘you’, ‘you’
‘that’
‘that’, ‘do’
‘you’
‘you’, ‘not’
‘do’
‘do’, ‘know’
‘not’
‘not’, ‘your’
‘know’
‘know’, ‘history’ ‘your’
‘your’
‘history’
The production of the word embeddings is structurally quite similar to autoen-
