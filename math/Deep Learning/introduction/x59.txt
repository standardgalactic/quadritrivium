neuron. Let us revisit the weight updates we calculated:
• wold
1
= 0.1, wnew
1
= 0.1007
• wold
2
= 0.5, wnew
2
= 0.502
• wold
3
= 0.4, wnew
3
= 0.4024
• wold
4
= 0.3, wnew
4
= 0.307
• wold
5
= 0.2, wnew
5
= 0.2373
• wold
6
= 0.6, wnew
6
= 0.6374
Just by looking at the amount of the weight update you might notice that two
weights have been updated with a significantly larger amount than the other weights.
These two weights (w5 and w6) are the weights connecting the output layer with the
hidden layer. The rest of the weights connect the input layer with the hidden layer. But
why are they larger? The reason is that we had to backpropagate through few layers,
and they remained larger: backpropagation is, structurally speaking, just the chain
rule.Thechainruleisjustmultiplicationofderivatives.And,derivativesofeverything
we needed10 have values between 0 and 1. So, by adding layers through which we
had to backpropagate, we needed to multiply more and more 0 to 1 numbers, and this
generally tends to become very small very quickly. And this is without regularization,
with regularization it would be even worse, since it would prefer small weights at all
times(sincetheweightupdateswouldbesmallbecauseofthederivatives,therewould
be little chance of the unregularized part to increase the weights). This phenomena
is called vanishing gradient.
We could try to circumvent this problem by initializing the weights to a very large
value and hope that backpropagation will just chip them to the correct value.11 In
this case, we might get a very large gradient which would also hinder learning since
a step in the direction of the gradient would be the right direction but the magnitude
of the step would take us farther away from the solution than we were before the
step. The moral of the story is that usually the problem is the vanishing gradient, but
9A single hidden layer with two neurons in it. It it was (3, 2, 4, 1) we would know it has two hidden
layer, the first one with two neurons and the second one with four.
10Ok, we have used the adjusted the values to make this statement true. Several of the derivatives we
need will become a value between 0 and 1 soon, but it the sigmoid derivatives are mathematically
bound between 0 and 1, and if we have many layers (e.g. 8), the sigmoid derivatives would dominate
backpropagation.
11If the regular approach was something like making a clay statue (removing clay, but sometimes
adding), the intuition behind initializing the weights to large values would be taking a block of stone
or wood and start chipping away pieces.

5.5
Problems for Multiple Hidden Layers:Vanishing and Exploding Gradients
119
if we change radically our approach we would be blown in the opposite direction
which is even worse. Gradient descent, as a method, is simply too unstable if we use
many layers through which we need to backpropagate.
To put the importance of the the vanishing gradient problem, we must note that
the vanishing gradient is the problem to which deep learning is the solution. What
truly defines deep learning are the techniques which make possible to stack many
layers and yet avoid the vanishing gradient problem. Some deep learning techniques
deal with the problem head on (LSTM), while some are trying to circumvent it (con-
volutional neural networks), some are using different connections than simple neural
networks (Hopfield networks), some are hacking the solution (residual connections),
while some have been using weird neural network phenomena to gain the upper hand
(autoencoders). The rest of this book is devoted to these techniques and architectures.
Historically speaking, the vanishing gradient was first identified by Sepp Hochreiter
in 1991in his diploma thesis [15]. His thesis advisor was Jürgen Schmidhuber, and
the two will develop one of the most influential recurrent neural network architec-
tures (LSTM) in 1997 [16], which we will explore in detail in the following chapters.
An interesting paper by the same authors which brings more detail to the discussion
of the vanishing gradient is [17].
We make a final remark before continuing to the second part of this book. We have
chosen what we believe to be the most popular and influential neural architectures,
but there are many more and many more will be discovered. The aim of this book
is not to provide a comprehensive view of everything there is or will be, but to
help the reader acquire the knowledge and intuition needed to pursue research-level
deep learning papers and monographs. This is not a final tome about deep learning,
but a first introduction which is necessarily incomplete. We made a serious effort
to include a range of neural architectures which will demonstrate to the reader the
vast richness and fulfilling diversity of this amazing field of cognitive science and
artificial intelligence.
References
1. A.N. Tikhonov, On the stability of inverse problems. Dokl. Akad. Nauk SSSR 39(5), 195–198
(1943)
2. A.N. Tikhonov, Solution of incorrectly formulated problems and the regularization method.
Sov. Math. 4, 1035–1038 (1963)
3. M.A. Nielsen, Neural Networks and Deep Learning (Determination Press, 2015)
4. R. Tibshirani, Regression shrinkage and selection via the lasso. J. Roy. Stat. Soc. Ser B
(Methodol.) 58(1), 267–288 (1996)
