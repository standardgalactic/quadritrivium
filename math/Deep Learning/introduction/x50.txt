wnew = wold − η∇E
Informally speaking, the learning rate controls by how much we should update.
There are a couple of possibilities (we will discuss the learning rate in more detail
later):
1. Fixed learning rate
2. Adaptable global learning rate
3. Adaptable learning rate for each connection
We will address these issues in more detail later, but before that, we will show a
detailed calculation for error backpropagation in a simple neural network, and in the
next section, we will code the network. The remainder of this chapter is probably
the most important part of the whole book, so be sure to go through all the details.
Let us see a working example18 of a simple and shallow feedforward neural
network. The network is represented in Fig.4.5. Using the notation, the starting
weights and the inputs specified in the image, we will calculate all the intricacies of
the forward pass and backpropagation for this network. Notice the enlarged neuron
D. We have used this to illustrate, where the logit zD is and how it becomes the output
of D (yD) by applying to it the logistic function σ.
We will assume (as we did previously) that all the neurons have a logistic activation
function. So we need to do a forward pass, a backpropagation, and a second forward
pass to see the decrease in the error. Let us briefly comment on the network itself.
Our network has three layers, with the input and hidden layers consisting of two
neurons, and the output error which consists of one neuron. We have denoted the
layers with capital letters, but we have skipped the letter E to avoid confusing it with
the error function, so we have neurons named A, B, C, D and F. This is not usual.
17We must then use the gradient, not individual partial derivatives.
18This is a modified version of the example by Matt Mazur available at https://mattmazur.
com/2015/03/17/a-step-by-step-backpropagation-example/.

98
4
Feedforward Neural Networks
Fig.4.5 Backpropagation in a complete simple neural network
The usual procedure is to name them by referring to the layer and neuron in the layer,
e.g. ‘third neuron in the first layer’ or ‘1, 3’. The input layer takes in two inputs, the
neuron A takes in xA = 0.23 and the neuron B takes in xB = 0.82. The target for this
training case (consisting of xA and xB) will be 1. As we noted earlier, the hidden and
output layers have the logistic activation function (also called logistic nonlinearity),
which is defined as σ(z) =
1
1+e−z .
We start by computing the forward pass. The first step is to calculate the outputs
of C and D, which are referred to as yC and yD, respectably:
yC = σ(0.23 · 0.1 + 0.82 · 0.4) = σ(0.351) = 0.5868
yD = σ(0.23 · 0.5 + 0.82 · 0.3) = σ(0.361) = 0.5892
And now we use yC and yD as inputs to the neuron F which will give us the final
result:
yF = σ(0.5868 · 0.2 + 0.5892 · 0.6) = σ(0.4708) = 0.6155
Now, we need to calculate the output error. Recall that we are using the mean
squared error function, i.e. E = 1
2(t − y)2. So we plug in the target (1) and output
(0.6155) and get:

4.6
Backpropagation
99
E = 1
2(t − y)2 = 1
2(1 − 0.6155)2 = 0.0739
Now we are all set to calculate the derivatives. We will explain how to calculate
w5 and w3 but all other weights are calculated with the same procedure. As back-
propagation proceeds in the opposite direction that the forward pass, calculating w5
is easier and we will do that first. We need to know how the change in w5 affects E
and we want to take those changes which minimize E. As noted earlier, the chain
rule for derivatives will do most of the work for us. Let us rewrite what we need to
calculate:
∂E
∂w5
= ∂E
∂yF
· ∂yF
∂zF
· ∂zF
∂w5
We have found the derivatives for all of these in the previous sections so we will
not repeat their derivations. Note that we need to use partial derivatives because
every derivation is made with respect to an indexed term. Also, note that the vector
containing all partial derivatives (for all indices i) is the gradient. Let us address ∂E
∂yF
now. As we have seen earlier:
∂E
∂yF
= −(t − yF)
In our case that means:
∂E
∂yF
= −(1 − 0.6155) = −0.3844
Now we address ∂yF
∂zF . We know that this is equal to yF(1 − yF). In our case this
means:
∂yF
∂zF
= yF(1 − yF) = 0.6155(1 − 0.6155) = 0.2365
The only thing left to calculate is ∂zF
∂w5 . Remember that:
zF = yC · w5 + yD · w6
By using the rules of differentiation (derivatives of constants (w6 is treated like a
constant) and differentiating the differentiation variable) we get:
∂zF
∂w5
= yC · 1 + yD · 0 = yC = 0.5868
