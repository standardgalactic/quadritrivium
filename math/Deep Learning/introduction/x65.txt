and add a third dimension along which they will be ‘glued’. This simply means if we
have 1000 M by L f inal matrices, that we will make one M by L f inal by 1000 tensor.
Depending on the implementation you will use, it might make sense to make a 1000
by M by L f inal tensor. Now initialize this tensor (a 3D Numpy array) with all zeros,
and devise a function which will put a 1 where it should be. Try to write Keras code
which implements this architecture. As always, if you get stuck, StackOverflow it.
If you have never done anything similar before, it might take you even a week12 to
get it to work, even though the end result does not have many lines of code. This is
a great exercise in deep learning, so don’t skip it.
References
1. Y. LeCun, L. Bottou, Y. Bengio, P. Haffner, Gradient-based learning applied to document recog-
nition. Proc. IEEE 86(11), 2278–2324 (1998)
12A couple of hours each day—not a literal week.

References
133
2. D.H. Hubel, T.N. Wiesel, Receptive fields and functional architecture of monkey striate cortex.
J. Physiol. 195(1), 215–243 (1968)
3. X. Zhang, J. Zhao, Y. LeCun, Character-level convolutional networks for text classification, in
Advances in Neural Information Processing Systems 28, NIPS (2015)

7
Recurrent Neural Networks
7.1 Sequences of Unequal Length
Let us take bird’s eye view of things. Feedforward neural networks can process
vectors,andconvolutionalneuralnetworkscanprocessmatrices(whicharetranslated
into vectors). How would we process sequences of unequal length? If we are talking
about, e.g. images of different sizes, then we could simply re-scale them to match.
If we have a 800 by 600 image and a 1600 by 1200, it is obvious we can simply
resize one of the images. We have two options. The first option is to make the bigger
picture smaller. We could do this in two ways: either by taking the average of four
pixels, or by max-pooling them. On the other hand, we can similarly make the image
bigger by interpolating pixels. If the images do not scale nicely, e.g. one is 800 by
600 an the other is 800 by 555, we can simply expand the image in one direction.
The deformations made will not affect the image processing since the image will
retain most of the shapes. A case where it would affect the neural network would be
if we were to build a classifier to discriminate between ellipses and circles and then
resize the images, since that would make circles look like ellipses. Note, that if all
matrices, we analyse are of the same size they can be represented by long vectors, as
we have seen in the section on MNIST. If they vary in size, we cannot encode them
as vectors and keep the nice properties since the rows would be of different lengths.
If all images are 20 by 20, then we can translate them in a vector of size 400. This
means that the second pixel in the third row of the image is the 43 component of the
400-dimensional vector. If we have two images one 20 by 20 and one 30 by 30, then
the 43rd component of the ?-dimensional vector (suppose for a second that we can
fit a dimensionality here somehow), would be the second pixel in the third row of the
first image and the thirteenth pixel of the second row of the second image. But, the
real problem is how to fit vectors of different dimensions (400 and 300) in a neural
network. Everything we have seen so far, needs a fixed-dimensional vectors.
© Springer International Publishing AG, part of Springer Nature 2018
S. Skansi, Introduction to Deep Learning, Undergraduate Topics
in Computer Science, https://doi.org/10.1007/978-3-319-73004-2_7
135

136
7
Recurrent Neural Networks
The problem of varying dimensionality can be seen as the problem of learning
sequences of unequal length, and audio processing is a nice example of how we
might need this, since various audio clips are necessarily of different lengths. We
could in theory just take the longest and then make all others of the same length as
that one, but this is waste in terms of the space needed. But there is a deeper problem
here. Silence is a part of language, and it is often used for communicating meaning,
so a sound clip with some content labeled with the label 1in the training set might
be correct, but if add 10 s of silence at the beginning or the end of the clip, the
label 1 might not be appropriate anymore, since the clip with the silence may have
a different meaning. Think about irony, sarcasm and similar phenomena.
So the question is what we can do? The answer is that we need a different nerural
network architecture than we have seen before. Every neural network we have seen
so far has connections which push the information forward, and this is why we have
called them ‘feedforward neural networks’. It will turn out that by having connections
that feed the output back into a layer as inputs, we can process sequences of unequal
length. This makes the network deep, but it does share weights so it partly avoids
the vanishing gradient problem. Networks that have such feedback loops are called
recurrent neural networks. In the history of recurrent neural networks, there is an
interesting twist. As soon as the idea of the perceptron did not seem good, the idea
of making a ‘multi-layer perceptron’ seemed natural. Remember that this idea was
theoretical and predated backpropagation (which was widely accepted after 1986),
which means that no one was able to make it work back then. Among the theoretical
ideas explored was adding a single layer, adding multiple layers and adding feedback
loops, which are all natural and simple ideas. This was before 1986.
Since backpropagation was not yet available, J. J. Hopfield introduced the idea of
Hopfield networks [1], which can be thought of the first successful recurrent neural
networks. We will explore them in detail in Chap.10. They were specific since they
were different from what we consider today to be recurrent neural networks. The
most important recurrent neural networks are the long short-term memory networks
or LSTMs which were invented in 1997 by Hochreiter and Schmidhuber [2]. To this
day, they remain the most widely used recurrent neural networks and are responsible
for many state-of-the-art results in various fields, from speech recognition to machine
translation. In this chapter, we will focus on developing the necessary concepts to
explain the LSTM in detail.
7.2 The Three Settings of Learning with Recurrent Neural
Networks
Let us return a bit to the naive Bayes classifier. As we saw in Chap.3, the naive Bayes
classifier calculates P(target| f eatures) after we calculate P( f eature1|target),
P( f eature2|target), etc., from the dataset. This is how the naive Bayes clas-
sifier works, but all classifiers (supervised learning algorithms) try to calculate
P(target| f eatures) or P(t|x) in some way. Recall that any predicate P such that

7.2
