late the gradients of the error E with respect to wx, wh and wo.
When we we were talking of the MSE and SSE error functions, we have seen
that we resort to summing up the errors, and that this is good enough for machine
learning. We can also just sum up the gradients for each training sample at a given
point in time:
∂E
∂wi
=
�
t
∂Et
∂wi
.
(7.14)

7.6
Using a Recurrent Neural Network for Predicting Following Words
151
Let us see how this works in a whole example. Say, we want to calculate the
gradient for E2:
∂E2
∂wo
= ∂E2
∂y2
∂y2
∂z2
∂z2
∂wo
.
(7.15)
This means that for wo the time component plays no part. As expected, for wh
(wx is similar) it is a bit different which is as follows:
∂E2
∂wh
= ∂E2
∂y2
∂y2
∂h2
∂h2
∂wh
.
(7.16)
But remember that h2 = fh(whh1 + wxx2) which means the whole expression
depends on h1, so if we want the derivative with respect to wh we cannot treat it as
a constant. The proper way to do it is to split the last term into a sum as follows:
∂h2
∂wh
=
2
�
i=0
∂h2
∂hi
∂hi
∂wh
.
(7.17)
So, except for the summation, backpropagation through time is exactly the same as
standard backpropagation. This simplicity of calculation is actually the reason why
SRNs are more resistant to the vanishing gradient than a feedforward network with
the same number of hidden layers. Let us address a final issue. The error function we
have previously used was MSE, and this is a valid choice for regression and binary
classification. A better choice for multi-class classification is the cross-entropy error
function, which is defined as
C E = −1
n
�
i∈curr Batch
(ti ln yi + (1 − yi) ln(1 − yi)).
(7.18)
Where t is the target, y is the classifier outcome, i is the dummy variable which
iterates over the current batch targets and outputs, and n is the number of all samples
in the batch. The cross-entropy error function is derived from the log-likelihood,
but this derivation is rather tedious and beyond our needs so we skip it. The cross-
entropy is a more natural choice of error functions, but it is less straightforward
to understand conceptually, so we used the MSE throughout this book, but you
will want to use the CE for all multiclass classification tasks. The Keras code is
loss=categorical_crossentropy, but feel free to browse all loss functions
https://keras.io/losses/, you might be surprised to find some functions

152
7
Recurrent Neural Networks
which we will discuss in a different context can also be used as a loss or error function
in neural network training. In fact, finding or defining a good loss function is often
a very important part of getting a good accuracy with a deep learning model.
References
1. J.J. Hopfield, Neural networks and physical systems with emergent collective computational
abilities. Proc. Nat. Acad. Sci. U.S.A 79(8), 2554–2558 (1982)
2. S. Hochreiter, J. Schmidhuber, Long short-term memory. Neural Comput. 9(8), 1735–1780
(1997)
3. J.L. Elman, Finding structure in time. Cogn. Sci. 14, 179–211 (1990)
4. M.I. Jordan, Attractor dynamics and parallelism in a connectionist sequential machine, in Pro-
ceedings of the 26th Annual International Conference on Machine Learning, Erlbaum, NJ, USA
(Cognitive Science Society, 1986), pp. 531–546
5. A. Graves, Supervised Sequence Labelling with Recurrent Neural Networks (Springer, New York,
2012)
6. A. Gulli, S. Pal, Deep Learning with Keras (Packt publishing, Birmingham, 2017)

8
