subfield of AI.
v

There is an old idea from Kendo1 which seems to find its way to the new world
of cutting-edge technology. The idea is that you learn a martial art in four stages:
big, strong, fast, light. ‘Big’ is the phase where all movements have to be big and
correct. One here focuses on correct techniques, and one’s muscles adapt to the new
movements. While doing big movements, they unconsciously start becoming
strong. ‘Strong’ is the next phase, when one focuses on strong movements. We
have learned how to do it correctly, and now we add strength, and subconsciously
they become faster and faster. While learning ‘Fast’, we start ‘cutting corners’, and
adopt a certain ‘parsimony’. This parsimony builds ‘Light’, which means ‘just
enough’. In this phase, the practitioner is a master, who does everything correctly,
and movements can shift from strong to fast and back to strong, and yet they seem
effortless and light. This is the road to mastery of the given martial art, and to an art
in general. Deep learning can be thought of an art in this metaphorical sense, since
there is an element of continuous improvement. The present volume is intended not
to be an all-encompassing reference, but it is intended to be the textbook for the
“big” phase in deep learning. For the strong phase, we recommend [1], for the fast
we recommend [2] and for the light phase, we recommend [3]. These are important
works in deep learning, and a well-rounded researcher should read them all.
After this, the ‘fellow’ becomes a ‘master’ (and mastery is not the end of the
road, but the true beginning), and she should be ready to tackle research papers,
which are best found on arxiv.com under ‘Learning’. Most deep learning
researchers are very active on arxiv.com, and regularly publish their preprints.
Be sure to check out also ‘Computation and Language’, ‘Sound’ and ‘Computer
Vision’ categories depending on your desired specialization direction. A good
practice is just to put the desired category on your web browser home screen and
check it daily. Surprisingly, the arxiv.com ‘Neural and Evolutionary Compu-
tation’ is not the best place for finding deep learning papers, since it is a rather
young category, and some researchers in deep learning do not tag their work with
this category, but it will probably become more important as it matures.
The code in this book is Python 3, and most of the code using the library Keras is
a modified version of the codes presented in [2]. Their book2 offers a lot of code and
some explanations with it, whereas we give a modest amount of code, rewritten to
be intuitive and comment on it abundantly. The codes we offer have all been
extensively tested, and we hope they are in working condition. But since this book
is an introduction and we cannot assume the reader is very familiar with coding
deep architectures, I will help the reader troubleshoot all the codes from this book.
A complete list of bug fixes and updated codes, as well as contact details for
submitting new bugs are available at the book’s repository github.com/
skansi/dl_book, so please check the list and the updated version of the code
before submitting a new bug fix request.
1A Japanese martial art similar to fencing.
2This is the only book that I own two copies of, one eBook on my computer and one hard copy—it
is simply that good and useful.
vi
Preface

Artificial intelligence as a discipline can be considered to be a sort of ‘philo-
sophical engineering’. What I mean by this is that AI is a process of taking
philosophical ideas and making algorithms that implement them. The term
‘philosophical’ is taken broadly as a term which also encompasses the sciences
which recently3 became independent sciences (psychology, cognitive science and
structural linguistics), as well as sciences that are hoping to become independent
(logic and ontology4).
Why is philosophy in this broad sense so interesting to replicate? If you consider
what topics are interesting in AI, you will discover that AI, at the most basic level,
wishes to replicate philosophical concepts, e.g. to build machines that can think,
know stuff, understand meaning, act rationally, cope with uncertainty, collaborate to
achieve a goal, handle and talk about objects. You will rarely see a definition of an
AI agent using non-philosophical terms such as ‘a machine that can route internet
traffic’, or ‘a program that will predict the optimal load for a robotic arm’ or ‘a
program that identifies computer malware’ or ‘an application that generates a for-
mal proof for a theorem’ or ‘a machine that can win in chess’ or ‘a subroutine which
can recognize letters from a scanned page’. The weird thing is, all of these are
actual historical AI applications, and machines such as these always made the
headlines.
But the problem is, once we got it to work, it was no longer considered ‘in-
telligent’, but merely an elaborate computation. AI history is full of such examples.5
The systematic solution of a certain problem requires a full formal specification
of the given problem, and after a full specification is made, and a known tool is
applied to it,6 it stops being considered a mystical human-like machine and starts
being considered ‘mere computation’. Philosophy deals with concepts that are
inherently tricky to define such as knowledge, meaning, reference, reasoning, and
all of them are considered to be essential for intelligent behaviour. This is why, in a
broad sense, AI is the engineering of philosophical concepts.
But do not underestimate the engineering part. While philosophy is very prone to
reexamining ideas, engineering is very progressive, and once a problem is solved, it
is considered done. AI has the tendency to revisit old tasks and old problems (and
this makes it very similar to philosophy), but it does require measurable progress, in
the sense that new techniques have to bring something new (and this is its
3Philosophy is an old discipline, dating back at least 2300 years, and ‘recently’ here means ‘in the
last 100 years’.
4Logic, as a science, was considered independent (from philosophy and mathematics) by a large
group of logicians for at least since Willard Van Orman Quine’s lectures from the 1960s, but
thinking of ontology as an independent discipline is a relatively new idea, and as far as I was able
to pinpoint it, this intriguing and promising initiative came from professor Barry Smith form the
Department of Philosophy of the University of Buffalo.
5John McCarthy was amused by this phenomenon and called it the ‘look ma’, no hands’ period of
AI history, but the same theme keeps recurring.
6Since new tools are presented as new tools for existing problems, it is not very common to tackle
a new problem with newly invented tools.
Preface
vii

engineering side). This novelty can be better results than the last result on that
problem,7 the formulation of a new problem8 or results below the benchmark but
which can be generalized to other problems as well.
Engineering is progressive, and once something is made, it is used and built
