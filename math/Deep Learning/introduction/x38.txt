and the PCA. We will briefly address PCA in the next section (especially the intuition
behind it), but we will be returning to it in Chap.9 where we will be giving the
technical details. PCA represents a branch of unsupervised learning called distributed
representations, and it is one of the most important topics in deep learning today,
and PCA is the simplest algorithm for building distributed representations.25 Another
branch of unsupervised learning which is conceptually simpler is called clustering.
The goal of clustering is to assign all datapoints to clusters which (hopefully) capture
their similarity in n-dimensional space. K-means is the simplest clustering algorithm,
and we will use it to illustrate how a clustering algorithm works.26
But before we proceed to K-means, let us comment briefly what is unsupervised
learning. Unsupervised learning is learning without labels or targets. Since unsu-
pervised learning is usually the last of the three areas to be defined (supervised and
reinforcement being the other two), there is a tendency to put everything which is not
supervised or reinforcement learning in unsupervised learning. This is a very broad
definition, but it is very interesting, since it begs the cognitive question of how we
learn without feedback, and is learning without feedback actually learning or is it a
different phenomenon? By exploring unsupervised learning, we are dwelling deep
in cognitive modelling and this makes this an exciting and colourful area.
Let us demonstrate how K-means works. K-means is a clustering algorithm, which
means it will produce clusters of data. Producing clusters actually means assigning a
cluster name to all datapoints so that similar datapoints share the same cluster name.
The usual cluster names are ‘1’, ‘2’, ‘3’, etc. Assume we have two features so that
we work in 2D space. In unsupervised learning, we do not have a training and testing
set, but all datapoints we have are ‘training’ datapoints, and we build the clusters
(which will define the hyperplane) from them. The input row vectors do not have a
label; they consist only of features.
The K-means algorithm takes as an input the number of centroids to be used. Each
centroid will define a cluster. At the very start of the algorithm, the centroids are
placed in a random location in the datapoint vector space. K-means has two phases,
one called ‘assign’ and the another ‘minimize’ forming a cycle, and it repeats this
cycle a number of times.27 During the assign phase, each datapoint is assigned to
the nearest centroid in terms of Euclidean distance. During the ‘minimize’ phase,
centroids are moved in a direction that minimizes the sum of the distance of all
datapoints assigned to it.28 This completes a cycle. The next cycle begins by dis-
25But PCA itself is not that simple to understand.
26K-means (also called the Lloyd-Forgy algorithm) was first proposed by independently by S. P.
Lloyd in [16] and E. W. Forgy in [17].
27Usually, in a predefined number of times, there are other tactics as well.
28Imagine that a centroid is pinned down and connected to all its datapoints with rubber bands, and
then you unpin it from the surface. It will move so that the rubber bands are less tense in total (even
though individual rubber bands may become more tense).

3.6
Learning Without Labels:K-Means
71
Fig.3.9 Two complete cycles of K-means with two centroids
associating all datapoints from centroids. Centroids stay where they are, but a new
assignment phase begins, which may make a different assignment than the previous
one. You can see this in Fig. 3.9. After the end of the cycles, we have a hyperplane
ready: when we get a new datapoint, it will be assigned to the closest centroid. In
other words, it will get the name of the closest centroid as a label.
In the usual setting, we do not have labels when using clustering (and we do
not need them for unsupervised learning). The evaluation metrics we discussed in
the previous sections are useless without labels since we cannot calculate the true
positives, false positives, true negatives and false negatives. It can happen that we
have access to labels but prefer to use clustering, or that we will obtain the true labels
at a later time. In such case, we may evaluate the results of clustering as if they were
classification results, and this is called external evaluation of clustering. A detailed
exposition of using classification evaluation metrics for the external evaluation of
clustering is given in [11].
But sometimes we do not have any labels and we must work without them. In
such cases, we can use a class of evaluation metrics called internal evaluation of
clustering. There are several evaluation metrics, but the Dunn coefficient [12] is the
mostpopular.Themainideaistomeasurehowdensetheclustersareinn-dimensional
space. So for each cluster29 C the Dunn coefficient is calculated by
DC = min{d(i, j)|i, j ∈ Centroids}
din(C)
(3.15)
29Recall that a cluster in K-means is a region around a centroid separated by the hyperplane.

72
3
Machine Learning Basics
Here, d(i, j) is the Euclidean distance between centroids i and j and din(C) is
the intra-cluster distance which is taken to be the distance:
din(C) = max{d(x, y)|x, y ∈ C},
(3.16)
where C is the cluster for which we calculate the Dunn coefficient. The Dunn coef-
ficient is calculated for each cluster and the quality of each cluster can be assessed
by it. The Dunn coefficient can be used to evaluate different clusterings by taking
the average of the Dunn coefficients for each cluster in both clusterings30 and then
comparing them.
3.7 Learning Different Representations: PCA
The data we used so far has local representations. If the value of a feature named
‘Height’ is 180, then that piece of information about that datapoint (we could even say
‘that property of the entity’) exists only there. A different column ‘Weight’ contains
no information on height. Such representations of the properties of the entities that
we are describing as features of a datapoint are called local representations. Notice
that the fact that the object has some height does put a constraint on weight. This is
not a hard constraint but more of an ‘epistemic shortcut’: if we know that the person
is 180cm tall, then they will probably have around 80 kg. Individual persons may
vary, but in general we could make a relatively decent guess of the person’s weight
just by knowing their height. This phenomenon is called correlation and it is a tricky
phenomenon. If two features are highly correlated, they are very hard to tell apart.
Ideally, we would want to find a transformation of the data which has weird features,
but that are not correlated. In this representation, we would have a feature ‘Argh’
which captures the underlying component31 by which we were able to deduce the
weight from the height, and leave ‘Haght’ and ‘Waght’ as the part which is left in
‘Height’ and ‘Weight’ after ‘Argh’ was removed from them. Such representations
are called distributed representations.
