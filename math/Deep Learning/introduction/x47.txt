−
dy
dz 1 + dy
dz e−z
(1 + e−z)2
On the first summand in the numerator, we apply Const and it becomes 0, and on
the second we apply ChainExp and it becomes e−z · dy
dz (−z), and so we have
−e−z · dy
dz (−z)
(1 + e−z)2
By applying LD to the constant factor −1 implicit with z we get
−−1 · dy
dz z · e−z
(1 + e−z)2
11For the sake of easy readability, we deliberately combine Newton and Leibniz notation in the
rules, since some of them are more intuitive in one, while some of them are more intuitive in the
second. We refer the reader back to Chap.1 where all the formulations in both notations were given.

92
4
Feedforward Neural Networks
which by DerDifVar becomes
− −1 · e−z
(1 + e−z)2
We tidy up the signs and get
e−z
(1 + e−z)2
Therefore,
dy
dz =
e−z
(1 + e−z)2
Let us factorize the right-hand side in two factors which we will call A and B:
e−z
(1 + e−z)2 =
1
1 + e−z ·
e−z
1 + e−z
It is obvious that A = y from the definition of y. Let us turn our attention to B:
e−z
1 + e−z = (1 + e−z) − 1
1 + e−z
= 1 + e−z
1 + e−z −
1
1 + e−z = 1 −
1
1 + e−z = 1 − y
Therefore A = y and B = 1 − y, and dy
dz = A · B, from which follows that
dy
dz = y(1 − y)
Since we have ∂z
∂wi and dy
dz with the chain rule we get
∂y
∂wi
= xiy(1 − y)
The next thing we need is dE
dy .12 We will be using the same rules for this derivation
as we did for dy
dz . Recall that E = 1
2(t(n) − y(n))2, but we will use the version E =
1
2(t − y)2 which is focused on a single target value t and a single prediction y.
Therefore, we need to find
dE
dy [1
2(t − y)2]
12Strictly speaking, we would need
∂E
∂y(n) but this generalization is trivial and we chose the simpli-
fication since we wanted to improve readability.

4.5
From the Logistic Neuron to Backpropagation
93
By applying LD we get
1
2
dE
dy (t − y)2
By applying Exp we get
1
2 · 2 · (t − y) · dE
dy (t − y)
Simple cancellation yields
(t − y) · dE
dy (t − y)
With LD we get
(t − y) · dE
dy t · dE
dy y
Since t is a constant, its derivative is 0 (rule Const), and since y is the differentiation
variable, its derivative is 1 (DerDiVar). By tidying up the expression we get (t −
y)(0 − 1) and finally, −1 · (t − y).
Now, we have all the elements for formulating the learning rule for the logistic
neuron using the chain rule:
