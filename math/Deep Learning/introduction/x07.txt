6
1
From Logic to Cognitive Science
by the Air Force Office of Scientific Research) which implemented neural networks
called SNARC (Stochastic Neural Analog Reinforcement Calculator), which was the
first major computer implementation of a neural network. As a bit of trivia, Marvin
Minsky was an advisor to Arthur C. Clarke’s and Stanley Kubrick’s 2001: A Space
Odyssey movie. Also, Isaac Asimov claimed that Marvin Minsky was one of two
people he has ever met whose intelligence surpassed his own (the other one being
Carl Sagan). Minsky will return to our story soon, but first let us present another hero
of deep learning.
Frank Rosenblatt received his PhD in Psychology at Cornell University in 1956.
Rosenblatt made a crucial contribution to neural networks, by discovering the per-
ceptron learning rule, a rule which governs how to update the weights of neural
networks, which we shall explore in detail in the forthcoming chapters. His percep-
trons were initially developed as a program on an IBM 704 computer at Cornell
Aeronautical Laboratory in 1957, but Rosenblatt would eventually develop the Mark
I Perceptron, a computer built with the sole purpose of implementing neural net-
works with the perceptron rule. But Rosenblatt did more than just implement the
perceptron. His 1962 book Principles of Neurodynamics [11] explored a number of
architectures, and his paper [12] explored the idea of multilayered networks similar
to modern convolutional networks, which he called C-system, which might be seen
as the theoretical birth of deep learning. Rosenblatt died in 1971 on his 43rd birthday
in a boating accident.
There were two major trends underlying the research in the 1960s. The first one
wastheresultsthatweredeliveredbyprogramsworkingonsymbolicreasoning,using
deductive logical systems. The two most notable were the Logic Theorist by Herbert
Simon, Cliff Shaw and Allen Newell, and their later program, the General Problem
Solver [13]. Both programs produced working results, something neural networks
did not. Symbolic systems were also appealing since they seemed to provide control
and easy extensibility. The problem was not that neural networks were not giving
any result, just that the results they have been giving (like image classification) were
not really considered that intelligent at the time, compared to symbolic systems
that were proving theorems and playing chess—which were the hallmark of human
intelligence. The idea of this intelligence hierarchy was explored by Hans Moravec
in the 1980s [14], who concluded that symbolic thinking is considered a rare and
desirable aspect of intelligence in humans, but it comes rather natural to computers,
which have much more trouble with reproducing ‘low-level’ intelligent behaviour
that many humans seem to exhibit with no trouble, such as recognizing that an animal
in a photo is a dog, and picking up objects.9
The second trend was the Cold War. Starting with 1954, the US military wanted to
have a program to automatically translate Russian documents and academic papers.
9Even today people consider playing chess or proving theorems as a higher form of intelligence than
for example gossiping, since they point to the rarity of such forms of intelligence. The rarity of an
aspect of intelligence does not directly correlate with its computational properties, since problems
that are computationally easy to describe are easier to solve regardless of the cognitive rarity in
humans (or machines for that matter).

1.2
The XOR Problem
7
Funding was abundant, but many technically inclined researchers underestimated
the linguistic complexities involved in extracting meaning from words. A famous
example was the back and forth translation from English to Russian and back to
English of the phrase ‘the spirit was willing but the flesh was weak’ which produced
the sentence ‘the vodka was good, but the meat was rotten’. In 1964, there were
some concerns about wasting government money in a dead end, so the National
Research Council formed the Automatic Language Processing Advisory Committee
or ALPAC [13]. The ALPAC report from 1966 cut funding to all machine translation
projects, and without the funding, the field lingered. This in turn created turmoil in
the whole AI community.
But the final stroke which nearly killed off neural networks came in 1969, from
Marvin Minsky and Seymour Papert [15], in their monumental book Perceptrons:
An Introduction to Computational Geometry. Remember that McCulloch and Pitts
proved that a number of logical functions can be computed with a neural network. It
turns out, as Minsky and Papert showed in their book, they missed a simple one, the
equivalence. The computer science and AI community tend to favour looking at this
problem as the XOR function, which is the negation of an equivalence, but it really
does not matter, since the only thing different is how you place the labels.
It turns out that perceptrons, despite the peculiar representations of the data they
process, are only linear classifiers. The perceptron learning procedure is remarkable,
since it is guaranteed to converge (terminate), but it did not add a capability of cap-
turing nonlinear regularities to the neural network. The XOR is a nonlinear problem,
but this is not clear at first.10 To see the problem, imagine11 a simple 2D coordinate
system, with only 0 and 1 on both axes. The XOR of 0 and 0 is 0, and write an O at
coordinates (0, 0). The XOR of 0 and 1 is 1, and now write an X at the coordinates
(0,1). Continue with XOR(1, 0) = 1 and XOR(1, 1) = 0. You should have two Xs
and two Os. Now imagine you are the neural network, and you have to find out how
to draw a curve to separate the Xs from Os. If you can draw anything, it is easy. But
you are not a modern neural network, but a perceptron, and you must use a straight
line—no curves. It soon becomes obvious that this is impossible.12 The problem with
the perceptron was the linearity. The idea of a multilayered perceptron was here, but
it was impossible to build such a device with the perceptron learning rule. And so,
seemingly, no neural network could handle (learn to compute) even the basic logical
operations, something symbolic systems could do in an instant. A quiet darkness fell
across the neural networks, lasting many years. One might wonder what was hap-
pening in the USSR at this time, and the short answer is that cybernetics, as neural
networks were still called in the USSR in this period, was considered a bourgeois
pseudoscience. For a more detailed account, we refer the reader to [16].
10The view is further dimmed by the fact that the perceptron could process an image (at least
rudimentary), which intuitively seems to be quite harder than simple logical operations.
11Pick up a pen and paper and draw along.
12If you wish to try the equivalence instead of XOR, you should do the same but with
EQUIV(0, 0) = 1, EQUIV(0, 1) = 0, EQUIV(1, 0) = 0, EQUIV(1, 1) = 1, keeping the Os for
0 and Xs for 1. You will see it is literally the same thing as XOR in the context of our problem.

8
1
From Logic to Cognitive Science
1.3 From Cognitive Science to Deep Learning
