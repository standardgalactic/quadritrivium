Let us see how the memory works. The memory is represented by a matrix (or
possibly higher order tensor) Mt. Each row in this matrix is called a memory location.
If there are n rows in the memory, the controller produces a weighting vector of size n
1Forafullydetailedview,seetheblogentryofoneofthecreatorsoftheNTM,https://medium.
com/aidangomez/the-neural-turing-machine-79f6e806\penalty-
\@Mc0a1.

10.2
Memory-Based Models
179
Fig.10.3 Neural
Turing-machines
(components range from 0 to 1) which indicates how much of each of those locations
to take in consideration. This can be a crisp access to a one or several locations or a
fuzzy access to those locations. Since this vector is trainable, it is almost never crisp.
This is the reading operation, defined simply as the Hadamard product (pointwise
multiplication) of m by n matrix Mt and B, where B is obtained by transposing the
m-dimensional row vector wt, and then broadcasting its values (just copying this
column n − 1 times) to match the dimensions of Mt.
The neural Turing-machine will now write. It always reads and writes, but some-
times it writes very similar values, so we have the impression that the content is not
changed. This is important since it is a common source of confusion thinking that the
NTM makes a decision whether to (over)write or not. It does not make this decision
(it does not have a separate decision mechanism), it always performs the writing, but
sometimes the value written is the same as the old value.
The write operation itself is composed by two components: (i) the erase compo-
nent, and (ii) add component. The erase operation resets the components of a memory
location to zero only if both the weighting vector wt component for that location and
the erase vector et component are both 1. In symbols: ˆMt = Mt−1 · (I − wt · et),
where I is a row vector of 1s, and all products are Hadamard or pointwise, so these
multiplicationsarecommutative.Totakecareofthedimensions,transposeandbroad-
cast as needed. The add operation performs exactly the same taking in ˆMt instead
of Mt−1, but by using the equation: Mt = ˆMt + wt · at). Remember, the way these
things work is the same, they are all operations on trainable components–there is
no intrinsic difference, only operations and trainable differences. We now have to
connect the two parts, and this is done by addressing. Addressing is the part which
describes how the weighting vectors wt are produced. It is a relatively complex pro-
cedure involving a number of components, and we refer the reader to the original
paper [9] for details. What is important to note is that neural Turing-machines have
location-based addressing and content-based addressing.
Asecondmemory-basedmodel,muchsimplerandequallypowerfulisthememory
networks (MemNN) introduced in [10]. The idea is to extend LSTM to make the long
term dependency memory better. Memory networks have several components, and
aside from the memory, all of them are neural networks, which makes memory

180
10
An Overview of Different Neural Network Architectures
networks even more aligned with the spirit of connectionism than neural Turing-
machines, while retaining all the power. The components of the memory network
are:
• Memory (M): An array of vectors
• Input feature map (I): converts the input into a distributed representation
• Updater (G): decides how to update the memory given the distributed represen-
tation passed in by I
• Output feature map (O): receives the input distributed representation and finds
supportive vectors from memory, and produces an output vector
• Responder (R): Additionally formats the output vectors given by O
Their connections are illustrated in Fig. 10.4. All of these components except
memory are functions described by neural networks and hence trainable. In a simple
version, I would be word2vec, G would simply store the representation in the next
available memory slot, R would modify the output by replacing indexes with words
and adding some filler words. O is the one that does the hard work. It would have to
find a number of supporting memories (a single memory scan and update is called
a hop2), and then find a way of ‘bundling’ them with what I has forwarded. This
‘bundling’ is simple matrix multiplication, of the input and the memory, but with also
some additional learned weights. This is how it always should be in connectionists
models: just adding, multiplying and weights. And the weights are where the magic
happens. A fully trainable complex memory network is presented in [11].
One problem that both neural Turing-machines and memory networks have in
common is that they have to use segmented vector-based memory. It would be inter-
esting to see how to make a memory-based model with a continuous memory, perhaps
with encoding vectors in floats. But a word of warning, even plain-vanilla memory
networks have a lot more trainable parameters than LSTMs, and training could take
a lot of time, so one of the major challenges in memory models mentioned in [11]
is how to reuse parameters in various components, which would speed up learning.
Memory networks memory addressing is only content-based.
Fig.10.4 Memory networks
2By default, memory networks make one hop, but it has been shown that multiple hops are beneficial,
especially in natural language processing.

10.3
The Kernel of General Connectionist Intelligence:The bAbI Dataset
181
10.3 The Kernel of General Connectionist Intelligence:The bAbI
Dataset
Despite their colourful past, neural networks today are a recognized subfield of AI,
and deep learning is making a run for the whole AI. A natural question arises, how
can we evaluate neural networks as an AI system, and it seems that the old idea of
the Turing test is coming back. Fortunately, there is a dataset of toy tasks called bAbI
[12], which was made with the idea of it becoming a kernel for general AI: Any
agent hoping to be recognized as general AI should be able to pass all the toy tasks
in the bAbI dataset. The bAbI dataset is one of the most important general AI tasks
to be confronted with a purely connectionistic approach.
The tasks in the dataset are expressed in natural language, and there are twenty
categories of them. The first category addresses single supporting fact, and it has
samples that try to capture a simple repetition of what was already stated like the
example produced ‘Mary went to the bathroom. John moved to the hallway. Mary
travelled to the office. Where is Mary?. The next two tasks introduce more supporting
facts, i.e. more actions by the same person. The next task focuses on learning and
