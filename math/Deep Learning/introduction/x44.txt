current classification result:
z = b +
�
i
wixi = 0.5 + 2 · 0.3 + (−3) · 0.4 = −0.1
As z < 0, the output of the perceptron is 0 and should have been 1. This means
that we have to use clause (3) from the perceptron rule and add the input vector to
the weight vector:
(w, b) ← (w, b) + (x, 1) = (2, −3, 0.5) + (0.3, 0.4, 1) = (2.3, −2.6, 1.5)
If adding handcrafted features is not a option, the perceptron algorithm is very
limited.ToseeasimpleproblemthatMinskyandPapertexposedin1969[5],consider
that each classification problem can be understood as a query on the data. This means
that we have a property we want the input to satisfy. Machine learning is just a method
for defining this complex property in terms of the (numerical) properties present in
the input. A query then retrieves all the input points satisfying this property. Suppose
we have a dataset consisting of people and their height and weight. To return only
those higher than say 175cm, one would make a query of the form select ∗
from table where cm>175. If we, on the other hand, only have jpg files of
mugshots with the black and white meter behind the faces, then we would need a
classifier to determine the people’s height and then sort them accordingly. Note that
this classifier would not use numbers, but rather pixels, so it might find people of e.g.
155cm similar to those of height 175, but not those of 165, since the black and white
parts of the background are similar. This means that the machine learning algorithm
learns ‘similar’ in terms of the information representation it is given: what might
seem similar in terms of numbers might not be similar in terms of pixels and vice
versa. Consider the numbers 6 and 9: visually they are close (just rotate one to get
3Formally speaking, all units using the perceptron rule should be called perceptrons, not just binary
threshold units.
4The target is also called expected value or true label, and it is usually denoted by t.

86
4
Feedforward Neural Networks
the other) but numerically they are not. If the representation given to an algorithm is
in pixels, and it can be rotated,5 the algorithm will consider them the same.
When classifying, the machine learning algorithm (and perceptrons are a type of
machine learning algorithms) selects some datapoints as belonging to a class and
leaves the other out. This means that some of them get the label 1 and some get the
label0,andthislearnedpartitioninghopefullycapturestheunderlyingreality:thatthe
datapoints labelled 1 really are ‘ones’ and the datapoints labelled 0 really are ‘zeros’.
A classic query in logic and theoretical computer science is called parity. This query
is done over binary strings of data, and only those with an equal number of ones and
zeros are selected and given the label 1. Parity can be relaxed so it considers only
strings of length n, then we can formally name it parityn(x0, x1, . . . , xn), where
each xi is a single binary digit (or bit). parity2 is also called XOR and it is also
a logical function called exclusive disjunction. XOR takes two bits and returns 1 if
and only if there is the same amount of 1 and 0, and since they are binary strings,
this means that there is one 1 and one 0. Note that we can equally use the logical
equivalence which has the resulting 0 and 1 exchanged, since they are just names for
classes and do not carry much more meaning. So XOR gives the following mapping:
(0, 0) �→ 0, (0, 1) �→ 1, (1, 0) �→ 1, (1, 1) �→ 0.
When we have XOR as the problem (or any instance of parity for that matter),
the perceptron is unable to learn to classify the input so that they get the correct
labels. This means that a perceptron that has two input neurons (for accepting the
two bits for XOR) cannot adjust its two weights to separate the 1 and 0 as they come
in the XOR. More formally, if we denote by w1, w2 and b the weights and biases
of the perceptron, and take the following instance of parity (0, 0) �→ 1, (0, 1) �→ 0,
(1, 0) �→ 0 i (1, 1) �→ 1, we get four inequalities:
1. w1 + w2 ≥ b,
2. 0 ≥ b,
3. w1 < b,
4. w2 < b
The inequality (a) holds since if (x1 = 1, x2 = 1) �→ 1, and we can get 1 as an out-
put only if w1x1 + w2x2 = w1 · 1 + w2 · 1 = w1 + w2 is greater or equal b, which
means w1 + w2 ≥ b.
The inequality (b) holds since if (x1 = 0, x2 = 0) �→ 1, and we can get 1 as an
output only if w1x1 + w2x2 = w1 · 0 + w2 · 0 = 0 is greater or equal b, which means
0 ≥ b.
The inequality (c) holds since if (1, 0) �→ 0, then w1x1 + w2x2 = w1 · 1 + w2 ·
0 = w1, and for the perceptron to give 0, w1 has to be less than the bias b, i.e. w1 < b.
5As a simple application, think of an image recognition system for security cameras, where one
needs to classify numbers seen regardless of their orientation.

4.3
The Perceptron Rule
87
The inequality (d) is derived in a similar fashion to (c). By adding (a) and (b) we
get w1 + w2 ≥ 2b, and by adding (c) and (d) we get w1 + w2 < 2b. It is easy to see
that the system of inequalities has no solution.
This means that the perceptron, which was claimed to be a contendant for general
artificial intelligence could not even learn logical equality. The proposed solution
was to make a ‘multilayered perceptron’.
4.4 The Delta Rule
The main problem with making the ‘multilayered perceptron’ is that it is unknown
how to extend the perceptron learning rule to work with multiple layers. Since mul-
tiple layers are needed, the only option seemed to be to abandon the perceptron rule
and use a different rule which is more robust and capable of learning weights accross
layer. We already mentioned this rule—backpropagation. It was first discovered by
Paul Werbos in his PhD thesis [6], but it remained unnoticed. It was discovered for the
second time by David Parker in 1981, who tried to get a patent but he subsequently
published it in 1985 [7]. The third and the last time it was discovered independently
by Yann LeCun in 1985 [8] and by Rumelhart, Hinton and Williams in 1986 [9].
To see what we want to archive, let us consider an example6 imagine that each
day we buy lunch at the nearby supermarket. Every day our meal consists of a piece
of chicken, two grilled zucchinis and a scoop of rice. The cashier just gives us the
total amount, which varies each day. Suppose that the price of the components does
not very over time and that we can weight the food to see how much we have. Note
that one meal will not be enough to deduce the prices, since we have three of them7
and we do not know which component is responsible in what proportion for a total
price increase in one euro.
