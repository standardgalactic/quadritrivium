v(1)
2
· · · v(1)
d
v(2)
1
v(2)
2
· · · v(2)
d
...
...
...
...
v(d)
1
v(d)
2
· · · v(d)
d
⎤
⎥⎥⎥⎥⎦
=
⎡
⎢⎢⎢⎣
v11 v12 · · · v1d
v21 v22 · · · v2d
...
...
...
...
vd1 vd2 · · · vdd
⎤
⎥⎥⎥⎦
We now create a blank matrix of zeros (size d × d) and put the lambdas in descend-
ing order on the diagonal. We call this matrix �:
V =
⎡
⎢⎢⎢⎣
λ1 0 · · · 0
0 λ2 · · · 0
...
... ... ...
0
0 · · · λd
⎤
⎥⎥⎥⎦
With this, we turn to the eigendecomposition of a matrix. We need to have a
symmetric matrix A and then its eigendecomposition is:
A = V �V −1
(8.3)
The only condition is that all eigenvectors vi are linearly independent. Since
� is a symmetrical matrix with linearly independent eigenvectors, we can use the
eigendecomposition to get the following equations which hold for any covariance
matrix �:
� = V �V −1
(8.4)
�V = V �
(8.5)
Since V is orthonormal,2 we also have V ⊤V = I. Now we are ready to return to
Z = X Q. Let us take a look at the transformed data Z. We can express the covariance
of Z as the covariance of X multiplied by Q:
�Z = 1
d ((Z − M E AN(Z))⊤(Z − M E AN(Z))) =
(8.6)
= 1
d ((X Q − M E AN(X)Q)⊤(X Q − M E AN(X)Q)) =
(8.7)
= 1
d Q⊤(X − M E AN(X))⊤(X − M E AN(X))Q =
(8.8)
= Q⊤�X Q
(8.9)
2We omit the proof but it can be found in any linear algebra textbook, such as e.g. [1].

156
8
Autoencoders
We now have to choose a matrix Q so that we get what we want (correlation zero
and features ordered according to variance). We simply chose Q := V . Then we
have:
�Z = V ⊤�X V = V ⊤V � = �
(8.10)
Let us see what we have achieved. All elements except the diagonal elements of
�Z are zero, which means that the only correlation left in Z is along the diagonal.
This is the covariance of a variable with itself, which is actually the variance we have
encountered earlier, and the matrix is ordered in descending variance (V AR(Xi) =
COV (Xi, Xi) = λi). This is everything we wanted. Note that we have done PCA for
the 2D case with matrices but the same ideas hold for tensors. More on the principal
component analysis can be found in [2].
So we have seen how we can create a different representation of the same data
such that the features it is described with have a covariance of zero, and are sorted by
variance. In doing so we have created a distributed representation of the data, since
a column named ‘height’ does not exist anymore, and we have synthetic columns.
The point here is that we can build various distributed representations, but we have
to know what constraint we want the final data to obey. If we want this constraint to
be left unspecified and we want to specify it not directly but by feeding examples,
then we will have to employ a more general approach. This is the approach that leads
us to autoencoders, which offer a surprising generality across many tasks.
8.2 Different Autoencoder Architectures
