Let us turn our attention to the weights. The bias is a bit of a bother, so we can
turn it in one of the weights. To do this, we have to add a single column of 1’s as
the first column of the input matrix. Notice that this will not be an approximation,
but will capture exactly the calculation we need to perform. As for the weights, we
will be needing as many weights as there are inputs. Also, if we have more than
one workhorse neuron, we would need to have that many times the weights, e.g. if
we have 5 inputs (5-dimensional input row vectors) and 3 workhorse neurons, we
would need 5 × 3 weights. This 5 × 3 is deliberate, since we would be using a 5 × 3
matrix20 to store it in, since then we could do all the calculations needed for the logit
with a simple matrix multiplication. This illustrates something that could be called
‘the general deep learning strategy for fast computation’: try to do as much work as
you can with matrix (and vector) multiplication and transpositions.
Returning to our example, we have three inputs and we add the column of 1’s in
front of the inputs to make room for the bias in the weight matrix. The new input
matrix is now a 3 × 4 matrix:
x =
⎡
⎣
1 0.2 0.5 0.91
1 0.4 0.01 0.5
1 0.3 1.1
0.8
⎤
⎦
Now we can define the weight matrix. It is a 4 × 1 matrix consisting of the bias
followed by weight:
w =
⎡
⎢⎢⎣
0.66
0.1
0.35
0.7
⎤
⎥⎥⎦
20Recall that this is not the same as a 3 × 5 matrix.

3.4
A Simple Neural Network:Logistic Regression
67
This matrix can be equivalently represented as (0.66, 0.1, 0.35, 0.7)⊤, but we
will use the matrix form for now. Now, to calculate the logit we do simple matrix
multiplication of the two matrices, with which we will get a 3 × 1 matrix in which
every row (there is a single value in every row) will represent the logit for each
training case (compare this with the previous calculation):
z = xw =
⎡
⎣
1 0.2 0.5 0.91
1 0.4 0.01 0.5
1 0.3 1.1
0.8
⎤
⎦ ·
⎡
⎢⎢⎣
0.66
0.1
0.35
0.7
⎤
⎥⎥⎦ =
(3.11)
=
⎡
⎣
1 · 0.66 + 0.2 · 0.1 + 0.5 · 0.35 + 0.91 · 0.7
1 · 0.66 + 0.4 · 0.1 + 0.01 · 0.35 + 0.5 · 0.7
1 · 0.66 + 0.3 · 0.1 + 1.1 · 0.35 + 0.8 · 0.7
⎤
⎦ =
(3.12)
=
⎡
⎣
1.492
1.0535
1.635
⎤
⎦
(3.13)
Now we must only apply the logistic functionσ to z. This is done by simply
applying the function to each element of the matrix:
σ(z) =
⎡
⎣
σ(1.492)
σ(1.0535)
σ(1.635)
⎤
⎦ =
⎡
⎣
0.8163
0.7414
0.8368
⎤
⎦
We add a final remark. The logistic function is the main component of the logistic
regression. But if we treat the logistic regression as a simple neural network, we
