and wnew
4
we need to go across D. Therefore we need:
∂E
∂w3
= ∂E
∂yD
· ∂yD
∂zD
· ∂zD
∂w3
But we know the procedure, so:
∂E
∂yD
= w6 · ∂E
∂zF
= 0.6 · (−0.0909) = −0.0545
∂yC
∂zC
= yD(1 − yD) = 0.5892(1 − 0.5892) = 0.2420
And:
∂zD
∂w2
= 0.23
∂zD
∂w4
= 0.82
Finally, we have (remember we have the 0.7 learning rate):
wnew
2
= 0.5 − 0.7 · (−0.0545 · 0.2420 · 0.23) = 0.502
wnew
4
= 0.3 − 0.7 · (−0.0545 · 0.2420 · 0.82) = 0.307
And we are done. To recap, we have:
• wnew
1
= 0.1007
• wnew
2
= 0.502
• wnew
3
= 0.4024
• wnew
4
= 0.307
• wnew
5
= 0.2373
• wnew
6
= 0.6374
• Eold = 0.0739

102
4
Feedforward Neural Networks
We can now make another forward pass with the new weights to make sure that
the error has decreased:
ynew
C
= σ(0.23 · 0.1007 + 0.82 · 0.4024) = σ(0.3531) = 0.5873
ynew
D
= 0.5907
ynew
F
= σ(0.5873 · 0.2373 + 0.5907 · 0.6374) = σ(0.5158) = 0.6261
Enew = 1
2(1 − 0.6261)2 = 0.0699
Which shows that the error has decreased. Note that we have processed only
one training sample, i.e. the input vector (0.23, 0.82). It is possible to use multiple
training samples to generate the error and find the gradients (mini-batch training21),
and we can do this a number of times and each repetition is called an iteration.
Iterations are sometimes erroneously called epochs. The two terms are very similar
and we can consider them synonyms for now, but quite soon we will need to delineate
the difference, and we will do this in the next chapter.
An alternative to this would be to update the weights after every single training
example.22 This is called online learning. In online learning, we process a single
input vector (training sample) per iteration. We will discuss this in the next chapter
in more detail.
In the remainder of this chapter, we will integrate all the ideas we have presented
so far in a fully functional feedforward neural network, written in Python code. This
example will be fully functional Python 3.x code, but we will write out some things
that could be better left for a Python module to do.
Technically speaking, in anything but the most basic setting, we shall not use
the SSE, but its variant, the mean squared error (MSE). This is because we need
to be able to rewrite the cost function as the average of the cost functions SSEx for
individual training samples x, and we therefore define MSE := 1
n
�
x SSEx.
4.7 A Complete Feedforward Neural Network
Let us see a complete feedforward neural network which does a simple classification.
The scenario is that we have a webshop selling books and other stuff, and we want
to know whether a customer will abandon a shopping basket at checkout. This is
why we are making a neural network to predict it. For simplicity, all the data is just
numbers. Open a new text file, rename it to data.csv and write in the following:
21Or full-batch if we use the whole training set.
