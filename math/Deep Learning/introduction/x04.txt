136
7.3
Adding Feedback Loops and Unfolding a Neural Network . . . .
139
7.4
Elman Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
140
7.5
Long Short-Term Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
142
7.6
Using a Recurrent Neural Network for Predicting
Following Words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
145
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
152
8
Autoencoders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
153
8.1
Learning Representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
153
8.2
Different Autoencoder Architectures . . . . . . . . . . . . . . . . . . . . . .
156
8.3
Stacking Autoencoders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
158
8.4
Recreating the Cat Paper . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
161
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
163
9
Neural Language Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
165
9.1
Word Embeddings and Word Analogies. . . . . . . . . . . . . . . . . . .
165
9.2
CBOW and Word2vec . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
166
9.3
Word2vec in Code . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
168
xii
Contents

9.4
Walking Through the Word-Space: An Idea That
Has Eluded Symbolic AI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
171
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
173
10
An Overview of Different Neural Network Architectures. . . . . . . . .
175
10.1
Energy-Based Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
175
10.2
Memory-Based Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
178
10.3
The Kernel of General Connectionist Intelligence:
The bAbI Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
181
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
182
11
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
185
11.1
An Incomplete Overview of Open Research Questions . . . . . . .
185
11.2
The Spirit of Connectionism and Philosophical Ties . . . . . . . . .
186
Reference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
187
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
189
Contents
xiii

1
From Logic to Cognitive Science
1.1 The Beginnings of Artificial Neural Networks
Artificial intelligence has it roots in two philosophical ideas of Gottfried Leibniz, the
great seventeenth-century philosopher and mathematician, viz. the characteristica
universalis and the calculus ratiocinator. The characteristica universalis is an ide-
alized language, in which all of science could in principle be translated. It would be
language in which every natural language would translate, and as such it would be
the language of pure meaning, uncluttered by linguistic technicalities. This language
can then serve as a background for explicating rational thinking, in a manner so pre-
cise, a machine could be made to replicate it. The calculus ratiocinator would be a
name for such a machine. There is a debate among historians of philosophy whether
this would mean making a software or a hardware, but this is in fact a insubstantial
question since to get the distinction we must understand the concept of an universal
machine accepting different instructions for different tasks, an idea that would come
