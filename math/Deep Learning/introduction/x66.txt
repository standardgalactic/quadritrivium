The Three Settings of Learning with Recurrent Neural Networks
137
(i) P(A) ≥ 0, (ii) P(�) = 1, where � is the possibility space and (iii) for all disjoint
An, n ∈ N, P(�∞
n=1 An) = �∞
n=1 P(An) is a probability predicate. Moreover, it is
the probability predicate (try to work out the why by yourself).
Taking the probabilistic interpretation to analyze the machine learning algorithms
from a bird’s-eye perspective, we could say that what a supervised machine learning
algorithm does is calculate P(t|x) (where x denotes an input vector, and t denotes
the target vector1). This is the classic setting, simple supervised learning with labels.
Recurrent neural networks can learn in this standard setting by simply digesting
a lot of labelled sequences and then they predict the label of each finished sequence.
An example might be classifying audio clips according to emotions. But recurrent
neural networks are capable of much more. They can also learn from sequences with
multiple labels. Imagine an industrial robotic arm that we wish to train to perform
a task. It has a multitude of sensors and it has to learn directions (for simplicity
suppose we have only four, North, South, East and West). The training set is then
produced with movement sequences, each consisting of a string of directions, e.g.
x1Nx2Nx3W x4Ex5W x6W or just x1Nx2W. Notice how different this is from what
we have seen before. Here we have a sequence of sensor data (xi) and movements (N,
E, S or W, we will denote them by D). Notice that it would be a very bad idea to break
up the sequences in x D pieces, since a movement of the form x Nx N might happen
most often when broken, it might make sense only in the beginning of the sequence
(e.g. as a ‘get out of the dock’ command) and in any other case it would be disastrous.
Sequences cannot be broken, and it is not enough to know the previous state to be
able to predict the next. The idea that the next state depends only on the current is
known as the Markov assumption, and one of the greatest strengths of the recurrent
neural networks is that they do not need to make the Markov assumption—they can
model more complex behaviour. This means that the recurrent network learns from
uneven sequences whose parts are labelled and it creates a bunch of labels when it
predicts over an unknown vector. This we will call sequential setting.
There is a third setting which is an evolved form of the sequential setting and we
can call it the predict-next setting. This setting does not need labels at all and it is
commonly used for natural language processing. Actually, it has labels, but they are
implicit. The idea is that for every input sequence (sentence), the recurrent network
breaks it down to subsequences and use the next word as the target. We will need
special tokens for the start and end of the sentence, which we must put in manually,
and we denote them here by $ (‘start’) and & (‘end’). If we have a sentence ‘All I
want for Christmas is you’, then we first have to transform it into ‘$ all I want for
Christmas is you &’.2 Then the sentence is broken into inputs and targets, which we
will denote as (‘input string’,‘target’):
1In machine learning literature, it is common to find the notation ˆy, which denotes the results from
the predictor, and y is kept for denoting target values. We have used a different notation, more
common to deep learning, where y denotes the outputs from the predictor, and t is used to denote
actual values or targets.
2Notice which capital letters we kept and try to conclude why.

138
7
Recurrent Neural Networks
• (‘$’,‘all’)
• (‘$ all’,‘I’)
• (‘$ all I’,‘want’)
• (‘$ all I want’, ‘for’)
• (‘$ all I want for’, ‘Christmas’)
• (‘$ all I want for Christmas’, ‘is’)
• (‘$ all I want for Christmas is’, ‘you’)
• (‘$ all I want for Christmas is you’, ‘&’).
Then, the recurrent network will learn how to return the most likely next word
after hearing a word sequence. This means that the recurrent network is learning a
probability distribution from the inputs, i.e. P(x), which actually makes this unsu-
pervised learning, since there are no targets. Targets here are synthesized from the
inputs.
Note that we will usually want to limit how many words we want to look back
(i.e. the word-wise length of the ‘input string’ part). Notice that this is actually quite
a big deal since this can be seen as a question answering capability, which is the
basis of the Turing test, and this is a step towards not just a useful tool, but also
towards general AI. But, we have to make one tiny adjustment here. Notice that if
the recurrent network learns which is the most probable word following a sequence,
it might become repetitive. Imagine that we have the following five sentences in the
training set:
• ‘My name is Cassidy’
• ‘My name is Myron’
• ‘My name is Marcus’
• ‘My name is Marcus’
• ‘My name is Marcus’.
Now, the recurrent neural network would conclude that P(Marcus) = 0.6,
P(Myron) = 0.2 and P(Cassidy) = 0.2. So when given a sequence ‘My name is’
it would always pick ‘Marcus’ since it has the highest probability. The trick here is
not to let it pick the one with the highest probability, but rather the recurrent neural
network should build a probability distribution for every input sequence with the
individual probabilities of all outcomes and then randomly sample it. The result will
be that in 60% of the time it will give ‘Marcus’ but sometimes it will also produce
‘Myron’ and ‘Cassidy’. Note that this actually solves quite a bit of problems which
might arise. If it were not so, we would have always the same response to the same
sequences of words. Now that we have given a quick black box view, it is time to
dig deep into the mechanics of recurrent neural networks.

7.3
Adding Feedback Loops and Unfolding a Neural Network
139
7.3 Adding Feedback Loops and Unfolding a Neural Network
Let us now see how recurrent neural networks work. Remember the vanishing gra-
dient problem? There we have seen that adding layers one after the other would
severely cripple the ability to learn weights by gradient descent, since the move-
ments would be really small, sometimes even rounded to zero. Convolutional neural
networks solved this problem by using a shared set of weights, so learning even
little by little is not a problem since each time the same weights get updated. The
only problem is that convolutional neural networks have a very specific architecture
