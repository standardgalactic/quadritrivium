3.2
Evaluating Classification Results . . . . . . . . . . . . . . . . . . . . . . . .
57
3.3
A Simple Classifier: Naive Bayes. . . . . . . . . . . . . . . . . . . . . . . .
59
3.4
A Simple Neural Network: Logistic Regression . . . . . . . . . . . . .
61
3.5
Introducing the MNIST Dataset . . . . . . . . . . . . . . . . . . . . . . . . .
68
3.6
Learning Without Labels: K-Means . . . . . . . . . . . . . . . . . . . . . .
70
3.7
Learning Different Representations: PCA . . . . . . . . . . . . . . . . . .
72
3.8
Learning Language: The Bag of Words Representation . . . . . . .
75
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
4
Feedforward Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
79
4.1
Basic Concepts and Terminology for Neural Networks . . . . . . .
79
4.2
Representing Network Components with Vectors
and Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
82
4.3
The Perceptron Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
84
4.4
The Delta Rule . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
87
xi

4.5
From the Logistic Neuron to Backpropagation . . . . . . . . . . . . . .
89
4.6
Backpropagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
4.7
A Complete Feedforward Neural Network . . . . . . . . . . . . . . . . .
102
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
105
5
Modifications and Extensions to a Feed-Forward Neural
Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
107
5.1
The Idea of Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
107
5.2
L1 and L2 Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
109
5.3
Learning Rate, Momentum and Dropout. . . . . . . . . . . . . . . . . . .
111
5.4
Stochastic Gradient Descent and Online Learning . . . . . . . . . . .
116
5.5
Problems for Multiple Hidden Layers: Vanishing
and Exploding Gradients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
118
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
119
6
Convolutional Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
121
6.1
A Third Visit to Logistic Regression . . . . . . . . . . . . . . . . . . . . .
121
6.2
Feature Maps and Pooling . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
125
6.3
A Complete Convolutional Network. . . . . . . . . . . . . . . . . . . . . .
127
6.4
Using a Convolutional Network to Classify Text . . . . . . . . . . . .
130
References. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
132
7
Recurrent Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
135
7.1
Sequences of Unequal Length . . . . . . . . . . . . . . . . . . . . . . . . . .
135
7.2
The Three Settings of Learning with Recurrent Neural
Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
